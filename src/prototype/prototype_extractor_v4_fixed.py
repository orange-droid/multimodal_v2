#!/usr/bin/env python3
"""
åŸå‹æå–å™¨V4ä¿®æ­£ç‰ˆ - åŸºäºéœ¸å‡Œä¼šè¯çš„åŸå‹æå–

æ ¹æ®ç”¨æˆ·æŒ‡å¯¼é‡æ–°è®¾è®¡ï¼š
1. ä»éœ¸å‡Œæ ‡ç­¾çš„ä¼šè¯å¼€å§‹
2. ä»¥æ”»å‡»æ€§è¯„è®ºä¸ºä¸­å¿ƒæå–6-20ä¸ªèŠ‚ç‚¹çš„å­å›¾
3. è®¡ç®—å­å›¾çš„æƒ…æ„Ÿåˆ†æ•°
4. åŸºäºæƒ…æ„Ÿé˜ˆå€¼ç­›é€‰éœ¸å‡Œå­å›¾
5. ä»éœ¸å‡Œå­å›¾ä¸­æå–åŸå‹
"""

import numpy as np
import torch
import pickle
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Set
from collections import defaultdict, Counter
from tqdm import tqdm
import networkx as nx
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

class PrototypeExtractorV4Fixed:
    """åŸºäºéœ¸å‡Œä¼šè¯çš„åŸå‹æå–å™¨V4ä¿®æ­£ç‰ˆ"""
    
    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–åŸå‹æå–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.logger = self._setup_logger()
        
        # æ ¸å¿ƒå‚æ•°
        self.subgraph_size_range = self.config.get('subgraph_size_range', (6, 30))  # å­å›¾å¤§å°6-30ä¸ªèŠ‚ç‚¹
        self.emotion_threshold = self.config.get('emotion_threshold', -0.3)  # æƒ…æ„Ÿé˜ˆå€¼ï¼Œé€šè¿‡è°ƒå‚ç¡®å®š
        self.clustering_eps = self.config.get('clustering_eps', 0.4)  
        self.min_cluster_size = self.config.get('min_cluster_size', 2)  
        
        # å¤šæ ·æ€§é‡‡æ ·é…ç½®
        self.max_samples = self.config.get('max_samples', 1000)  # å¤§å¹…å¢åŠ é‡‡æ ·æ•°é‡åˆ°1000ä¸ª (åŸ300ä¸ª)
        self.diversity_sampling = self.config.get('diversity_sampling', True)  # å¯ç”¨å¤šæ ·æ€§é‡‡æ ·
        self.size_diversity_bins = self.config.get('size_diversity_bins', 5)  # å¢åŠ åˆ°5ä¸ªåŒºé—´é‡‡æ ·ï¼Œæé«˜å¤šæ ·æ€§
        self.quality_sampling_ratio = self.config.get('quality_sampling_ratio', 0.7)  # 70%åŸºäºè´¨é‡é‡‡æ ·ï¼Œ30%éšæœºé‡‡æ ·
        self.adaptive_sampling = self.config.get('adaptive_sampling', True)  # å¯ç”¨è‡ªé€‚åº”é‡‡æ ·
        
        # åˆ†å±‚é‡‡æ ·é…ç½® (æ–°å¢)
        self.stratified_sampling = self.config.get('stratified_sampling', True)  # å¯ç”¨åˆ†å±‚é‡‡æ ·
        self.attack_level_bins = self.config.get('attack_level_bins', [
            (0.05, 0.15),  # è½»åº¦æ”»å‡»æ€§
            (0.15, 0.30),  # ä¸­åº¦æ”»å‡»æ€§  
            (0.30, 0.50),  # é«˜åº¦æ”»å‡»æ€§
            (0.50, 1.00),  # æé«˜æ”»å‡»æ€§
        ])
        self.samples_per_bin = self.config.get('samples_per_bin', 200)  # æ¯ä¸ªæ”»å‡»æ€§ç­‰çº§é‡‡æ ·200ä¸ª
        
        # æƒ…æ„Ÿåˆ†æ•°æƒé‡ - å¯é€šè¿‡è°ƒå‚ä¼˜åŒ–
        self.emotion_weights = self.config.get('emotion_weights', {
            'comment': 0.4,    # è¯„è®ºèŠ‚ç‚¹æƒé‡
            'user': 0.3,       # ç”¨æˆ·èŠ‚ç‚¹æƒé‡  
            'word': 0.2,       # è¯æ±‡èŠ‚ç‚¹æƒé‡
            'others': 0.1      # å…¶ä»–èŠ‚ç‚¹æƒé‡
        })
        
        # æ”»å‡»æ€§è¯„è®ºè¯†åˆ«é˜ˆå€¼
        self.attack_word_ratio_threshold = self.config.get('attack_word_ratio_threshold', 0.05)
        self.attack_word_count_threshold = self.config.get('attack_word_count_threshold', 1)
        self.uppercase_ratio_threshold = self.config.get('uppercase_ratio_threshold', 0.25)
        self.exclamation_threshold = self.config.get('exclamation_threshold', 2)
        
        # æ•°æ®å­˜å‚¨
        self.graph = None
        self.session_labels = {}  # ä¼šè¯éœ¸å‡Œæ ‡ç­¾
        self.bullying_sessions = []  # éœ¸å‡Œä¼šè¯åˆ—è¡¨
        self.aggressive_comments = []  # æ”»å‡»æ€§è¯„è®ºèŠ‚ç‚¹
        self.extracted_subgraphs = []  # æå–çš„å­å›¾
        self.bullying_subgraphs = []  # ç­›é€‰åçš„éœ¸å‡Œå­å›¾
        self.prototypes = []  # æœ€ç»ˆåŸå‹
        
        # èŠ‚ç‚¹ç´¢å¼•èŒƒå›´
        self.node_ranges = {}
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿— - é¿å…é‡å¤åˆå§‹åŒ–"""
        logger_name = 'PrototypeExtractorV4Fixed'
        logger = logging.getLogger(logger_name)
        
        # å¦‚æœloggerå·²ç»æœ‰handlersï¼Œç›´æ¥è¿”å›
        if logger.handlers:
            return logger
            
        logger.setLevel(logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def load_data(self, graph_path: str, labels_path: str = None):
        """åŠ è½½å›¾æ•°æ®å’Œä¼šè¯æ ‡ç­¾"""
        self.logger.info(f"ğŸ”„ åŠ è½½æ•°æ®...")
        
        # åŠ è½½å›¾æ•°æ®
        if not self.load_graph(graph_path):
            return False
            
        # åŠ è½½ä¼šè¯æ ‡ç­¾
        if not self.load_session_labels(labels_path):
            return False
            
        return True
    
    def load_graph(self, graph_path: str):
        """åŠ è½½å›¾æ•°æ®"""
        self.logger.info(f"ğŸ”„ åŠ è½½å›¾æ•°æ®: {graph_path}")
        
        try:
            with open(graph_path, 'rb') as f:
                self.graph = pickle.load(f)
            
            self.logger.info(f"âœ… å›¾æ•°æ®åŠ è½½æˆåŠŸ")
            self.logger.info(f"   èŠ‚ç‚¹ç±»å‹: {list(self.graph.node_types)}")
            
            # è®¡ç®—èŠ‚ç‚¹ç´¢å¼•èŒƒå›´
            self._calculate_node_ranges()
            
            # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡
            total_nodes = sum(self.graph[node_type].x.size(0) for node_type in self.graph.node_types)
            total_edges = sum(self.graph[edge_type].edge_index.size(1) for edge_type in self.graph.edge_types)
            
            self.logger.info(f"   æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
            self.logger.info(f"   æ€»è¾¹æ•°: {total_edges}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å›¾æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_session_labels(self, labels_path: str = None):
        """ä»å›¾ä¸­çš„media_sessionèŠ‚ç‚¹åŠ è½½ä¼šè¯éœ¸å‡Œæ ‡ç­¾"""
        self.logger.info(f"ğŸ”„ ä»å›¾ä¸­åŠ è½½ä¼šè¯æ ‡ç­¾...")
        
        try:
            # æ£€æŸ¥å›¾ä¸­æ˜¯å¦æœ‰media_sessionèŠ‚ç‚¹
            if 'media_session' not in self.graph.node_types:
                self.logger.error("âŒ å›¾ä¸­æ²¡æœ‰media_sessionèŠ‚ç‚¹")
                return False
            
            # ä»media_sessionèŠ‚ç‚¹ç‰¹å¾ä¸­æå–æ ‡ç­¾
            media_session_features = self.graph['media_session'].x
            session_count = media_session_features.size(0)
            
            self.logger.info(f"   æ‰¾åˆ° {session_count} ä¸ªåª’ä½“ä¼šè¯èŠ‚ç‚¹")
            
            # ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯éœ¸å‡Œæ ‡ç­¾
            labels = media_session_features[:, 0].numpy()
            
            # åˆ›å»ºä¼šè¯æ ‡ç­¾æ˜ å°„
            self.session_labels = {}
            for idx, label in enumerate(labels):
                session_id = f"media_session_{idx}"
                self.session_labels[session_id] = int(label)
            
            # æå–éœ¸å‡Œä¼šè¯
            self.bullying_sessions = [
                session_id for session_id, label in self.session_labels.items() 
                if label == 1
            ]
            
            normal_sessions = [
                session_id for session_id, label in self.session_labels.items() 
                if label == 0
            ]
            
            self.logger.info(f"âœ… ä¼šè¯æ ‡ç­¾åŠ è½½æˆåŠŸ")
            self.logger.info(f"   æ€»ä¼šè¯æ•°: {len(self.session_labels)}")
            self.logger.info(f"   éœ¸å‡Œä¼šè¯: {len(self.bullying_sessions)} ({len(self.bullying_sessions)/len(self.session_labels)*100:.1f}%)")
            self.logger.info(f"   æ­£å¸¸ä¼šè¯: {len(normal_sessions)} ({len(normal_sessions)/len(self.session_labels)*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä»å›¾ä¸­åŠ è½½ä¼šè¯æ ‡ç­¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _calculate_node_ranges(self):
        """è®¡ç®—å„èŠ‚ç‚¹ç±»å‹çš„ç´¢å¼•èŒƒå›´"""
        self.logger.info("ğŸ”§ è®¡ç®—èŠ‚ç‚¹ç´¢å¼•èŒƒå›´...")
        
        start_idx = 0
        for node_type in self.graph.node_types:
            node_count = self.graph[node_type].x.size(0)
            self.node_ranges[node_type] = {
                'start': start_idx,
                'end': start_idx + node_count,
                'count': node_count
            }
            start_idx += node_count
            
            self.logger.info(f"   {node_type}: {self.node_ranges[node_type]['start']}-{self.node_ranges[node_type]['end']-1} ({node_count}ä¸ª)")
    
    def identify_aggressive_comments_in_bullying_sessions(self):
        """åœ¨éœ¸å‡Œä¼šè¯ä¸­è¯†åˆ«æ”»å‡»æ€§è¯„è®ºèŠ‚ç‚¹"""
        self.logger.info("ğŸ¯ åœ¨éœ¸å‡Œä¼šè¯ä¸­è¯†åˆ«æ”»å‡»æ€§è¯„è®º...")
        
        if 'comment' not in self.graph.node_types:
            self.logger.error("âŒ å›¾ä¸­æ²¡æœ‰è¯„è®ºèŠ‚ç‚¹")
            return []
        
        comment_features = self.graph['comment'].x
        comment_count = comment_features.size(0)
        
        # è¯„è®ºç‰¹å¾åˆ†æï¼ˆåŸºäºå®é™…æ£€æŸ¥ï¼‰
        text_lengths = comment_features[:, 0].numpy()
        word_counts = comment_features[:, 1].numpy()
        ratio_feature = comment_features[:, 2].numpy()  # å¯èƒ½æ˜¯å¤§å†™æ¯”ä¾‹æˆ–æ”»å‡»è¯æ¯”ä¾‹
        binary_feature = comment_features[:, 3].numpy()  # å¯èƒ½æ˜¯æ”»å‡»æ€§æˆ–è´Ÿé¢æƒ…æ„Ÿæ ‡è®°
        
        self.logger.info(f"   è¯„è®ºç‰¹å¾åˆ†æ:")
        self.logger.info(f"     å¹³å‡æ–‡æœ¬é•¿åº¦: {text_lengths.mean():.1f}")
        self.logger.info(f"     å¹³å‡è¯æ•°: {word_counts.mean():.1f}")
        self.logger.info(f"     æ¯”ä¾‹ç‰¹å¾å‡å€¼: {ratio_feature.mean():.3f}")
        self.logger.info(f"     äºŒå…ƒç‰¹å¾åˆ†å¸ƒ: {np.bincount(binary_feature.astype(int))}")
        
        # å¤šç»´åº¦æ”»å‡»æ€§æ£€æµ‹ï¼ˆåŸºäºå®é™…ç‰¹å¾ï¼‰
        aggressive_mask = (
            (binary_feature == 1) |  # äºŒå…ƒç‰¹å¾ä¸º1ï¼ˆå¯èƒ½è¡¨ç¤ºæ”»å‡»æ€§ï¼‰
            (ratio_feature > 0.01) |  # æ¯”ä¾‹ç‰¹å¾>1%ï¼ˆå¯èƒ½æ˜¯æ”»å‡»è¯æ¯”ä¾‹ï¼‰
            ((text_lengths > 50) & (word_counts > 8)) |  # é•¿æ–‡æœ¬ä¸”è¯æ•°å¤šï¼ˆå¯èƒ½åŒ…å«æ”»å‡»å†…å®¹ï¼‰
            (text_lengths > 80)  # éå¸¸é•¿çš„è¯„è®ºï¼ˆå¯èƒ½åŒ…å«æ”»å‡»æ€§å†…å®¹ï¼‰
        )
        
        aggressive_indices = np.where(aggressive_mask)[0]
        
        # TODO: è¿™é‡Œåº”è¯¥æ·»åŠ é€»è¾‘æ¥ç­›é€‰åªå±äºéœ¸å‡Œä¼šè¯çš„æ”»å‡»æ€§è¯„è®º
        # ç”±äºå½“å‰ç¼ºå°‘è¯„è®ºåˆ°ä¼šè¯çš„æ˜ å°„å…³ç³»ï¼Œæš‚æ—¶ä½¿ç”¨æ‰€æœ‰æ”»å‡»æ€§è¯„è®º
        
        # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
        comment_start = self.node_ranges['comment']['start']
        aggressive_global_indices = [comment_start + idx for idx in aggressive_indices]
        
        self.aggressive_comments = aggressive_global_indices
        
        self.logger.info(f"âœ… è¯†åˆ«åˆ° {len(self.aggressive_comments)} ä¸ªæ”»å‡»æ€§è¯„è®º")
        self.logger.info(f"   æ”»å‡»æ€§æ¯”ä¾‹: {len(self.aggressive_comments)}/{comment_count} = {len(self.aggressive_comments)/comment_count*100:.1f}%")
        
        return self.aggressive_comments
    
    def extract_subgraphs_from_aggressive_comments(self):
        """ä»æ”»å‡»æ€§è¯„è®ºä¸­å¿ƒæå–6-30ä¸ªèŠ‚ç‚¹çš„å­å›¾"""
        self.logger.info("ğŸ•¸ï¸ ä»æ”»å‡»æ€§è¯„è®ºä¸­å¿ƒæå–å­å›¾...")
        
        if not self.aggressive_comments:
            self.logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ”»å‡»æ€§è¯„è®º")
            return []
        
        subgraphs = []
        
        # æ„å»ºå…¨å±€é‚»æ¥è¡¨
        adj_list = self._build_adjacency_list()
        
        # å®æ–½å¤šæ ·æ€§é‡‡æ ·ç­–ç•¥
        if self.diversity_sampling:
            sampled_comments = self._diverse_sampling(self.aggressive_comments)
        else:
            # åŸæœ‰çš„éšæœºé‡‡æ ·
            max_samples = min(self.max_samples, len(self.aggressive_comments))
            sampled_comments = np.random.choice(
                self.aggressive_comments, 
                max_samples, 
                replace=False
            )
        
        self.logger.info(f"   ä»{len(self.aggressive_comments)}ä¸ªæ”»å‡»æ€§è¯„è®ºä¸­é‡‡æ ·{len(sampled_comments)}ä¸ª")
        
        for comment_idx in tqdm(sampled_comments, desc="æå–å­å›¾"):
            try:
                # åŠ¨æ€é€‰æ‹©å­å›¾å¤§å°ï¼Œå¢åŠ ç»“æ„å¤šæ ·æ€§
                target_size = self._select_diverse_subgraph_size()
                
                # ä»æ”»å‡»æ€§è¯„è®ºå¼€å§‹æ‰©å±•ï¼Œæ”¶é›†æŒ‡å®šæ•°é‡çš„èŠ‚ç‚¹
                subgraph_nodes = self._collect_diverse_subgraph_nodes(comment_idx, adj_list, target_size)
                
                if len(subgraph_nodes) < self.subgraph_size_range[0]:
                    continue
                
                # æå–å­å›¾ç»“æ„
                subgraph = self._extract_subgraph_structure(subgraph_nodes, comment_idx)
                
                if subgraph:
                    subgraphs.append(subgraph)
                    
            except Exception as e:
                self.logger.warning(f"æå–å­å›¾å¤±è´¥ (comment {comment_idx}): {e}")
                continue
        
        self.extracted_subgraphs = subgraphs
        
        self.logger.info(f"âœ… æˆåŠŸæå– {len(subgraphs)} ä¸ªå­å›¾")
        if subgraphs:
            avg_size = np.mean([sg['total_nodes'] for sg in subgraphs])
            size_range = (min([sg['total_nodes'] for sg in subgraphs]), 
                         max([sg['total_nodes'] for sg in subgraphs]))
            self.logger.info(f"   å¹³å‡å­å›¾å¤§å°: {avg_size:.1f} ä¸ªèŠ‚ç‚¹")
            self.logger.info(f"   å­å›¾å¤§å°èŒƒå›´: {size_range[0]}-{size_range[1]} ä¸ªèŠ‚ç‚¹")
        
        return subgraphs
    
    def _diverse_sampling(self, aggressive_comments: List[int]) -> List[int]:
        """å¤šæ ·æ€§é‡‡æ ·ç­–ç•¥ - å¤§å¹…æ”¹è¿›ç‰ˆ"""
        self.logger.info("ğŸ¯ æ‰§è¡Œå¤šæ ·æ€§é‡‡æ ·ç­–ç•¥...")
        
        # è®¡ç®—æ‰€æœ‰è¯„è®ºçš„æ”»å‡»æ€§åˆ†æ•°
        attack_scores = []
        valid_comments = []
        
        for comment_idx in aggressive_comments:
            try:
                attack_score = self._calculate_comment_aggressiveness(comment_idx)
                attack_scores.append(attack_score)
                valid_comments.append(comment_idx)
            except Exception as e:
                continue
        
        if not valid_comments:
            self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ”»å‡»æ€§è¯„è®º")
            return []
        
        self.logger.info(f"   æœ‰æ•ˆæ”»å‡»æ€§è¯„è®º: {len(valid_comments)}ä¸ª")
        self.logger.info(f"   æ”»å‡»æ€§åˆ†æ•°èŒƒå›´: {np.min(attack_scores):.3f} - {np.max(attack_scores):.3f}")
        
        selected_samples = []
        
        # ç­–ç•¥1: åˆ†å±‚é‡‡æ · (å¦‚æœå¯ç”¨)
        if self.stratified_sampling:
            stratified_samples = self._stratified_attack_sampling(valid_comments, attack_scores)
            selected_samples.extend(stratified_samples)
            self.logger.info(f"   åˆ†å±‚é‡‡æ ·è·å¾—: {len(stratified_samples)}ä¸ªæ ·æœ¬")
        
        # ç­–ç•¥2: è´¨é‡ä¼˜å…ˆé‡‡æ · (å¦‚æœå¯ç”¨è‡ªé€‚åº”é‡‡æ ·)
        if self.adaptive_sampling and len(selected_samples) < self.max_samples:
            remaining_quota = self.max_samples - len(selected_samples)
            quality_samples = self._quality_priority_sampling(
                valid_comments, attack_scores, remaining_quota, exclude=selected_samples
            )
            selected_samples.extend(quality_samples)
            self.logger.info(f"   è´¨é‡ä¼˜å…ˆé‡‡æ ·è·å¾—: {len(quality_samples)}ä¸ªæ ·æœ¬")
        
        # ç­–ç•¥3: éšæœºè¡¥å……é‡‡æ ·
        if len(selected_samples) < self.max_samples:
            remaining_quota = self.max_samples - len(selected_samples)
            random_samples = self._random_supplement_sampling(
                valid_comments, remaining_quota, exclude=selected_samples
            )
            selected_samples.extend(random_samples)
            self.logger.info(f"   éšæœºè¡¥å……é‡‡æ ·è·å¾—: {len(random_samples)}ä¸ªæ ·æœ¬")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        final_samples = list(set(selected_samples))[:self.max_samples]
        
        self.logger.info(f"âœ… å¤šæ ·æ€§é‡‡æ ·å®Œæˆ: {len(final_samples)}ä¸ªæ ·æœ¬")
        self.logger.info(f"   é‡‡æ ·æ¯”ä¾‹: {len(final_samples)/len(valid_comments)*100:.1f}%")
        
        return final_samples
    
    def _stratified_attack_sampling(self, comments: List[int], attack_scores: List[float]) -> List[int]:
        """åˆ†å±‚æ”»å‡»æ€§é‡‡æ ·"""
        self.logger.info("   ğŸ¯ æ‰§è¡Œåˆ†å±‚æ”»å‡»æ€§é‡‡æ ·...")
        
        # æŒ‰æ”»å‡»æ€§ç­‰çº§åˆ†ç»„
        stratified_groups = {i: [] for i in range(len(self.attack_level_bins))}
        
        for comment_idx, attack_score in zip(comments, attack_scores):
            for bin_idx, (min_attack, max_attack) in enumerate(self.attack_level_bins):
                if min_attack <= attack_score < max_attack:
                    stratified_groups[bin_idx].append(comment_idx)
                    break
        
        # ä»æ¯ä¸ªç»„ä¸­é‡‡æ ·
        selected_samples = []
        for bin_idx, (min_attack, max_attack) in enumerate(self.attack_level_bins):
            group_comments = stratified_groups[bin_idx]
            if not group_comments:
                continue
            
            # æ¯ç»„é‡‡æ ·æŒ‡å®šæ•°é‡
            sample_count = min(self.samples_per_bin, len(group_comments))
            bin_samples = np.random.choice(group_comments, sample_count, replace=False).tolist()
            selected_samples.extend(bin_samples)
            
            self.logger.info(f"     ç­‰çº§{bin_idx+1}({min_attack:.2f}-{max_attack:.2f}): {len(group_comments)}ä¸ªå€™é€‰, é‡‡æ ·{sample_count}ä¸ª")
        
        return selected_samples
    
    def _quality_priority_sampling(self, comments: List[int], attack_scores: List[float], 
                                 quota: int, exclude: List[int] = None) -> List[int]:
        """è´¨é‡ä¼˜å…ˆé‡‡æ ·"""
        exclude = exclude or []
        available_comments = [c for c in comments if c not in exclude]
        available_scores = [attack_scores[comments.index(c)] for c in available_comments]
        
        if not available_comments:
            return []
        
        # æŒ‰æ”»å‡»æ€§åˆ†æ•°æ’åº
        sorted_indices = np.argsort(available_scores)[::-1]  # é™åºæ’åˆ—
        
        # é€‰æ‹©è´¨é‡æœ€é«˜çš„æ ·æœ¬
        quality_count = int(quota * self.quality_sampling_ratio)
        quality_samples = [available_comments[i] for i in sorted_indices[:quality_count]]
        
        # éšæœºé€‰æ‹©å‰©ä½™æ ·æœ¬ä»¥å¢åŠ å¤šæ ·æ€§
        remaining_indices = sorted_indices[quality_count:]
        if remaining_indices.size > 0:
            random_count = quota - quality_count
            random_count = min(random_count, len(remaining_indices))
            random_indices = np.random.choice(remaining_indices, random_count, replace=False)
            random_samples = [available_comments[i] for i in random_indices]
            quality_samples.extend(random_samples)
        
        return quality_samples
    
    def _random_supplement_sampling(self, comments: List[int], quota: int, exclude: List[int] = None) -> List[int]:
        """éšæœºè¡¥å……é‡‡æ ·"""
        exclude = exclude or []
        available_comments = [c for c in comments if c not in exclude]
        
        if not available_comments:
            return []
        
        sample_count = min(quota, len(available_comments))
        return np.random.choice(available_comments, sample_count, replace=False).tolist()
    
    def _calculate_comment_aggressiveness(self, comment_idx: int) -> float:
        """è®¡ç®—è¯„è®ºçš„æ”»å‡»æ€§åˆ†æ•°"""
        if 'comment' not in self.graph.node_types:
            return 0.5
            
        comment_features = self.graph['comment'].x
        comment_start = self.node_ranges['comment']['start']
        relative_idx = comment_idx - comment_start
        
        if 0 <= relative_idx < comment_features.size(0):
            features = comment_features[relative_idx]
            
            # åŸºäºæ”»å‡»æ€§ç‰¹å¾è®¡ç®—åˆ†æ•°
            ratio_feature = features[2].item()  # æ”»å‡»è¯æ¯”ä¾‹
            binary_feature = features[3].item()  # æ”»å‡»æ€§æ ‡è®°
            uppercase_ratio = features[4].item() if features.size(0) > 4 else 0  # å¤§å†™æ¯”ä¾‹
            
            # ç»¼åˆæ”»å‡»æ€§åˆ†æ•° - ç§»é™¤å¼ºåˆ¶å½’ä¸€åŒ–ä»¥ä¿æŒåŒºåˆ†åº¦
            score = (ratio_feature * 2.0 + binary_feature * 1.0 + uppercase_ratio * 0.5)
            return score  # ä¸å†å¼ºåˆ¶é™åˆ¶ä¸º1.0
        
        return 0.5
    
    def _select_diverse_subgraph_size(self) -> int:
        """åŠ¨æ€é€‰æ‹©å­å›¾å¤§å°ï¼Œå¢åŠ ç»“æ„å¤šæ ·æ€§"""
        min_size, max_size = self.subgraph_size_range
        
        # å®šä¹‰ä¸‰ä¸ªå¤§å°åŒºé—´ï¼šå°å‹(6-12)ã€ä¸­å‹(13-21)ã€å¤§å‹(22-30)
        small_range = (min_size, min_size + 6)
        medium_range = (min_size + 7, min_size + 15) 
        large_range = (min_size + 16, max_size)
        
        # éšæœºé€‰æ‹©åŒºé—´ï¼ˆå„1/3æ¦‚ç‡ï¼‰
        range_choice = np.random.choice([0, 1, 2])
        
        if range_choice == 0:  # å°å‹å­å›¾
            return np.random.randint(small_range[0], small_range[1] + 1)
        elif range_choice == 1:  # ä¸­å‹å­å›¾
            return np.random.randint(medium_range[0], medium_range[1] + 1)
        else:  # å¤§å‹å­å›¾
            return np.random.randint(large_range[0], large_range[1] + 1)
    
    def _collect_diverse_subgraph_nodes(self, center_comment: int, adj_list: Dict, target_size: int) -> Set[int]:
        """æ”¶é›†å¤šæ ·åŒ–çš„å­å›¾èŠ‚ç‚¹ï¼Œæ”¯æŒä¸åŒçš„æ‰©å±•ç­–ç•¥"""
        
        subgraph_nodes = {center_comment}
        candidates = set()
        
        # æ·»åŠ ä¸­å¿ƒè¯„è®ºçš„ç›´æ¥é‚»å±…
        if center_comment in adj_list:
            for neighbor in adj_list[center_comment]:
                candidates.add(neighbor)
        
        # æ ¹æ®ç›®æ ‡å¤§å°é€‰æ‹©ä¸åŒçš„æ‰©å±•ç­–ç•¥
        if target_size <= 12:
            # å°å‹å­å›¾ï¼šç´§å¯†è¿æ¥ï¼Œé€‰æ‹©é«˜ä¼˜å…ˆçº§èŠ‚ç‚¹
            strategy = "compact"
        elif target_size <= 21:
            # ä¸­å‹å­å›¾ï¼šå¹³è¡¡æ‰©å±•
            strategy = "balanced"
        else:
            # å¤§å‹å­å›¾ï¼šå¹¿æ³›æ‰©å±•ï¼ŒåŒ…å«æ›´å¤šèŠ‚ç‚¹ç±»å‹
            strategy = "extensive"
        
        # æŒ‰ç­–ç•¥æ‰©å±•å­å›¾
        while len(subgraph_nodes) < target_size and candidates:
            if strategy == "compact":
                next_node = self._select_priority_node(candidates, subgraph_nodes)
            elif strategy == "balanced":
                next_node = self._select_balanced_node(candidates, subgraph_nodes)
            else:  # extensive
                next_node = self._select_diverse_node(candidates, subgraph_nodes)
                
            if next_node is None:
                break
                
            subgraph_nodes.add(next_node)
            candidates.remove(next_node)
            
            # æ·»åŠ æ–°èŠ‚ç‚¹çš„é‚»å±…ä½œä¸ºå€™é€‰
            if next_node in adj_list:
                for neighbor in adj_list[next_node]:
                    if neighbor not in subgraph_nodes:
                        candidates.add(neighbor)
        
        return subgraph_nodes
    
    def _select_balanced_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """å¹³è¡¡é€‰æ‹©ç­–ç•¥ï¼šåœ¨ä¼˜å…ˆçº§å’Œå¤šæ ·æ€§ä¹‹é—´å¹³è¡¡"""
        # 50%æ¦‚ç‡æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ï¼Œ50%æ¦‚ç‡éšæœºé€‰æ‹©
        if np.random.random() < 0.5:
            return self._select_priority_node(candidates, existing_nodes)
        else:
            return next(iter(candidates)) if candidates else None
    
    def _select_diverse_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """å¤šæ ·æ€§é€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©å›¾ä¸­ç¼ºå°‘çš„èŠ‚ç‚¹ç±»å‹"""
        existing_types = {self._get_node_type(node) for node in existing_nodes}
        
        # ä¼˜å…ˆé€‰æ‹©å½“å‰å­å›¾ä¸­ç¼ºå°‘çš„èŠ‚ç‚¹ç±»å‹
        for node_type in ['word', 'time', 'location', 'user', 'comment']:
            if node_type not in existing_types:
                type_candidates = [
                    node for node in candidates 
                    if self._get_node_type(node) == node_type
                ]
                if type_candidates:
                    return type_candidates[0]
        
        # å¦‚æœæ‰€æœ‰ç±»å‹éƒ½æœ‰ï¼Œéšæœºé€‰æ‹©
        return next(iter(candidates)) if candidates else None
    
    def _select_priority_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼šè¯„è®º > ç”¨æˆ· > è¯æ±‡ > å…¶ä»–"""
        priority_order = ['comment', 'user', 'word', 'time', 'location']
        
        for node_type in priority_order:
            type_candidates = [
                node for node in candidates 
                if self._get_node_type(node) == node_type
            ]
            if type_candidates:
                return type_candidates[0]
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆç±»å‹ï¼Œè¿”å›ä»»æ„å€™é€‰
        return next(iter(candidates)) if candidates else None

    def calculate_emotion_scores(self):
        """è®¡ç®—æ¯ä¸ªå­å›¾çš„æƒ…æ„Ÿåˆ†æ•°"""
        self.logger.info("ğŸ˜Š è®¡ç®—å­å›¾æƒ…æ„Ÿåˆ†æ•°...")
        
        if not self.extracted_subgraphs:
            self.logger.error("âŒ æ²¡æœ‰æå–çš„å­å›¾")
            return
        
        for i, subgraph in enumerate(tqdm(self.extracted_subgraphs, desc="è®¡ç®—æƒ…æ„Ÿåˆ†æ•°")):
            try:
                emotion_score = self._calculate_subgraph_emotion_score(subgraph)
                subgraph['emotion_score'] = emotion_score
                
            except Exception as e:
                self.logger.warning(f"è®¡ç®—å­å›¾{i}æƒ…æ„Ÿåˆ†æ•°å¤±è´¥: {e}")
                subgraph['emotion_score'] = 0.0
        
        # ç»Ÿè®¡æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒ
        emotion_scores = [sg.get('emotion_score', 0.0) for sg in self.extracted_subgraphs]
        if emotion_scores:
            self.logger.info(f"âœ… æƒ…æ„Ÿåˆ†æ•°è®¡ç®—å®Œæˆ")
            self.logger.info(f"   å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {np.mean(emotion_scores):.3f}")
            self.logger.info(f"   æƒ…æ„Ÿåˆ†æ•°èŒƒå›´: {np.min(emotion_scores):.3f} - {np.max(emotion_scores):.3f}")
            self.logger.info(f"   è´Ÿé¢æƒ…æ„Ÿå­å›¾: {sum(1 for s in emotion_scores if s < 0)}/{len(emotion_scores)}")

    def _calculate_subgraph_emotion_score(self, subgraph: Dict) -> float:
        """è®¡ç®—å•ä¸ªå­å›¾çš„æƒ…æ„Ÿåˆ†æ•°"""
        nodes_by_type = subgraph.get('nodes_by_type', {})
        total_weighted_score = 0.0
        total_weight = 0.0
        
        # ä¸ºä¸åŒç±»å‹çš„èŠ‚ç‚¹è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        for node_type, node_indices in nodes_by_type.items():
            if not node_indices:
                continue
                
            node_weight = self.emotion_weights.get(node_type, self.emotion_weights['others'])
            type_emotion_score = self._calculate_node_type_emotion_score(node_type, node_indices)
            
            total_weighted_score += type_emotion_score * node_weight * len(node_indices)
            total_weight += node_weight * len(node_indices)
        
        # åŠ æƒå¹³å‡
        if total_weight > 0:
            return total_weighted_score / total_weight
        else:
            return 0.0

    def _calculate_node_type_emotion_score(self, node_type: str, node_indices: List[int]) -> float:
        """è®¡ç®—ç‰¹å®šç±»å‹èŠ‚ç‚¹çš„æƒ…æ„Ÿåˆ†æ•°"""
        if node_type == 'comment':
            return self._calculate_comment_emotion_scores(node_indices)
        elif node_type == 'user':
            return self._calculate_user_emotion_scores(node_indices)
        elif node_type == 'word':
            return self._calculate_word_emotion_scores(node_indices)
        else:
            # æ—¶é—´ã€ä½ç½®ç­‰èŠ‚ç‚¹ä¸ç»™æƒ…æ„Ÿåˆ†æ•°
            return 0.0

    def _calculate_comment_emotion_scores(self, comment_indices: List[int]) -> float:
        """åŸºäºè¯„è®ºæ–‡æœ¬ç‰¹å¾è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"""
        if 'comment' not in self.graph.node_types:
            return 0.0
            
        comment_features = self.graph['comment'].x
        comment_start = self.node_ranges['comment']['start']
        
        scores = []
        for global_idx in comment_indices:
            relative_idx = global_idx - comment_start
            if 0 <= relative_idx < comment_features.size(0):
                features = comment_features[relative_idx]
                
                # åŸºäºæ”»å‡»æ€§ç‰¹å¾è®¡ç®—è´Ÿé¢æƒ…æ„Ÿåˆ†æ•°
                ratio_feature = features[2].item()  # æ”»å‡»è¯æ¯”ä¾‹
                binary_feature = features[3].item()  # æ”»å‡»æ€§æ ‡è®°
                
                # æƒ…æ„Ÿåˆ†æ•°ï¼šæ”»å‡»æ€§è¶Šé«˜ï¼Œåˆ†æ•°è¶Šè´Ÿé¢
                emotion_score = -(ratio_feature * 2.0 + binary_feature * 1.0)
                scores.append(emotion_score)
        
        return np.mean(scores) if scores else 0.0

    def _calculate_user_emotion_scores(self, user_indices: List[int]) -> float:
        """åŸºäºç”¨æˆ·å‘è¡¨çš„æ”»å‡»æ€§è¨€è®ºæ•°é‡è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"""
        if 'user' not in self.graph.node_types:
            return 0.0
        
        # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ”»å‡»æ€§è¯„è®ºæ•°é‡
        user_scores = []
        
        for user_global_idx in user_indices:
            # è®¡ç®—è¯¥ç”¨æˆ·å‘è¡¨çš„æ”»å‡»æ€§è¯„è®ºæ•°é‡
            attack_comment_count = self._count_user_aggressive_comments(user_global_idx)
            
            # æ ¹æ®æ”»å‡»æ€§è¯„è®ºæ•°é‡è®¡ç®—é£é™©åˆ†æ•°
            if attack_comment_count == 0:
                risk_score = 0.0  # ä½é£é™©
            elif attack_comment_count <= 2:
                risk_score = -0.3 * attack_comment_count  # ä¸­é£é™©
            elif attack_comment_count <= 5:
                risk_score = -0.6 - 0.2 * (attack_comment_count - 2)  # é«˜é£é™©
            else:
                risk_score = -1.5 - 0.1 * min(attack_comment_count - 5, 10)  # æé«˜é£é™©
                
            user_scores.append(risk_score)
        
        return np.mean(user_scores) if user_scores else 0.0

    def _count_user_aggressive_comments(self, user_global_idx: int) -> int:
        """è®¡ç®—ç”¨æˆ·å‘è¡¨çš„æ”»å‡»æ€§è¯„è®ºæ•°é‡"""
        # é€šè¿‡user_posts_commentè¾¹æ‰¾åˆ°ç”¨æˆ·å‘è¡¨çš„è¯„è®º
        if ('user', 'posts', 'comment') not in self.graph.edge_types:
            return 0
        
        edge_index = self.graph[('user', 'posts', 'comment')].edge_index
        user_start = self.node_ranges['user']['start']
        user_relative_idx = user_global_idx - user_start
        
        # æ‰¾åˆ°è¯¥ç”¨æˆ·å‘è¡¨çš„æ‰€æœ‰è¯„è®º
        user_comment_mask = (edge_index[0] == user_relative_idx)
        user_comment_relative_indices = edge_index[1][user_comment_mask].numpy()
        
        # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
        comment_start = self.node_ranges['comment']['start']
        user_comment_global_indices = [comment_start + idx for idx in user_comment_relative_indices]
        
        # è®¡ç®—å…¶ä¸­æœ‰å¤šå°‘æ˜¯æ”»å‡»æ€§è¯„è®º
        attack_count = sum(1 for idx in user_comment_global_indices if idx in self.aggressive_comments)
        
        return attack_count

    def _calculate_word_emotion_scores(self, word_indices: List[int]) -> float:
        """åŸºäºè¯æ±‡çš„æ”»å‡»æ€§è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"""
        # ç®€åŒ–å®ç°ï¼šå‡è®¾è¯æ±‡ä¸­æœ‰æ”»å‡»æ€§è¯æ±‡åˆ™ç»™è´Ÿåˆ†
        # å®é™…å®ç°ä¸­å¯ä»¥å»ºç«‹æ”»å‡»æ€§è¯æ±‡è¡¨è¿›è¡ŒåŒ¹é…
        return -0.2  # å›ºå®šçš„è½»å¾®è´Ÿé¢åˆ†æ•°

    def filter_bullying_subgraphs(self):
        """åŸºäºæƒ…æ„Ÿé˜ˆå€¼ç­›é€‰éœ¸å‡Œå­å›¾"""
        self.logger.info("ğŸ” åŸºäºæƒ…æ„Ÿé˜ˆå€¼ç­›é€‰éœ¸å‡Œå­å›¾...")
        
        if not self.extracted_subgraphs:
            self.logger.error("âŒ æ²¡æœ‰æå–çš„å­å›¾")
            return []
        
        # ç­›é€‰æƒ…æ„Ÿåˆ†æ•°ä½äºé˜ˆå€¼çš„å­å›¾
        self.bullying_subgraphs = [
            subgraph for subgraph in self.extracted_subgraphs
            if subgraph.get('emotion_score', 0.0) < self.emotion_threshold
        ]
        
        self.logger.info(f"âœ… éœ¸å‡Œå­å›¾ç­›é€‰å®Œæˆ")
        self.logger.info(f"   åŸå§‹å­å›¾æ•°: {len(self.extracted_subgraphs)}")
        self.logger.info(f"   éœ¸å‡Œå­å›¾æ•°: {len(self.bullying_subgraphs)}")
        self.logger.info(f"   ç­›é€‰æ¯”ä¾‹: {len(self.bullying_subgraphs)/len(self.extracted_subgraphs)*100:.1f}%")
        
        if self.bullying_subgraphs:
            emotion_scores = [sg['emotion_score'] for sg in self.bullying_subgraphs]
            self.logger.info(f"   éœ¸å‡Œå­å›¾æƒ…æ„Ÿåˆ†æ•°èŒƒå›´: {np.min(emotion_scores):.3f} - {np.max(emotion_scores):.3f}")
        
        return self.bullying_subgraphs

    def _build_adjacency_list(self):
        """æ„å»ºå…¨å±€é‚»æ¥è¡¨"""
        self.logger.info("   ğŸ“‹ æ„å»ºé‚»æ¥è¡¨...")
        
        adj_list = defaultdict(set)
        
        for edge_type in self.graph.edge_types:
            edge_index = self.graph[edge_type].edge_index
            if edge_index.size(1) == 0:
                continue
                
            src_indices = edge_index[0].numpy()
            dst_indices = edge_index[1].numpy()
            
            for src, dst in zip(src_indices, dst_indices):
                adj_list[src].add(dst)
                adj_list[dst].add(src)  # æ— å‘å›¾
        
        self.logger.info(f"   âœ… é‚»æ¥è¡¨æ„å»ºå®Œæˆ - {len(adj_list)} ä¸ªèŠ‚ç‚¹æœ‰é‚»å±…")
        return adj_list
    
    def _extract_subgraph_structure(self, subgraph_nodes: Set[int], center_comment: int) -> Dict:
        """æå–å­å›¾çš„ç»“æ„ä¿¡æ¯"""
        
        # æŒ‰èŠ‚ç‚¹ç±»å‹åˆ†ç±»
        nodes_by_type = defaultdict(list)
        for node_idx in subgraph_nodes:
            node_type = self._get_node_type(node_idx)
            nodes_by_type[node_type].append(node_idx)
        
        # æå–å­å›¾çš„è¾¹
        subgraph_edges = self._extract_subgraph_edges(subgraph_nodes)
        
        # è®¡ç®—ç»“æ„ç‰¹å¾
        structural_features = self._calculate_subgraph_features(nodes_by_type, subgraph_edges)
        
        # è®¡ç®—æƒ…æ„Ÿç‰¹å¾
        emotion_features = self._calculate_emotion_features(nodes_by_type)
        
        # è®¡ç®—æ”»å‡»æ€§ç‰¹å¾
        aggression_features = self._calculate_aggression_features(nodes_by_type)
        
        # è®¡ç®—éœ¸å‡Œå¼ºåº¦
        bullying_intensity = self._calculate_bullying_intensity(
            nodes_by_type, center_comment, emotion_features, aggression_features
        )
        
        subgraph = {
            'center_comment': center_comment,
            'nodes_by_type': dict(nodes_by_type),
            'edges': subgraph_edges,
            'structural_features': structural_features,
            'emotion_features': emotion_features,
            'aggression_features': aggression_features,
            'bullying_intensity': bullying_intensity,
            'total_nodes': len(subgraph_nodes),
            'creation_time': datetime.now().isoformat()
        }
        
        return subgraph
    
    def _get_node_type(self, node_idx: int) -> str:
        """æ ¹æ®èŠ‚ç‚¹ç´¢å¼•ç¡®å®šèŠ‚ç‚¹ç±»å‹"""
        for node_type, range_info in self.node_ranges.items():
            if range_info['start'] <= node_idx < range_info['end']:
                return node_type
        return 'unknown'
    
    def _extract_subgraph_edges(self, subgraph_nodes: Set[int]) -> Dict:
        """æå–å­å›¾å†…çš„è¾¹"""
        subgraph_edges = {}
        
        for edge_type in self.graph.edge_types:
            edge_index = self.graph[edge_type].edge_index
            if edge_index.size(1) == 0:
                continue
            
            src_indices = edge_index[0].numpy()
            dst_indices = edge_index[1].numpy()
            
            # æ‰¾åˆ°å­å›¾å†…çš„è¾¹
            internal_edges = []
            for i, (src, dst) in enumerate(zip(src_indices, dst_indices)):
                if src in subgraph_nodes and dst in subgraph_nodes:
                    internal_edges.append((src, dst))
            
            if internal_edges:
                subgraph_edges[edge_type] = internal_edges
        
        return subgraph_edges
    
    def _calculate_subgraph_features(self, nodes_by_type: Dict, edges: Dict) -> Dict:
        """è®¡ç®—å­å›¾ç»“æ„ç‰¹å¾"""
        features = {}
        
        # èŠ‚ç‚¹ç»Ÿè®¡
        features['node_counts'] = {k: len(v) for k, v in nodes_by_type.items()}
        features['total_nodes'] = sum(features['node_counts'].values())
        
        # è¾¹ç»Ÿè®¡
        features['edge_counts'] = {k: len(v) for k, v in edges.items()}
        features['total_edges'] = sum(features['edge_counts'].values())
        
        # å¯†åº¦
        total_nodes = features['total_nodes']
        if total_nodes > 1:
            max_edges = total_nodes * (total_nodes - 1) / 2
            features['density'] = features['total_edges'] / max_edges
        else:
            features['density'] = 0.0
        
        # å¤šæ ·æ€§
        features['node_type_diversity'] = len(features['node_counts'])
        features['edge_type_diversity'] = len(features['edge_counts'])
        
        return features
    
    def _calculate_emotion_features(self, nodes_by_type: Dict) -> Dict:
        """è®¡ç®—æƒ…æ„Ÿç‰¹å¾"""
        emotion_features = {}
        
        # åŸºäºè¯„è®ºèŠ‚ç‚¹è®¡ç®—æƒ…æ„Ÿ
        comment_nodes = nodes_by_type.get('comment', [])
        if comment_nodes and 'comment' in self.graph.node_types:
            comment_features = self.graph['comment'].x
            comment_start = self.node_ranges['comment']['start']
            
            # è½¬æ¢ä¸ºç›¸å¯¹ç´¢å¼•
            relative_indices = [idx - comment_start for idx in comment_nodes 
                              if comment_start <= idx < self.node_ranges['comment']['end']]
            
            if relative_indices:
                selected_features = comment_features[relative_indices]
                
                # åŸºäºå®é™…ç‰¹å¾è®¡ç®—æƒ…æ„Ÿ
                text_lengths = selected_features[:, 0]
                word_counts = selected_features[:, 1]
                ratio_feature = selected_features[:, 2]
                binary_feature = selected_features[:, 3]
                
                emotion_features['avg_text_length'] = float(text_lengths.mean())
                emotion_features['avg_word_count'] = float(word_counts.mean())
                emotion_features['avg_ratio_feature'] = float(ratio_feature.mean())
                emotion_features['negative_ratio'] = float((binary_feature == 1).sum() / len(binary_feature))
                emotion_features['negative_intensity'] = float(
                    (text_lengths / 50.0 + ratio_feature * 5.0 + binary_feature).mean()
                )
                emotion_features['comment_count'] = len(relative_indices)
        
        return emotion_features
    
    def _calculate_aggression_features(self, nodes_by_type: Dict) -> Dict:
        """è®¡ç®—æ”»å‡»æ€§ç‰¹å¾"""
        aggression_features = {}
        
        # åŸºäºè¯„è®ºèŠ‚ç‚¹è®¡ç®—æ”»å‡»æ€§
        comment_nodes = nodes_by_type.get('comment', [])
        word_nodes = nodes_by_type.get('word', [])
        
        if comment_nodes and 'comment' in self.graph.node_types:
            comment_features = self.graph['comment'].x
            comment_start = self.node_ranges['comment']['start']
            
            relative_indices = [idx - comment_start for idx in comment_nodes 
                              if comment_start <= idx < self.node_ranges['comment']['end']]
            
            if relative_indices:
                selected_features = comment_features[relative_indices]
                
                # åŸºäºå®é™…ç‰¹å¾è®¡ç®—æ”»å‡»æ€§
                text_lengths = selected_features[:, 0]
                word_counts = selected_features[:, 1]
                ratio_feature = selected_features[:, 2]  # å¯èƒ½æ˜¯æ”»å‡»è¯æ¯”ä¾‹
                binary_feature = selected_features[:, 3]  # å¯èƒ½æ˜¯æ”»å‡»æ€§æ ‡è®°
                
                aggression_features['avg_text_length'] = float(text_lengths.mean())
                aggression_features['avg_word_count'] = float(word_counts.mean())
                aggression_features['avg_ratio_feature'] = float(ratio_feature.mean())
                aggression_features['max_ratio_feature'] = float(ratio_feature.max())
                aggression_features['aggressive_comment_ratio'] = float(
                    (binary_feature == 1).sum() / len(binary_feature)
                )
                aggression_features['aggression_score'] = float(
                    (text_lengths / 20.0 + ratio_feature * 10.0 + binary_feature * 2.0).mean()
                )
        
        # åŸºäºè¯æ±‡èŠ‚ç‚¹è®¡ç®—æ”»å‡»è¯å¯†åº¦
        if word_nodes and 'word' in self.graph.node_types:
            word_features = self.graph['word'].x
            word_start = self.node_ranges['word']['start']
            
            relative_indices = [idx - word_start for idx in word_nodes 
                              if word_start <= idx < self.node_ranges['word']['end']]
            
            if relative_indices:
                selected_features = word_features[relative_indices]
                # è¯æ±‡ç‰¹å¾ï¼š[é¢‘ç‡, æ˜¯å¦æ”»å‡»è¯, é•¿åº¦, å¸¸é‡]
                if selected_features.size(1) >= 2:
                    attack_word_ratio = float(selected_features[:, 1].mean())
                    aggression_features['attack_word_density'] = attack_word_ratio
                    aggression_features['vocab_size'] = len(relative_indices)
        
        return aggression_features
    
    def _calculate_bullying_intensity(self, nodes_by_type: Dict, center_comment: int, 
                                    emotion_features: Dict, aggression_features: Dict) -> float:
        """è®¡ç®—éœ¸å‡Œå¼ºåº¦åˆ†æ•°"""
        
        intensity_factors = []
        
        # 1. æ”»å‡»æ€§å¼ºåº¦ (æƒé‡ 40%)
        aggression_score = aggression_features.get('aggression_score', 0)
        if aggression_score > 0:
            intensity_factors.append(min(aggression_score / 5.0, 1.0) * 0.4)
        
        # 2. æƒ…æ„Ÿè´Ÿé¢åº¦ (æƒé‡ 30%)
        negative_intensity = emotion_features.get('negative_intensity', 0)
        if negative_intensity > 0:
            intensity_factors.append(min(negative_intensity / 3.0, 1.0) * 0.3)
        
        # 3. å‚ä¸ç”¨æˆ·æ•° (æƒé‡ 20%)
        user_count = nodes_by_type.get('user', [])
        if len(user_count) > 1:
            user_factor = min(len(user_count) / 10.0, 1.0) * 0.2
            intensity_factors.append(user_factor)
        
        # 4. ç»“æ„å¤æ‚æ€§ (æƒé‡ 10%)
        total_nodes = sum(len(nodes) for nodes in nodes_by_type.values())
        if total_nodes > 10:
            complexity_factor = min((total_nodes - 10) / 50.0, 1.0) * 0.1
            intensity_factors.append(complexity_factor)
        
        return sum(intensity_factors) if intensity_factors else 0.0
    
    def cluster_bullying_subgraphs(self):
        """å¯¹éœ¸å‡Œå­å›¾è¿›è¡Œèšç±»ä»¥ç”ŸæˆåŸå‹ - å¤šæ ·åŒ–ç­–ç•¥"""
        self.logger.info("ğŸ¤– å¯¹éœ¸å‡Œå­å›¾è¿›è¡Œèšç±»...")
        
        if not self.bullying_subgraphs:
            self.logger.error("âŒ æ²¡æœ‰éœ¸å‡Œå­å›¾ç”¨äºèšç±»")
            return []
        
        # æå–ç‰¹å¾å‘é‡ç”¨äºèšç±»
        feature_vectors = []
        valid_subgraphs = []
        
        for subgraph in self.bullying_subgraphs:
            try:
                feature_vector = self._extract_feature_vector(subgraph)
                if feature_vector is not None and len(feature_vector) > 0:
                    feature_vectors.append(feature_vector)
                    valid_subgraphs.append(subgraph)
            except Exception as e:
                self.logger.warning(f"æå–ç‰¹å¾å‘é‡å¤±è´¥: {e}")
                continue
        
        if len(feature_vectors) < self.min_cluster_size:
            self.logger.warning(f"æœ‰æ•ˆå­å›¾æ•°é‡ä¸è¶³è¿›è¡Œèšç±» ({len(feature_vectors)} < {self.min_cluster_size})")
            # å¦‚æœå­å›¾æ•°é‡å¤ªå°‘ï¼Œå°†æ‰€æœ‰å­å›¾ä½œä¸ºä¸€ä¸ªåŸå‹
            if valid_subgraphs:
                prototype = self._generate_prototype(0, valid_subgraphs)
                self.prototypes = [prototype]
                self.logger.info(f"âœ… ç”Ÿæˆå•ä¸€åŸå‹åŒ…å« {len(valid_subgraphs)} ä¸ªå­å›¾")
                return self.prototypes
            else:
                return []
        
        # æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
        feature_matrix = np.array(feature_vectors)
        self.logger.info(f"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {feature_matrix.shape}")
        
        # ä½¿ç”¨å¤šç§èšç±»ç­–ç•¥ç”Ÿæˆå¤šæ ·åŒ–åŸå‹
        all_prototypes = []
        
        # ç­–ç•¥1: åŸºäºç»“æ„ç›¸ä¼¼æ€§çš„DBSCANèšç±»
        structural_prototypes = self._dbscan_clustering(feature_matrix, valid_subgraphs, "ç»“æ„ç›¸ä¼¼æ€§")
        all_prototypes.extend(structural_prototypes)
        
        # ç­–ç•¥2: åŸºäºå­å›¾å¤§å°çš„åˆ†å±‚èšç±»
        size_prototypes = self._size_based_clustering(valid_subgraphs)
        all_prototypes.extend(size_prototypes)
        
        # ç­–ç•¥3: åŸºäºæ”»å‡»æ€§å¼ºåº¦çš„åˆ†å±‚èšç±»
        intensity_prototypes = self._intensity_based_clustering(valid_subgraphs)
        all_prototypes.extend(intensity_prototypes)
        
        # å»é‡å’Œåˆå¹¶ç›¸ä¼¼åŸå‹
        final_prototypes = self._merge_similar_prototypes(all_prototypes)
        
        self.prototypes = final_prototypes
        self.logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(final_prototypes)} ä¸ªå¤šæ ·åŒ–åŸå‹")
        
        # æ˜¾ç¤ºåŸå‹å¤šæ ·æ€§ç»Ÿè®¡
        self._log_prototype_diversity(final_prototypes)
        
        return final_prototypes
    
    def _dbscan_clustering(self, feature_matrix: np.ndarray, valid_subgraphs: List[Dict], strategy_name: str) -> List[Dict]:
        """ä½¿ç”¨DBSCANè¿›è¡Œèšç±»"""
        clustering = DBSCAN(eps=self.clustering_eps, min_samples=self.min_cluster_size)
        cluster_labels = clustering.fit_predict(feature_matrix)
        
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        self.logger.info(f"   {strategy_name}èšç±»ç»“æœ: {n_clusters} ä¸ªèšç±», {n_noise} ä¸ªå™ªå£°ç‚¹")
        
        prototypes = []
        for cluster_id in unique_labels:
            if cluster_id == -1:  # è·³è¿‡å™ªå£°ç‚¹
                continue
            
            cluster_mask = (cluster_labels == cluster_id)
            cluster_subgraphs = [valid_subgraphs[i] for i, mask in enumerate(cluster_mask) if mask]
            
            if len(cluster_subgraphs) >= self.min_cluster_size:
                prototype = self._generate_prototype(f"struct_{cluster_id}", cluster_subgraphs)
                prototype['clustering_strategy'] = strategy_name
                prototypes.append(prototype)
                self.logger.info(f"   {strategy_name}èšç±» {cluster_id}: {len(cluster_subgraphs)} ä¸ªå­å›¾")
        
        return prototypes
    
    def _size_based_clustering(self, valid_subgraphs: List[Dict]) -> List[Dict]:
        """åŸºäºå­å›¾å¤§å°è¿›è¡Œåˆ†å±‚èšç±»"""
        self.logger.info("   åŸºäºå¤§å°çš„åˆ†å±‚èšç±»...")
        
        # æŒ‰å­å›¾å¤§å°åˆ†ç»„
        size_groups = {'small': [], 'medium': [], 'large': []}
        
        for subgraph in valid_subgraphs:
            size = subgraph['total_nodes']
            if size <= 12:
                size_groups['small'].append(subgraph)
            elif size <= 21:
                size_groups['medium'].append(subgraph)
            else:
                size_groups['large'].append(subgraph)
        
        prototypes = []
        for size_type, subgraphs in size_groups.items():
            if len(subgraphs) >= self.min_cluster_size:
                prototype = self._generate_prototype(f"size_{size_type}", subgraphs)
                prototype['clustering_strategy'] = f"åŸºäºå¤§å°({size_type})"
                prototypes.append(prototype)
                self.logger.info(f"   å¤§å°èšç±» {size_type}: {len(subgraphs)} ä¸ªå­å›¾")
        
        return prototypes
    
    def _intensity_based_clustering(self, valid_subgraphs: List[Dict]) -> List[Dict]:
        """åŸºäºæ”»å‡»æ€§å¼ºåº¦è¿›è¡Œåˆ†å±‚èšç±»"""
        self.logger.info("   åŸºäºæ”»å‡»æ€§å¼ºåº¦çš„åˆ†å±‚èšç±»...")
        
        # è®¡ç®—æ”»å‡»æ€§å¼ºåº¦åˆ†å¸ƒ
        intensities = [sg.get('bullying_intensity', 0) for sg in valid_subgraphs]
        if not intensities:
            return []
        
        # æŒ‰æ”»å‡»æ€§å¼ºåº¦åˆ†ç»„
        intensity_threshold_low = np.percentile(intensities, 33)
        intensity_threshold_high = np.percentile(intensities, 67)
        
        intensity_groups = {'low': [], 'medium': [], 'high': []}
        
        for subgraph in valid_subgraphs:
            intensity = subgraph.get('bullying_intensity', 0)
            if intensity <= intensity_threshold_low:
                intensity_groups['low'].append(subgraph)
            elif intensity <= intensity_threshold_high:
                intensity_groups['medium'].append(subgraph)
            else:
                intensity_groups['high'].append(subgraph)
        
        prototypes = []
        for intensity_type, subgraphs in intensity_groups.items():
            if len(subgraphs) >= self.min_cluster_size:
                prototype = self._generate_prototype(f"intensity_{intensity_type}", subgraphs)
                prototype['clustering_strategy'] = f"åŸºäºå¼ºåº¦({intensity_type})"
                prototypes.append(prototype)
                self.logger.info(f"   å¼ºåº¦èšç±» {intensity_type}: {len(subgraphs)} ä¸ªå­å›¾")
        
        return prototypes
    
    def _merge_similar_prototypes(self, all_prototypes: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç›¸ä¼¼çš„åŸå‹ï¼Œé¿å…é‡å¤"""
        if len(all_prototypes) <= 1:
            return all_prototypes
        
        # è®¡ç®—åŸå‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_threshold = 0.8  # ç›¸ä¼¼åº¦é˜ˆå€¼
        merged_prototypes = []
        used_indices = set()
        
        for i, proto1 in enumerate(all_prototypes):
            if i in used_indices:
                continue
                
            similar_prototypes = [proto1]
            used_indices.add(i)
            
            for j, proto2 in enumerate(all_prototypes[i+1:], i+1):
                if j in used_indices:
                    continue
                    
                # è®¡ç®—ä¸¤ä¸ªåŸå‹çš„ç›¸ä¼¼åº¦
                similarity = self._calculate_prototype_similarity(proto1, proto2)
                if similarity > similarity_threshold:
                    similar_prototypes.append(proto2)
                    used_indices.add(j)
            
            # å¦‚æœæœ‰ç›¸ä¼¼åŸå‹ï¼Œåˆå¹¶å®ƒä»¬
            if len(similar_prototypes) > 1:
                merged_prototype = self._merge_prototypes(similar_prototypes)
                merged_prototypes.append(merged_prototype)
            else:
                merged_prototypes.append(proto1)
        
        self.logger.info(f"   åŸå‹åˆå¹¶: {len(all_prototypes)} -> {len(merged_prototypes)}")
        return merged_prototypes
    
    def _calculate_prototype_similarity(self, proto1: Dict, proto2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªåŸå‹ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
        try:
            features1 = np.array(proto1['average_features'])
            features2 = np.array(proto2['average_features'])
            
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity([features1], [features2])[0][0]
            return similarity
        except:
            return 0.0
    
    def _merge_prototypes(self, similar_prototypes: List[Dict]) -> Dict:
        """åˆå¹¶ç›¸ä¼¼çš„åŸå‹"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŸå‹ä½œä¸ºåŸºç¡€
        merged_prototype = similar_prototypes[0].copy()
        
        # æ›´æ–°IDå’Œç­–ç•¥
        strategies = [p.get('clustering_strategy', 'unknown') for p in similar_prototypes]
        merged_prototype['prototype_id'] = f"merged_{len(similar_prototypes)}"
        merged_prototype['clustering_strategy'] = f"åˆå¹¶({', '.join(set(strategies))})"
        
        # åˆå¹¶èšç±»å¤§å°
        total_size = sum(p.get('cluster_size', 0) for p in similar_prototypes)
        merged_prototype['cluster_size'] = total_size
        
        # åˆå¹¶æˆå‘˜è¯„è®º
        all_comments = []
        for p in similar_prototypes:
            all_comments.extend(p.get('member_comments', []))
        merged_prototype['member_comments'] = list(set(all_comments))
        
        return merged_prototype
    
    def _log_prototype_diversity(self, prototypes: List[Dict]):
        """è®°å½•åŸå‹å¤šæ ·æ€§ç»Ÿè®¡"""
        if not prototypes:
            return
            
        # ç»Ÿè®¡ä¸åŒç­–ç•¥çš„åŸå‹æ•°é‡
        strategies = {}
        sizes = []
        intensities = []
        
        for proto in prototypes:
            strategy = proto.get('clustering_strategy', 'unknown')
            strategies[strategy] = strategies.get(strategy, 0) + 1
            sizes.append(proto.get('cluster_size', 0))
            intensities.append(proto.get('avg_bullying_intensity', 0))
        
        self.logger.info("   ğŸŒˆ åŸå‹å¤šæ ·æ€§ç»Ÿè®¡:")
        for strategy, count in strategies.items():
            self.logger.info(f"     {strategy}: {count} ä¸ªåŸå‹")
        
        if sizes:
            self.logger.info(f"     å¹³å‡èšç±»å¤§å°: {np.mean(sizes):.1f}")
            self.logger.info(f"     èšç±»å¤§å°èŒƒå›´: {min(sizes)}-{max(sizes)}")
        
        if intensities:
            self.logger.info(f"     å¹³å‡éœ¸å‡Œå¼ºåº¦: {np.mean(intensities):.3f}")
            self.logger.info(f"     å¼ºåº¦èŒƒå›´: {min(intensities):.3f}-{max(intensities):.3f}")
    
    def _extract_feature_vector(self, subgraph: Dict) -> np.ndarray:
        """æå–å­å›¾çš„ç‰¹å¾å‘é‡ç”¨äºèšç±»"""
        features = []
        
        # ç»“æ„ç‰¹å¾
        struct = subgraph['structural_features']
        features.extend([
            struct.get('total_nodes', 0),
            struct.get('total_edges', 0),
            struct.get('density', 0),
            struct.get('node_type_diversity', 0),
            struct.get('edge_type_diversity', 0)
        ])
        
        # èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        node_counts = struct.get('node_counts', {})
        for node_type in ['user', 'comment', 'word', 'video']:
            features.append(node_counts.get(node_type, 0))
        
        # æƒ…æ„Ÿç‰¹å¾
        emotion = subgraph['emotion_features']
        features.extend([
            emotion.get('avg_text_length', 0),
            emotion.get('avg_word_count', 0),
            emotion.get('avg_ratio_feature', 0),
            emotion.get('negative_ratio', 0),
            emotion.get('negative_intensity', 0),
            emotion.get('comment_count', 0)
        ])
        
        # æ”»å‡»æ€§ç‰¹å¾
        aggression = subgraph['aggression_features']
        features.extend([
            aggression.get('avg_text_length', 0),
            aggression.get('avg_word_count', 0),
            aggression.get('avg_ratio_feature', 0),
            aggression.get('max_ratio_feature', 0),
            aggression.get('aggressive_comment_ratio', 0),
            aggression.get('aggression_score', 0),
            aggression.get('attack_word_density', 0)
        ])
        
        # éœ¸å‡Œå¼ºåº¦
        features.append(subgraph.get('bullying_intensity', 0))
        
        return np.array(features)
    
    def _generate_prototype(self, cluster_id: int, cluster_subgraphs: List[Dict]) -> Dict:
        """ä»èšç±»ç”ŸæˆåŸå‹"""
        
        # è®¡ç®—å¹³å‡ç‰¹å¾
        feature_vectors = [self._extract_feature_vector(sg) for sg in cluster_subgraphs]
        avg_features = np.mean(feature_vectors, axis=0)
        
        # é€‰æ‹©æœ€ä»£è¡¨æ€§çš„å­å›¾
        similarities = []
        for i, features in enumerate(feature_vectors):
            sim = cosine_similarity([features], [avg_features])[0][0]
            similarities.append(sim)
        
        representative_idx = np.argmax(similarities)
        representative_subgraph = cluster_subgraphs[representative_idx]
        
        # è®¡ç®—èšç±»ç»Ÿè®¡
        bullying_intensities = [sg['bullying_intensity'] for sg in cluster_subgraphs]
        aggression_scores = [sg['aggression_features'].get('aggression_score', 0) 
                           for sg in cluster_subgraphs]
        
        # ç”ŸæˆåŸå‹
        prototype = {
            'prototype_id': cluster_id,
            'cluster_size': len(cluster_subgraphs),
            'representative_subgraph': representative_subgraph,
            'average_features': avg_features.tolist(),
            'quality_score': self._calculate_prototype_quality(cluster_subgraphs),
            'avg_bullying_intensity': float(np.mean(bullying_intensities)),
            'avg_aggression_score': float(np.mean(aggression_scores)),
            'creation_time': datetime.now().isoformat(),
            'member_comments': [sg['center_comment'] for sg in cluster_subgraphs]
        }
        
        return prototype
    
    def _calculate_prototype_quality(self, cluster_subgraphs: List[Dict]) -> float:
        """è®¡ç®—åŸå‹è´¨é‡åˆ†æ•°"""
        
        quality_factors = []
        
        # 1. èšç±»å¤§å° (æƒé‡ 25%)
        cluster_size = len(cluster_subgraphs)
        size_score = min(cluster_size / 10.0, 1.0)
        quality_factors.append(size_score * 0.25)
        
        # 2. å¹³å‡éœ¸å‡Œå¼ºåº¦ (æƒé‡ 35%)
        intensities = [sg.get('bullying_intensity', 0) for sg in cluster_subgraphs]
        if intensities:
            avg_intensity = np.mean(intensities)
            quality_factors.append(avg_intensity * 0.35)
        
        # 3. æ”»å‡»æ€§ä¸€è‡´æ€§ (æƒé‡ 25%)
        aggression_scores = [sg['aggression_features'].get('aggression_score', 0) 
                           for sg in cluster_subgraphs]
        if aggression_scores:
            # ä¸€è‡´æ€§ = 1 - å˜å¼‚ç³»æ•°
            consistency = 1.0 - (np.std(aggression_scores) / (np.mean(aggression_scores) + 1e-6))
            quality_factors.append(max(0, consistency) * 0.25)
        
        # 4. ç»“æ„å¤æ‚æ€§ (æƒé‡ 15%)
        node_counts = [sg['structural_features'].get('total_nodes', 0) 
                      for sg in cluster_subgraphs]
        if node_counts:
            avg_nodes = np.mean(node_counts)
            # æœ€ä¼˜èŠ‚ç‚¹æ•°åœ¨30-60ä¹‹é—´
            if 30 <= avg_nodes <= 60:
                complexity_score = 1.0
            elif avg_nodes < 30:
                complexity_score = avg_nodes / 30.0
            else:
                complexity_score = max(0.1, 1.0 - (avg_nodes - 60) / 40.0)
            quality_factors.append(complexity_score * 0.15)
        
        return sum(quality_factors) if quality_factors else 0.0
    
    def save_prototypes(self, output_dir: str = "ProtoBully/data/prototype_v4_fixed"):
        """ä¿å­˜åŸå‹"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå‹
        prototypes_file = output_path / f"prototypes_v4_fixed_{timestamp}.pkl"
        with open(prototypes_file, 'wb') as f:
            pickle.dump(self.prototypes, f)
        
        # ä¿å­˜å­å›¾
        subgraphs_file = output_path / f"subgraphs_v4_fixed_{timestamp}.pkl"
        with open(subgraphs_file, 'wb') as f:
            pickle.dump(self.extracted_subgraphs, f)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'timestamp': timestamp,
            'aggressive_comments': len(self.aggressive_comments),
            'extracted_subgraphs': len(self.extracted_subgraphs),
            'final_prototypes': len(self.prototypes),
            'config': self.config,
            'prototype_quality_scores': [p['quality_score'] for p in self.prototypes],
            'avg_bullying_intensities': [p['avg_bullying_intensity'] for p in self.prototypes]
        }
        
        stats_file = output_path / f"extraction_stats_v4_fixed_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… ç»“æœä¿å­˜åˆ° {output_path}")
        
        return {
            'prototypes_file': str(prototypes_file),
            'subgraphs_file': str(subgraphs_file),
            'stats_file': str(stats_file)
        }
    
    def run_full_extraction(self, graph_path: str, labels_path: str = None) -> Dict:
        """è¿è¡Œå®Œæ•´çš„åŸå‹æå–æµç¨‹"""
        self.logger.info("ğŸš€ å¼€å§‹éœ¸å‡ŒåŸå‹æå–æµç¨‹")
        self.logger.info("="*50)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.logger.info("ğŸ“ æ­¥éª¤1: åŠ è½½æ•°æ®")
            if not self.load_data(graph_path, labels_path):
                return {'success': False, 'error': 'Data loading failed'}
            
            # 2. è¯†åˆ«éœ¸å‡Œä¼šè¯ä¸­çš„æ”»å‡»æ€§è¯„è®º
            self.logger.info("ğŸ¯ æ­¥éª¤2: è¯†åˆ«éœ¸å‡Œä¼šè¯ä¸­çš„æ”»å‡»æ€§è¯„è®º")
            aggressive_comments = self.identify_aggressive_comments_in_bullying_sessions()
            if not aggressive_comments:
                return {'success': False, 'error': 'No aggressive comments found in bullying sessions'}
            
            # 3. ä»æ”»å‡»æ€§è¯„è®ºä¸­å¿ƒæå–å­å›¾
            self.logger.info("ğŸ•¸ï¸ æ­¥éª¤3: ä»æ”»å‡»æ€§è¯„è®ºä¸­å¿ƒæå–å­å›¾")
            subgraphs = self.extract_subgraphs_from_aggressive_comments()
            if not subgraphs:
                return {'success': False, 'error': 'No subgraphs extracted'}
            
            # 4. è®¡ç®—å­å›¾æƒ…æ„Ÿåˆ†æ•°
            self.logger.info("ğŸ˜Š æ­¥éª¤4: è®¡ç®—å­å›¾æƒ…æ„Ÿåˆ†æ•°")
            self.calculate_emotion_scores()
            
            # 5. åŸºäºæƒ…æ„Ÿé˜ˆå€¼ç­›é€‰éœ¸å‡Œå­å›¾
            self.logger.info("ğŸ” æ­¥éª¤5: ç­›é€‰éœ¸å‡Œå­å›¾")
            bullying_subgraphs = self.filter_bullying_subgraphs()
            if not bullying_subgraphs:
                return {'success': False, 'error': 'No bullying subgraphs after filtering'}
            
            # 6. ä»éœ¸å‡Œå­å›¾ä¸­èšç±»æå–åŸå‹
            self.logger.info("ğŸ¤– æ­¥éª¤6: èšç±»æå–åŸå‹")
            prototypes = self.cluster_bullying_subgraphs()
            if not prototypes:
                return {'success': False, 'error': 'No prototypes generated'}
            
            # 7. ä¿å­˜ç»“æœ
            self.logger.info("ğŸ’¾ æ­¥éª¤7: ä¿å­˜åŸå‹")
            output_dir = f"ProtoBully/data/prototype_v4_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            save_results = self.save_prototypes(output_dir)
            
            self.logger.info("ğŸ‰ åŸå‹æå–æµç¨‹å®Œæˆï¼")
            self.logger.info("="*50)
            
            # è¿”å›ç»“æœæ‘˜è¦
            results = {
                'success': True,
                'aggressive_comments_count': len(aggressive_comments),
                'extracted_subgraphs_count': len(subgraphs),
                'bullying_subgraphs_count': len(bullying_subgraphs),
                'prototypes_count': len(prototypes),
                'output_directory': output_dir,
                'save_results': save_results
            }
            
            # æ‰“å°ç»“æœæ‘˜è¦
            self.logger.info("ğŸ“Š ç»“æœæ‘˜è¦:")
            self.logger.info(f"   éœ¸å‡Œä¼šè¯æ•°: {len(self.bullying_sessions)}")
            self.logger.info(f"   æ”»å‡»æ€§è¯„è®ºæ•°: {results['aggressive_comments_count']}")
            self.logger.info(f"   æå–å­å›¾æ•°: {results['extracted_subgraphs_count']}")
            self.logger.info(f"   éœ¸å‡Œå­å›¾æ•°: {results['bullying_subgraphs_count']}")
            self.logger.info(f"   ç”ŸæˆåŸå‹æ•°: {results['prototypes_count']}")
            self.logger.info(f"   è¾“å‡ºç›®å½•: {output_dir}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ åŸå‹æå–æµç¨‹å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {'success': False, 'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨åŸºäºéœ¸å‡Œä¼šè¯çš„åŸå‹æå–å™¨V4ä¿®æ­£ç‰ˆ")
    print("="*50)
    
    # é…ç½®å‚æ•° - ä¼˜åŒ–é‡‡æ ·ç­–ç•¥
    config = {
        'subgraph_size_range': (6, 30),  # æ‰©å¤§å­å›¾å¤§å°èŒƒå›´6-30ä¸ªèŠ‚ç‚¹
        'emotion_threshold': -0.3,       # æƒ…æ„Ÿé˜ˆå€¼ï¼Œé€šè¿‡è°ƒå‚ä¼˜åŒ–
        'clustering_eps': 0.4,           # æ”¾å®½èšç±»å‚æ•°ï¼Œä»0.2å¢åŠ åˆ°0.4
        'min_cluster_size': 2,
        
        # å¤šæ ·æ€§é‡‡æ ·é…ç½® - å¤§å¹…ä¼˜åŒ–
        'max_samples': 1000,             # å¤§å¹…å¢åŠ é‡‡æ ·æ•°é‡åˆ°1000ä¸ª (åŸ300ä¸ª)
        'diversity_sampling': True,      # å¯ç”¨å¤šæ ·æ€§é‡‡æ ·
        'size_diversity_bins': 5,        # å¢åŠ åˆ°5ä¸ªåŒºé—´é‡‡æ ·ï¼Œæé«˜å¤šæ ·æ€§
        'quality_sampling_ratio': 0.7,   # 70%åŸºäºè´¨é‡é‡‡æ ·ï¼Œ30%éšæœºé‡‡æ ·
        'adaptive_sampling': True,       # å¯ç”¨è‡ªé€‚åº”é‡‡æ ·
        
        # åˆ†å±‚é‡‡æ ·é…ç½® (æ–°å¢)
        'stratified_sampling': True,     # å¯ç”¨åˆ†å±‚é‡‡æ ·
        'attack_level_bins': [
            (0.05, 0.15),  # è½»åº¦æ”»å‡»æ€§
            (0.15, 0.30),  # ä¸­åº¦æ”»å‡»æ€§  
            (0.30, 0.50),  # é«˜åº¦æ”»å‡»æ€§
            (0.50, 1.00),  # æé«˜æ”»å‡»æ€§
        ],
        'samples_per_bin': 200,          # æ¯ä¸ªæ”»å‡»æ€§ç­‰çº§é‡‡æ ·200ä¸ª
        
        # æƒ…æ„Ÿåˆ†æ•°æƒé‡ - å¯é€šè¿‡è°ƒå‚ä¼˜åŒ–
        'emotion_weights': {
            'comment': 0.4,    # è¯„è®ºèŠ‚ç‚¹æƒé‡
            'user': 0.3,       # ç”¨æˆ·èŠ‚ç‚¹æƒé‡
            'word': 0.2,       # è¯æ±‡èŠ‚ç‚¹æƒé‡
            'others': 0.1      # å…¶ä»–èŠ‚ç‚¹æƒé‡
        },
        
        # æ”»å‡»æ€§è¯„è®ºè¯†åˆ«é˜ˆå€¼
        'attack_word_ratio_threshold': 0.05,
        'attack_word_count_threshold': 1,
        'uppercase_ratio_threshold': 0.25,
        'exclamation_threshold': 2
    }
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = PrototypeExtractorV4Fixed(config)
    
    # è®¾ç½®æ–‡ä»¶è·¯å¾„
    graph_path = "data/graphs/heterogeneous_graph_with_comments.pkl"
    labels_path = "data/processed/prototypes/session_labels.json"
    
    # æ‰§è¡Œæå–
    result = extractor.run_full_extraction(graph_path, labels_path)
    
    # è¾“å‡ºç»“æœ
    if result['success']:
        print(f"âœ… åŸå‹æå–æˆåŠŸ!")
        print(f"   éœ¸å‡Œä¼šè¯æ•°: {len(extractor.bullying_sessions)} ä¸ª")
        print(f"   æ”»å‡»æ€§è¯„è®ºæ•°: {result['aggressive_comments_count']} ä¸ª")
        print(f"   æå–å­å›¾æ•°: {result['extracted_subgraphs_count']} ä¸ª")
        print(f"   éœ¸å‡Œå­å›¾æ•°: {result['bullying_subgraphs_count']} ä¸ª")
        print(f"   ç”ŸæˆåŸå‹æ•°: {result['prototypes_count']} ä¸ª")
        print(f"   è¾“å‡ºç›®å½•: {result['output_directory']}")
        
        # å¦‚æœæœ‰åŸå‹ï¼Œæ˜¾ç¤ºè´¨é‡ç»Ÿè®¡
        if extractor.prototypes:
            quality_scores = [p.get('quality_score', 0) for p in extractor.prototypes]
            avg_quality = np.mean(quality_scores) if quality_scores else 0
            print(f"   å¹³å‡åŸå‹è´¨é‡: {avg_quality:.3f}")
            
            # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†æ•°ç»Ÿè®¡
            if extractor.bullying_subgraphs:
                emotion_scores = [sg.get('emotion_score', 0) for sg in extractor.bullying_subgraphs]
                avg_emotion = np.mean(emotion_scores) if emotion_scores else 0
                print(f"   å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {avg_emotion:.3f}")
        
    else:
        print(f"âŒ åŸå‹æå–å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    main() 