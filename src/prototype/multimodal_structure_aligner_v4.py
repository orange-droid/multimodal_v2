"""
å¤šæ¨¡æ€ç»“æ„å¯¹é½æ¨¡å— V4
åŸºäºSubgraphPrototypeExtractorV3çš„åŸå‹åŒ¹é…ç³»ç»Ÿ

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½ä»SubgraphPrototypeExtractorV3æå–çš„åŸå‹
2. ä¸ºå¾…æ£€æµ‹çš„ä¼šè¯æå–å­å›¾
3. è®¡ç®—å­å›¾ä¸åŸå‹çš„ç»“æ„ç›¸ä¼¼åº¦
4. å…¼å®¹å•ä¸ªå’Œå¤šä¸ªåŸå‹çš„æƒ…å†µ
"""

import pickle
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Set
import torch_geometric
from torch_geometric.data import HeteroData
from prototype_extractor_v4_fixed import PrototypeExtractorV4Fixed

class MultimodalStructureAlignerV4:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.extractor = None
        self.prototypes = []
        self.graph = None
        
        # åŠ è½½å›¾æ•°æ®
        self._load_graph()
        
        # åˆå§‹åŒ–åŸå‹æå–å™¨
        self._initialize_extractor()
        
        # åŠ è½½ç°æœ‰åŸå‹
        self._load_prototypes()
    
    def _load_graph(self):
        """åŠ è½½å›¾æ•°æ®"""
        graph_path = self.config.get('graph_path', 'data/graphs/media_session_graph_20250601_183403.pkl')
        
        try:
            with open(graph_path, 'rb') as f:
                self.graph = pickle.load(f)
            print(f"âœ… å›¾æ•°æ®åŠ è½½æˆåŠŸ: {self.graph.num_nodes:,}ä¸ªèŠ‚ç‚¹, {self.graph.num_edges:,}æ¡è¾¹")
        except Exception as e:
            raise RuntimeError(f"âŒ å›¾æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def _initialize_extractor(self):
        """åˆå§‹åŒ–åŸå‹æå–å™¨"""
        try:
            self.extractor = PrototypeExtractorV4Fixed(self.config)
            
            # åŠ è½½å¿…è¦çš„æ•°æ®
            graph_path = self.config.get('graph_path', 'data/graphs/text_graph_20250602_175019.pkl')
            labels_path = self.config.get('labels_path', 'data/session_labels_20250602_175019.json')
            
            # åŠ è½½æ•°æ®åˆ°æå–å™¨
            self.extractor.load_data(graph_path, labels_path)
            
            # å»ºç«‹é‚»æ¥è¡¨
            self.adj_list = self.extractor._build_adjacency_list()
            print(f"âœ… é‚»æ¥è¡¨å»ºç«‹æˆåŠŸ: {len(self.adj_list):,}ä¸ªèŠ‚ç‚¹")
            
            # è®¡ç®—èŠ‚ç‚¹ç±»å‹èµ·å§‹ç´¢å¼•
            self._calculate_node_start_indices()
            
            print("âœ… åŸå‹æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ åŸå‹æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _calculate_node_start_indices(self):
        """è®¡ç®—å„ç§èŠ‚ç‚¹ç±»å‹çš„èµ·å§‹ç´¢å¼•"""
        if not self.graph:
            raise RuntimeError("å›¾æ•°æ®æœªåŠ è½½")
        
        self.comment_start_idx = 0
        self.user_start_idx = 0
        
        # è®¡ç®—è¯„è®ºèŠ‚ç‚¹èµ·å§‹ç´¢å¼•
        for node_type in self.graph.node_types:
            if node_type == 'comment':
                break
            self.comment_start_idx += self.graph[node_type].x.size(0)
        
        # è®¡ç®—ç”¨æˆ·èŠ‚ç‚¹èµ·å§‹ç´¢å¼•
        for node_type in self.graph.node_types:
            if node_type == 'user':
                break
            self.user_start_idx += self.graph[node_type].x.size(0)
        
        print(f"ğŸ“ èŠ‚ç‚¹ç´¢å¼•è®¡ç®—å®Œæˆ:")
        print(f"   è¯„è®ºèŠ‚ç‚¹èµ·å§‹ç´¢å¼•: {self.comment_start_idx}")
        print(f"   ç”¨æˆ·èŠ‚ç‚¹èµ·å§‹ç´¢å¼•: {self.user_start_idx}")
    
    def _load_prototypes(self):
        """åŠ è½½ç°æœ‰åŸå‹"""
        prototype_dir = Path(self.config.get('prototype_output_dir', 'ProtoBully/data/prototype_v4_fixed_20250602_193953'))
        
        # æŸ¥æ‰¾v4åŸå‹æ–‡ä»¶
        prototype_file = prototype_dir / 'prototypes_v4_fixed_20250602_193953.pkl'
        
        if not prototype_file.exists():
            print(f"âš ï¸ æœªæ‰¾åˆ°v4åŸå‹æ–‡ä»¶: {prototype_file}")
            return
        
        try:
            with open(prototype_file, 'rb') as f:
                prototypes_data = pickle.load(f)
                
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(prototypes_data, list):
                self.prototypes = prototypes_data
            elif isinstance(prototypes_data, dict) and 'prototypes' in prototypes_data:
                self.prototypes = prototypes_data['prototypes']
            else:
                print(f"âš ï¸ åŸå‹æ–‡ä»¶æ ¼å¼ä¸è¯†åˆ«: {type(prototypes_data)}")
                return
                
            print(f"âœ… åŸå‹åŠ è½½æˆåŠŸ: {prototype_file.name}")
            print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(self.prototypes)} ä¸ªåŸå‹")
            
            # æ‰“å°åŸå‹åŸºæœ¬ä¿¡æ¯
            for i, prototype in enumerate(self.prototypes):
                if isinstance(prototype, dict):
                    prototype_id = prototype.get('prototype_id', f'prototype_{i}')
                    cluster_size = prototype.get('cluster_size', 'N/A')
                    quality_score = prototype.get('quality_score', 'N/A')
                    print(f"   åŸå‹ {i} ({prototype_id}): èšç±»å¤§å°={cluster_size}, è´¨é‡={quality_score}")
                    
        except Exception as e:
            print(f"âŒ åŸå‹åŠ è½½å¤±è´¥ {prototype_file.name}: {e}")
        
        if not self.prototypes:
            print("âš ï¸ æœªæˆåŠŸåŠ è½½ä»»ä½•åŸå‹")
    
    def extract_session_subgraph(self, session_id: str, target_size: int = 10) -> Optional[Dict]:
        """ä¸ºæŒ‡å®šä¼šè¯æå–å­å›¾ - ä½¿ç”¨verifiedæ–¹æ³•"""
        if not self.extractor:
            print("âŒ åŸå‹æå–å™¨æœªåˆå§‹åŒ–")
            return None
        
        try:
            # ä»ä¼šè¯æ˜ å°„æ–‡ä»¶ä¸­æŸ¥æ‰¾è¯„è®º
            session_comments = []
            
            # åŠ è½½ä¼šè¯æ˜ å°„æ–‡ä»¶
            session_mapping_path = self.config.get('session_mapping_path', 'data/graphs/complete_session_to_nodes_mapping.json')
            
            try:
                with open(session_mapping_path, 'r', encoding='utf-8') as f:
                    mapping_data = json.load(f)
                
                session_mapping = mapping_data.get('session_to_nodes_mapping', {})
                
                if session_id in session_mapping:
                    comment_indices = session_mapping[session_id].get('comment_indices', [])
                    # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
                    session_comments = [c + self.comment_start_idx for c in comment_indices]
                    print(f"   æ‰¾åˆ° {len(session_comments)} ä¸ªè¯„è®ºèŠ‚ç‚¹")
                else:
                    print(f"âš ï¸ ä¼šè¯ {session_id} ä¸åœ¨æ˜ å°„æ–‡ä»¶ä¸­")
                    
            except Exception as e:
                print(f"âŒ åŠ è½½ä¼šè¯æ˜ å°„å¤±è´¥: {e}")
            
            if not session_comments:
                print(f"âš ï¸ ä¼šè¯ {session_id} ä¸­æ²¡æœ‰æ‰¾åˆ°è¯„è®º")
                return None
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªè¯„è®ºä½œä¸ºä¸­å¿ƒ
            center_comment = session_comments[0]
            print(f"   ä½¿ç”¨ä¸­å¿ƒè¯„è®º: {center_comment}")
            
            # ä½¿ç”¨verifiedçš„å­å›¾èŠ‚ç‚¹æ”¶é›†æ–¹æ³•
            if not hasattr(self, 'adj_list') or not self.adj_list:
                print("   âŒ é‚»æ¥è¡¨æœªå»ºç«‹ï¼Œä½¿ç”¨åŸæœ‰æ–¹æ³•")
                # å›é€€åˆ°åŸæœ‰æ–¹æ³• - ä½¿ç”¨æ‰¾åˆ°çš„è¯„è®ºèŠ‚ç‚¹
                node_indices = session_comments[:target_size]  # é™åˆ¶å¤§å°
                
                subgraph_features = self._compute_subgraph_features(node_indices)
                return {
                    'session_id': session_id,
                    'node_indices': node_indices,
                    'features': subgraph_features,
                    'size': len(node_indices),
                    'extraction_method': 'fallback_method'
                }
            
            # ä½¿ç”¨verifiedæ–¹æ³•æ”¶é›†å¤šæ ·åŒ–å­å›¾èŠ‚ç‚¹
            subgraph_nodes = self._collect_diverse_subgraph_nodes(
                center_comment, self.adj_list, target_size
            )
            
            print(f"   å¤šæ ·åŒ–èŠ‚ç‚¹æ”¶é›†: {len(subgraph_nodes)}ä¸ªèŠ‚ç‚¹")
            
            # ä½¿ç”¨verifiedæ–¹æ³•æå–å­å›¾ç»“æ„
            structure = self._extract_subgraph_structure_v2(subgraph_nodes)
            
            print(f"   å­å›¾è¾¹æ•°: {structure['structural_features']['total_edges']}")
            print(f"   å­å›¾å¯†åº¦: {structure['structural_features']['density']:.3f}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¿æŒå…¼å®¹æ€§
            node_list = list(subgraph_nodes)
            
            # æ„å»ºfeaturesç”¨äºç›¸ä¼¼åº¦è®¡ç®—
            features = {
                'node_count': len(node_list),
                'comment_nodes': len(structure['nodes_by_type'].get('comment', [])),
                'user_nodes': len(structure['nodes_by_type'].get('user', [])),
                'avg_emotion_score': 0.0,  # ç®€åŒ–ç‰ˆ
                'avg_attack_ratio': 0.0,   # ç®€åŒ–ç‰ˆ
                'structural_features': structure['structural_features']
            }
            
            print(f"âœ… ä¼šè¯ {session_id} å­å›¾æå–æˆåŠŸ: {len(node_list)}ä¸ªèŠ‚ç‚¹")
            print(f"   èŠ‚ç‚¹èŒƒå›´: {min(node_list)}-{max(node_list)}")
            print(f"   ç‰¹å¾: å¯†åº¦={features['structural_features']['density']:.3f}")
            
            return {
                'session_id': session_id,
                'node_indices': node_list,
                'features': features,
                'size': len(node_list),
                'structure': structure,
                'extraction_method': 'verified_diverse_collection'
            }
            
        except Exception as e:
            print(f"âŒ ä¼šè¯ {session_id} å­å›¾æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _compute_subgraph_features(self, node_indices: List[int]) -> Dict:
        """è®¡ç®—å­å›¾ç‰¹å¾"""
        features = {
            'node_count': len(node_indices),
            'comment_nodes': 0,
            'user_nodes': 0,
            'emotion_scores': [],
            'attack_ratios': [],
            'structural_features': {}
        }
        
        try:
            # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹å’Œç‰¹å¾
            for node_idx in node_indices:
                if self.comment_start_idx <= node_idx < self.user_start_idx:
                    # è¯„è®ºèŠ‚ç‚¹
                    features['comment_nodes'] += 1
                    comment_idx = node_idx - self.comment_start_idx
                    
                    # æå–è¯„è®ºç‰¹å¾
                    comment_features = self.extractor.graph['comment'].x[comment_idx]
                    emotion_score = comment_features[1].item()
                    attack_ratio = comment_features[2].item()
                    
                    features['emotion_scores'].append(emotion_score)
                    features['attack_ratios'].append(attack_ratio)
                    
                elif node_idx >= self.user_start_idx:
                    # ç”¨æˆ·èŠ‚ç‚¹
                    features['user_nodes'] += 1
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            if features['emotion_scores']:
                features['avg_emotion_score'] = np.mean(features['emotion_scores'])
                features['std_emotion_score'] = np.std(features['emotion_scores'])
                features['min_emotion_score'] = np.min(features['emotion_scores'])
            else:
                features['avg_emotion_score'] = 0.0
                features['std_emotion_score'] = 0.0
                features['min_emotion_score'] = 0.0
            
            if features['attack_ratios']:
                features['avg_attack_ratio'] = np.mean(features['attack_ratios'])
                features['max_attack_ratio'] = np.max(features['attack_ratios'])
            else:
                features['avg_attack_ratio'] = 0.0
                features['max_attack_ratio'] = 0.0
            
            # è®¡ç®—ç»“æ„ç‰¹å¾
            features['structural_features'] = self._compute_structural_features(node_indices)
            
        except Exception as e:
            print(f"âŒ å­å›¾ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        
        return features
    
    def _compute_structural_features(self, node_indices: List[int]) -> Dict:
        """è®¡ç®—ç»“æ„ç‰¹å¾"""
        structural = {
            'edge_count': 0,
            'density': 0.0,
            'comment_to_user_edges': 0,
            'user_to_comment_edges': 0,
            'comment_to_comment_edges': 0
        }
        
        try:
            node_set = set(node_indices)
            
            # ç»Ÿè®¡ä¸åŒç±»å‹çš„è¾¹
            edge_types = [
                ('comment', 'posted_by', 'user'),
                ('user', 'posted', 'comment'),
                ('comment', 'replies_to', 'comment')
            ]
            
            for src_type, edge_type, dst_type in edge_types:
                if (src_type, edge_type, dst_type) in self.graph.edge_types:
                    edge_index = self.graph[src_type, edge_type, dst_type].edge_index
                    
                    # è°ƒæ•´èŠ‚ç‚¹ç´¢å¼•åˆ°å¯¹åº”ç±»å‹çš„æœ¬åœ°ç´¢å¼•
                    for i in range(edge_index.size(1)):
                        src_global = edge_index[0, i].item()
                        dst_global = edge_index[1, i].item()
                        
                        # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
                        if src_type == 'comment':
                            src_global += self.comment_start_idx
                        elif src_type == 'user':
                            src_global += self.user_start_idx
                        
                        if dst_type == 'comment':
                            dst_global += self.comment_start_idx
                        elif dst_type == 'user':
                            dst_global += self.user_start_idx
                        
                        # æ£€æŸ¥è¾¹æ˜¯å¦åœ¨å­å›¾å†…
                        if src_global in node_set and dst_global in node_set:
                            structural['edge_count'] += 1
                            
                            if edge_type == 'posted_by':
                                structural['comment_to_user_edges'] += 1
                            elif edge_type == 'posted':
                                structural['user_to_comment_edges'] += 1
                            elif edge_type == 'replies_to':
                                structural['comment_to_comment_edges'] += 1
            
            # è®¡ç®—å¯†åº¦
            n = len(node_indices)
            if n > 1:
                max_edges = n * (n - 1)  # æœ‰å‘å›¾
                structural['density'] = structural['edge_count'] / max_edges
            
        except Exception as e:
            print(f"âŒ ç»“æ„ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        
        return structural
    
    def calculate_prototype_similarity(self, session_subgraph: Dict, prototype: Dict) -> float:
        """è®¡ç®—ä¼šè¯å­å›¾ä¸åŸå‹çš„ç›¸ä¼¼åº¦"""
        try:
            session_features = session_subgraph['features']
            
            # å¤„ç†v4åŸå‹æ ¼å¼
            if 'representative_subgraph' in prototype:
                proto_subgraph = prototype['representative_subgraph']
            else:
                print(f"âš ï¸ åŸå‹æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œé”®: {list(prototype.keys()) if isinstance(prototype, dict) else 'Not dict'}")
                return 0.0
            
            # ä»åŸå‹ä¸­æå–ç‰¹å¾
            proto_features = self._extract_prototype_features(proto_subgraph)
            session_features_normalized = self._normalize_session_features(session_features)
            
            # ç‰¹å¾ç›¸ä¼¼åº¦æƒé‡
            weights = {
                'structural': 0.4,
                'node_composition': 0.3,
                'size': 0.3
            }
            
            similarity_scores = []
            
            # 1. ç»“æ„ç›¸ä¼¼åº¦ï¼ˆå¯†åº¦ï¼‰
            session_density = session_features_normalized.get('density', 0.0)
            proto_density = proto_features.get('density', 0.0)
            
            if session_density > 0 and proto_density > 0:
                density_sim = 1.0 - abs(session_density - proto_density) / max(session_density, proto_density)
            elif session_density == 0 and proto_density == 0:
                density_sim = 1.0  # éƒ½æ˜¯0ï¼Œå®Œå…¨ç›¸ä¼¼
            else:
                density_sim = 0.0  # ä¸€ä¸ªä¸º0ä¸€ä¸ªä¸ä¸º0
            
            density_sim = max(0.0, min(1.0, density_sim))
            similarity_scores.append(('structural', density_sim, weights['structural']))
            
            # 2. èŠ‚ç‚¹ç»„æˆç›¸ä¼¼åº¦
            session_comment_ratio = session_features_normalized.get('comment_ratio', 0.0)
            proto_comment_ratio = proto_features.get('comment_ratio', 0.0)
            
            composition_sim = 1.0 - abs(session_comment_ratio - proto_comment_ratio)
            composition_sim = max(0.0, min(1.0, composition_sim))
            
            similarity_scores.append(('composition', composition_sim, weights['node_composition']))
            
            # 3. å¤§å°ç›¸ä¼¼åº¦
            session_size = session_features_normalized.get('node_count', 0)
            proto_size = proto_features.get('node_count', 0)
            
            if session_size > 0 and proto_size > 0:
                size_sim = 1.0 - abs(session_size - proto_size) / max(session_size, proto_size)
            elif session_size == proto_size == 0:
                size_sim = 1.0
            else:
                size_sim = 0.0
            
            size_sim = max(0.0, min(1.0, size_sim))
            similarity_scores.append(('size', size_sim, weights['size']))
            
            # è®¡ç®—åŠ æƒæ€»ç›¸ä¼¼åº¦
            total_similarity = sum(score * weight for _, score, weight in similarity_scores)
            
            # è°ƒè¯•è¾“å‡º
            print(f"   ç›¸ä¼¼åº¦è¯¦æƒ…: ç»“æ„={density_sim:.3f}, ç»„æˆ={composition_sim:.3f}, å¤§å°={size_sim:.3f}")
            print(f"   æ€»ç›¸ä¼¼åº¦: {total_similarity:.3f}")
            
            return min(1.0, max(0.0, total_similarity))
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def _extract_prototype_features(self, proto_subgraph: Dict) -> Dict:
        """ä»åŸå‹å­å›¾ä¸­æå–æ ‡å‡†åŒ–ç‰¹å¾"""
        features = {}
        
        try:
            # æå–ç»“æ„ç‰¹å¾
            structural_features = proto_subgraph.get('structural_features', {})
            features['density'] = structural_features.get('density', 0.0)
            features['node_count'] = structural_features.get('total_nodes', 0)
            features['edge_count'] = structural_features.get('total_edges', 0)
            
            # æå–èŠ‚ç‚¹ç»„æˆ
            nodes_by_type = proto_subgraph.get('nodes_by_type', {})
            comment_count = len(nodes_by_type.get('comment', []))
            total_nodes = features['node_count']
            
            if total_nodes > 0:
                features['comment_ratio'] = comment_count / total_nodes
            else:
                features['comment_ratio'] = 0.0
            
            # æå–æƒ…æ„Ÿç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰
            emotion_features = proto_subgraph.get('emotion_features', {})
            features['avg_emotion_score'] = emotion_features.get('avg_text_length', 0.0)
            
            return features
            
        except Exception as e:
            print(f"âš ï¸ åŸå‹ç‰¹å¾æå–å¤±è´¥: {e}")
            return {'density': 0.0, 'node_count': 0, 'comment_ratio': 0.0}
    
    def _normalize_session_features(self, session_features: Dict) -> Dict:
        """æ ‡å‡†åŒ–ä¼šè¯ç‰¹å¾"""
        features = {}
        
        try:
            # åŸºæœ¬ç‰¹å¾
            features['node_count'] = session_features.get('node_count', 0)
            features['comment_nodes'] = session_features.get('comment_nodes', 0)
            features['user_nodes'] = session_features.get('user_nodes', 0)
            
            # è®¡ç®—æ¯”ä¾‹
            if features['node_count'] > 0:
                features['comment_ratio'] = features['comment_nodes'] / features['node_count']
            else:
                features['comment_ratio'] = 0.0
            
            # ç»“æ„ç‰¹å¾
            structural_features = session_features.get('structural_features', {})
            features['density'] = structural_features.get('density', 0.0)
            features['edge_count'] = structural_features.get('total_edges', 0)
            
            return features
            
        except Exception as e:
            print(f"âš ï¸ ä¼šè¯ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return {'node_count': 0, 'comment_ratio': 0.0, 'density': 0.0}
    
    def align_session_with_prototypes(self, session_id: str) -> Dict:
        """å°†å•ä¸ªä¼šè¯ä¸æ‰€æœ‰åŸå‹å¯¹é½"""
        if not self.prototypes:
            return {
                'session_id': session_id,
                'status': 'no_prototypes',
                'similarities': [],
                'max_similarity': 0.0,
                'prediction': 'normal'
            }
        
        # æå–ä¼šè¯å­å›¾
        session_subgraph = self.extract_session_subgraph(session_id)
        
        if not session_subgraph:
            return {
                'session_id': session_id,
                'status': 'extraction_failed',
                'similarities': [],
                'max_similarity': 0.0,
                'prediction': 'normal'
            }
        
        # ä¸æ‰€æœ‰åŸå‹è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        
        for i, prototype in enumerate(self.prototypes):
            similarity = self.calculate_prototype_similarity(session_subgraph, prototype)
            
            # æ­£ç¡®æå–åŸå‹æƒ…æ„Ÿåˆ†æ•°
            prototype_emotion = 0.0
            if 'representative_subgraph' in prototype:
                emotion_features = prototype['representative_subgraph'].get('emotion_features', {})
                prototype_emotion = emotion_features.get('avg_text_length', 0.0)
            
            similarities.append({
                'prototype_id': i,
                'similarity': similarity,
                'prototype_emotion': prototype_emotion
            })
        
        # æ’åºï¼Œè·å–æœ€é«˜ç›¸ä¼¼åº¦
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        max_similarity = similarities[0]['similarity'] if similarities else 0.0
        
        # é¢„æµ‹ï¼ˆåŸºäºç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
        threshold = self.config.get('similarity_threshold', 0.6)
        prediction = 'bullying' if max_similarity >= threshold else 'normal'
        
        return {
            'session_id': session_id,
            'status': 'success',
            'session_subgraph': session_subgraph,
            'similarities': similarities,
            'max_similarity': max_similarity,
            'prediction': prediction,
            'threshold_used': threshold
        }
    
    def batch_align_sessions(self, session_ids: List[str]) -> List[Dict]:
        """æ‰¹é‡å¯¹é½ä¼šè¯"""
        results = []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¯¹é½ {len(session_ids)} ä¸ªä¼šè¯...")
        
        for i, session_id in enumerate(session_ids):
            try:
                result = self.align_session_with_prototypes(session_id)
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    print(f"   è¿›åº¦: {i + 1}/{len(session_ids)}")
                    
            except Exception as e:
                print(f"âŒ ä¼šè¯ {session_id} å¯¹é½å¤±è´¥: {e}")
                results.append({
                    'session_id': session_id,
                    'status': 'error',
                    'error': str(e),
                    'similarities': [],
                    'max_similarity': 0.0,
                    'prediction': 'normal'
                })
        
        print(f"âœ… æ‰¹é‡å¯¹é½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªä¼šè¯")
        return results
    
    def save_alignment_results(self, results: List[Dict], output_path: str):
        """ä¿å­˜å¯¹é½ç»“æœ"""
        try:
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                'total_sessions': len(results),
                'prototype_count': len(self.prototypes),
                'config': self.config,
                'results': results,
                'statistics': self._compute_alignment_statistics(results)
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å¯¹é½ç»“æœå·²ä¿å­˜è‡³: {output_path}")
            
        except Exception as e:
            print(f"âŒ å¯¹é½ç»“æœä¿å­˜å¤±è´¥: {e}")
    
    def _compute_alignment_statistics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—å¯¹é½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'successful_alignments': 0,
            'failed_alignments': 0,
            'bullying_predictions': 0,
            'normal_predictions': 0,
            'avg_max_similarity': 0.0,
            'similarity_distribution': {}
        }
        
        similarities = []
        
        for result in results:
            if result['status'] == 'success':
                stats['successful_alignments'] += 1
                similarities.append(result['max_similarity'])
                
                if result['prediction'] == 'bullying':
                    stats['bullying_predictions'] += 1
                else:
                    stats['normal_predictions'] += 1
            else:
                stats['failed_alignments'] += 1
        
        if similarities:
            stats['avg_max_similarity'] = np.mean(similarities)
            stats['similarity_distribution'] = {
                'min': float(np.min(similarities)),
                'max': float(np.max(similarities)),
                'mean': float(np.mean(similarities)),
                'std': float(np.std(similarities)),
                'median': float(np.median(similarities))
            }
        
        return stats
    
    # =============================================================================
    # ä»prototype_extractor_v4_fixed.pyå¤åˆ¶çš„verifiedæ–¹æ³•
    # =============================================================================
    
    def _collect_diverse_subgraph_nodes(self, center_comment: int, adj_list: Dict, target_size: int) -> Set[int]:
        """æ”¶é›†å¤šæ ·åŒ–çš„å­å›¾èŠ‚ç‚¹ï¼Œæ”¯æŒä¸åŒçš„æ‰©å±•ç­–ç•¥"""
        
        subgraph_nodes = {center_comment}
        candidates = set()
        
        # æ·»åŠ ä¸­å¿ƒè¯„è®ºçš„ç›´æ¥é‚»å±…
        if center_comment in adj_list:
            for neighbor in adj_list[center_comment]:
                candidates.add(neighbor)
        
        # æ ¹æ®ç›®æ ‡å¤§å°é€‰æ‹©ä¸åŒçš„æ‰©å±•ç­–ç•¥
        if target_size <= 12:
            # å°å‹å­å›¾ï¼šç´§å¯†è¿æ¥ï¼Œé€‰æ‹©é«˜ä¼˜å…ˆçº§èŠ‚ç‚¹
            strategy = "compact"
        elif target_size <= 21:
            # ä¸­å‹å­å›¾ï¼šå¹³è¡¡æ‰©å±•
            strategy = "balanced"
        else:
            # å¤§å‹å­å›¾ï¼šå¹¿æ³›æ‰©å±•ï¼ŒåŒ…å«æ›´å¤šèŠ‚ç‚¹ç±»å‹
            strategy = "extensive"
        
        # æŒ‰ç­–ç•¥æ‰©å±•å­å›¾
        while len(subgraph_nodes) < target_size and candidates:
            if strategy == "compact":
                next_node = self._select_priority_node(candidates, subgraph_nodes)
            elif strategy == "balanced":
                next_node = self._select_balanced_node(candidates, subgraph_nodes)
            else:  # extensive
                next_node = self._select_diverse_node(candidates, subgraph_nodes)
                
            if next_node is None:
                break
                
            subgraph_nodes.add(next_node)
            candidates.remove(next_node)
            
            # æ·»åŠ æ–°èŠ‚ç‚¹çš„é‚»å±…ä½œä¸ºå€™é€‰
            if next_node in adj_list:
                for neighbor in adj_list[next_node]:
                    if neighbor not in subgraph_nodes:
                        candidates.add(neighbor)
        
        return subgraph_nodes
    
    def _select_balanced_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """å¹³è¡¡é€‰æ‹©ç­–ç•¥ï¼šåœ¨ä¼˜å…ˆçº§å’Œå¤šæ ·æ€§ä¹‹é—´å¹³è¡¡"""
        # 50%æ¦‚ç‡æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ï¼Œ50%æ¦‚ç‡éšæœºé€‰æ‹©
        if np.random.random() < 0.5:
            return self._select_priority_node(candidates, existing_nodes)
        else:
            return next(iter(candidates)) if candidates else None
    
    def _select_diverse_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """å¤šæ ·æ€§é€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©å›¾ä¸­ç¼ºå°‘çš„èŠ‚ç‚¹ç±»å‹"""
        existing_types = {self._get_node_type(node) for node in existing_nodes}
        
        # ä¼˜å…ˆé€‰æ‹©å½“å‰å­å›¾ä¸­ç¼ºå°‘çš„èŠ‚ç‚¹ç±»å‹
        for node_type in ['word', 'time', 'location', 'user', 'comment']:
            if node_type not in existing_types:
                type_candidates = [
                    node for node in candidates 
                    if self._get_node_type(node) == node_type
                ]
                if type_candidates:
                    return type_candidates[0]
        
        # å¦‚æœæ‰€æœ‰ç±»å‹éƒ½æœ‰ï¼Œéšæœºé€‰æ‹©
        return next(iter(candidates)) if candidates else None
    
    def _select_priority_node(self, candidates: Set[int], existing_nodes: Set[int]) -> int:
        """æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼šè¯„è®º > ç”¨æˆ· > è¯æ±‡ > å…¶ä»–"""
        priority_order = ['comment', 'user', 'word', 'time', 'location']
        
        for node_type in priority_order:
            type_candidates = [
                node for node in candidates 
                if self._get_node_type(node) == node_type
            ]
            if type_candidates:
                return type_candidates[0]
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆç±»å‹ï¼Œè¿”å›ä»»æ„å€™é€‰
        return next(iter(candidates)) if candidates else None
    
    def _get_node_type(self, node_idx: int) -> str:
        """æ ¹æ®èŠ‚ç‚¹ç´¢å¼•ç¡®å®šèŠ‚ç‚¹ç±»å‹"""
        if node_idx < 55038:
            return 'user'
        elif node_idx < 132441:
            return 'comment'
        elif node_idx < 133441:
            return 'word'
        elif node_idx < 134400:
            return 'video'
        else:
            return 'other'
    
    def _extract_subgraph_edges_v2(self, subgraph_nodes: Set[int]) -> Dict:
        """ä»åŸå‹æå–å™¨å¤åˆ¶çš„verifiedè¾¹æå–æ–¹æ³•"""
        subgraph_edges = {}
        
        # ä½¿ç”¨å·²ç»å»ºç«‹çš„é‚»æ¥è¡¨
        if hasattr(self, 'adj_list') and self.adj_list:
            edge_count = 0
            for node in subgraph_nodes:
                if node in self.adj_list:
                    for neighbor in self.adj_list[node]:
                        if neighbor in subgraph_nodes:
                            # ç¡®å®šè¾¹ç±»å‹
                            edge_type = self._determine_edge_type(node, neighbor)
                            if edge_type not in subgraph_edges:
                                subgraph_edges[edge_type] = []
                            subgraph_edges[edge_type].append((node, neighbor))
                            edge_count += 1
            
            print(f"   å‘ç°å­å›¾å†…éƒ¨è¾¹: {edge_count}æ¡")
        else:
            print("   âŒ é‚»æ¥è¡¨æœªå»ºç«‹ï¼Œæ— æ³•æå–è¾¹")
        
        return subgraph_edges
    
    def _determine_edge_type(self, node1: int, node2: int) -> str:
        """ç¡®å®šä¸¤ä¸ªèŠ‚ç‚¹é—´çš„è¾¹ç±»å‹"""
        type1 = self._get_node_type(node1)
        type2 = self._get_node_type(node2)
        
        # è¿”å›æ ‡å‡†åŒ–çš„è¾¹ç±»å‹åç§°
        if (type1 == 'user' and type2 == 'comment') or (type1 == 'comment' and type2 == 'user'):
            return 'user_posts_comment'
        elif (type1 == 'comment' and type2 == 'word') or (type1 == 'word' and type2 == 'comment'):
            return 'comment_contains_word'
        elif type1 == 'user' and type2 == 'user':
            return 'user_interacts_user'
        elif (type1 == 'video' and type2 == 'comment') or (type1 == 'comment' and type2 == 'video'):
            return 'video_contains_comment'
        else:
            return f'{type1}_{type2}'
    
    def _extract_subgraph_structure_v2(self, subgraph_nodes: Set[int]) -> Dict:
        """ä»åŸå‹æå–å™¨å¤åˆ¶çš„verifiedç»“æ„æå–æ–¹æ³•"""
        # æŒ‰ç±»å‹åˆ†ç»„èŠ‚ç‚¹
        nodes_by_type = {}
        for node in subgraph_nodes:
            node_type = self._get_node_type(node)
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)
        
        # æå–è¾¹
        edges = self._extract_subgraph_edges_v2(subgraph_nodes)
        
        # è®¡ç®—ç‰¹å¾
        structural_features = self._calculate_subgraph_features_v2(nodes_by_type, edges)
        
        return {
            'nodes_by_type': nodes_by_type,
            'edges': edges,
            'structural_features': structural_features
        }
    
    def _calculate_subgraph_features_v2(self, nodes_by_type: Dict, edges: Dict) -> Dict:
        """ä»åŸå‹æå–å™¨å¤åˆ¶çš„verifiedç‰¹å¾è®¡ç®—æ–¹æ³•"""
        features = {}
        
        # èŠ‚ç‚¹ç»Ÿè®¡
        features['node_counts'] = {k: len(v) for k, v in nodes_by_type.items()}
        features['total_nodes'] = sum(features['node_counts'].values())
        
        # è¾¹ç»Ÿè®¡
        features['edge_counts'] = {k: len(v) for k, v in edges.items()}
        features['total_edges'] = sum(features['edge_counts'].values())
        
        # å¯†åº¦
        total_nodes = features['total_nodes']
        if total_nodes > 1:
            max_edges = total_nodes * (total_nodes - 1) / 2
            features['density'] = features['total_edges'] / max_edges
        else:
            features['density'] = 0.0
        
        # å¤šæ ·æ€§
        features['node_type_diversity'] = len(features['node_counts'])
        features['edge_type_diversity'] = len(features['edge_counts'])
        
        return features 