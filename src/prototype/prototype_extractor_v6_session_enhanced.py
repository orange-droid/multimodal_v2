#!/usr/bin/env python3
"""
ProtoBullyé¡¹ç›® - åŸå‹æå–å™¨V6é‡æ„ç‰ˆ
åŸºäºå¼±ç›‘ç£å­¦ä¹ çš„åŸå‹è´¨é‡è¯„ä¼°æ–¹æ³•

ä¸»è¦åˆ›æ–°ï¼š
1. åˆ©ç”¨ä¼šè¯æ ‡ç­¾ä½œä¸ºå¼±ç›‘ç£ä¿¡å·
2. 4ä¸ªç§‘å­¦çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
3. æ•°æ®é©±åŠ¨çš„åŸå‹é€‰æ‹©ç­–ç•¥
4. é¿å…ä¸»è§‚æƒé‡åˆ†é…

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-06-28
ç‰ˆæœ¬ï¼šV6 Refactored
"""

import os
import json
import pickle
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict, Counter
from tqdm import tqdm

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report


class PrototypeExtractorV6Refactored:
    """
    åŸå‹æå–å™¨V6é‡æ„ç‰ˆ - åŸºäºå¼±ç›‘ç£å­¦ä¹ çš„è´¨é‡è¯„ä¼°
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - åˆ©ç”¨ä¼šè¯æ ‡ç­¾ä½œä¸ºå¼±ç›‘ç£ä¿¡å·
    - 4ä¸ªç§‘å­¦çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
    - æ•°æ®é©±åŠ¨çš„åŸå‹é€‰æ‹©ç­–ç•¥
    """
    
    def __init__(self, config: Dict = None):
        """åˆå§‹åŒ–åŸå‹æå–å™¨"""
        self.config = config or self._get_default_config()
        self.logger = self._setup_logger()
        
        # æ•°æ®å­˜å‚¨
        self.session_labels = {}  # ä¼šè¯æ ‡ç­¾æ˜ å°„
        self.heterogeneous_graph = None  # å¼‚æ„å›¾
        self.enhanced_subgraphs = []  # å¢å¼ºç‰ˆå­å›¾
        self.bullying_subgraphs = []  # éœ¸å‡Œå­å›¾
        self.normal_subgraphs = []  # æ­£å¸¸å­å›¾
        
        # ç‰¹å¾å’Œèšç±»
        self.subgraph_features = None  # å­å›¾ç‰¹å¾çŸ©é˜µ
        self.feature_names = []  # ç‰¹å¾åç§°
        self.clusters = None  # èšç±»ç»“æœ
        self.prototypes = []  # æå–çš„åŸå‹
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_sessions': 0,
            'bullying_sessions': 0,
            'normal_sessions': 0,
            'total_subgraphs': 0,
            'bullying_subgraphs': 0,
            'normal_subgraphs': 0,
            'extracted_prototypes': 0
        }
        
        self.logger.info("åŸå‹æå–å™¨V6é‡æ„ç‰ˆåˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'purity_threshold': 0.5,  # çº¯åº¦é˜ˆå€¼ï¼ˆåŸºäº31%éœ¸å‡Œæ•°æ®åˆ†å¸ƒè°ƒæ•´ï¼‰
            'min_cluster_size': 10,   # æœ€å°èšç±»å¤§å°
            'eps': 0.3,               # DBSCAN epså‚æ•°
            'min_samples': 5,         # DBSCAN min_sampleså‚æ•°
            'max_prototypes': 20,     # æœ€å¤§åŸå‹æ•°é‡
            'feature_dim': 10,        # ç‰¹å¾ç»´åº¦
            'random_state': 42
        }
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('PrototypeExtractorV6Refactored')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def load_session_labels(self, label_file: str = "data/processed/prototypes/session_label_mapping.json") -> bool:
        """åŠ è½½ä¼šè¯æ ‡ç­¾æ˜ å°„"""
        try:
            with open(label_file, 'r', encoding='utf-8') as f:
                self.session_labels = json.load(f)
            
            # ç»Ÿè®¡ä¼šè¯ä¿¡æ¯
            bullying_count = sum(1 for label in self.session_labels.values() if label == 1)
            normal_count = len(self.session_labels) - bullying_count
            
            self.stats.update({
                'total_sessions': len(self.session_labels),
                'bullying_sessions': bullying_count,
                'normal_sessions': normal_count
            })
            
            self.logger.info(f"åŠ è½½ä¼šè¯æ ‡ç­¾å®Œæˆ: {len(self.session_labels)}ä¸ªä¼šè¯")
            self.logger.info(f"  éœ¸å‡Œä¼šè¯: {bullying_count}ä¸ª ({bullying_count/len(self.session_labels)*100:.1f}%)")
            self.logger.info(f"  æ­£å¸¸ä¼šè¯: {normal_count}ä¸ª ({normal_count/len(self.session_labels)*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ä¼šè¯æ ‡ç­¾å¤±è´¥: {e}")
            return False
    
    def load_heterogeneous_graph(self, graph_file: str = "data/graphs/heterogeneous_graph_final.pkl") -> bool:
        """åŠ è½½å¼‚æ„å›¾"""
        try:
            with open(graph_file, 'rb') as f:
                self.heterogeneous_graph = pickle.load(f)
            
            # è®¡ç®—æ€»èŠ‚ç‚¹æ•°
            total_nodes = 0
            if hasattr(self.heterogeneous_graph, 'node_types'):
                for node_type in self.heterogeneous_graph.node_types:
                    node_data = self.heterogeneous_graph[node_type]
                    if hasattr(node_data, 'x') and node_data.x is not None:
                        total_nodes += node_data.x.shape[0]
            
            self.logger.info(f"åŠ è½½å¼‚æ„å›¾å®Œæˆ: {total_nodes:,}ä¸ªèŠ‚ç‚¹")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½å¼‚æ„å›¾å¤±è´¥: {e}")
            return False
    
    def load_enhanced_subgraphs(self, enhanced_dir: str = "data/subgraphs/universal_enhanced") -> bool:
        """åŠ è½½å¢å¼ºç‰ˆå­å›¾æ•°æ®"""
        try:
            enhanced_path = Path(enhanced_dir)
            if not enhanced_path.exists():
                self.logger.error(f"å¢å¼ºç‰ˆå­å›¾ç›®å½•ä¸å­˜åœ¨: {enhanced_dir}")
                return False
            
            pkl_files = list(enhanced_path.glob("*.pkl"))
            if not pkl_files:
                self.logger.error(f"æœªæ‰¾åˆ°å¢å¼ºç‰ˆå­å›¾æ–‡ä»¶: {enhanced_dir}")
                return False
            
            self.logger.info(f"å¼€å§‹åŠ è½½å¢å¼ºç‰ˆå­å›¾æ•°æ®ï¼Œå…±{len(pkl_files)}ä¸ªæ–‡ä»¶...")
            
            total_subgraphs = 0
            bullying_subgraphs = 0
            normal_subgraphs = 0
            
            # ä½¿ç”¨è¿›åº¦æ¡åŠ è½½æ•°æ®
            for pkl_file in tqdm(pkl_files, desc="åŠ è½½å­å›¾æ–‡ä»¶"):
                try:
                    with open(pkl_file, 'rb') as f:
                        data = pickle.load(f)
                    
                    # å…¼å®¹æ–°æ—§æ•°æ®æ ¼å¼
                    if isinstance(data, dict) and 'subgraphs' in data:
                        # æ–°æ ¼å¼ï¼šåŒ…å«subgraphsé”®çš„å­—å…¸
                        subgraphs = data['subgraphs']
                    elif isinstance(data, list):
                        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯å­å›¾åˆ—è¡¨
                        subgraphs = data
                    else:
                        self.logger.warning(f"æœªçŸ¥æ•°æ®æ ¼å¼: {pkl_file}")
                        continue
                    
                    for subgraph in subgraphs:
                        # æ·»åŠ å¼±ç›‘ç£æ ‡ç­¾
                        session_id = subgraph.get('session_id', '')
                        if session_id in self.session_labels:
                            subgraph['weak_label'] = self.session_labels[session_id]
                            self.enhanced_subgraphs.append(subgraph)
                            
                            # åˆ†ç±»å­˜å‚¨
                            if self.session_labels[session_id] == 1:  # éœ¸å‡Œ
                                self.bullying_subgraphs.append(subgraph)
                                bullying_subgraphs += 1
                            else:  # æ­£å¸¸
                                self.normal_subgraphs.append(subgraph)
                                normal_subgraphs += 1
                            
                            total_subgraphs += 1
                        
                except Exception as e:
                    self.logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {pkl_file}: {e}")
                    continue
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats.update({
                'total_subgraphs': total_subgraphs,
                'bullying_subgraphs': bullying_subgraphs,
                'normal_subgraphs': normal_subgraphs
            })
            
            self.logger.info(f"å¢å¼ºç‰ˆå­å›¾åŠ è½½å®Œæˆ:")
            self.logger.info(f"  æ€»å­å›¾æ•°: {total_subgraphs:,}ä¸ª")
            self.logger.info(f"  éœ¸å‡Œå­å›¾: {bullying_subgraphs:,}ä¸ª ({bullying_subgraphs/total_subgraphs*100:.1f}%)")
            self.logger.info(f"  æ­£å¸¸å­å›¾: {normal_subgraphs:,}ä¸ª ({normal_subgraphs/total_subgraphs*100:.1f}%)")
            
            return total_subgraphs > 0
            
        except Exception as e:
            self.logger.error(f"åŠ è½½å¢å¼ºç‰ˆå­å›¾å¤±è´¥: {e}")
            return False
    
    def extract_subgraph_features(self, subgraph: Dict) -> np.ndarray:
        """æå–å­å›¾ç‰¹å¾å‘é‡"""
        try:
            features = []
            
            # 1. åŸºç¡€ç»“æ„ç‰¹å¾
            # å…¼å®¹æ–°æ—§æ ¼å¼çš„å­—æ®µå
            total_nodes = subgraph.get('total_nodes', subgraph.get('size', 0))
            features.append(total_nodes)
            
            # 2. èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
            nodes = subgraph.get('nodes', {})
            comment_count = len(nodes.get('comment', []))
            user_count = len(nodes.get('user', []))
            word_count = len(nodes.get('word', []))
            video_count = len(nodes.get('video', []))
            
            features.extend([comment_count, user_count, word_count, video_count])
            
            # 3. è¾¹çš„æ•°é‡
            edges = subgraph.get('edges', {})
            total_edges = sum(len(edge_list[0]) if edge_list and len(edge_list) > 0 else 0 
                            for edge_list in edges.values())
            features.append(total_edges)
            
            # 4. èŠ‚ç‚¹å¯†åº¦
            node_density = total_edges / max(total_nodes, 1)
            features.append(node_density)
            
            # 5. ä¼šè¯ä¿¡æ¯
            session_id = subgraph.get('session_id', '')
            weak_label = subgraph.get('weak_label', 0)
            features.append(weak_label)
            
            # 6. å­å›¾ç±»å‹ç¼–ç 
            subgraph_type = subgraph.get('subgraph_type', 'unknown')
            type_encoding = 1 if subgraph_type == 'complete_enumeration' else 0
            features.append(type_encoding)
            
            # 7. æ—¶é—´æˆ³ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰
            # å…¼å®¹æ–°æ—§æ ¼å¼çš„æ—¶é—´æˆ³å­—æ®µå
            timestamp = subgraph.get('extraction_timestamp', subgraph.get('timestamp', ''))
            if timestamp:
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    hour_feature = dt.hour / 24.0  # å½’ä¸€åŒ–å°æ—¶
                    features.append(hour_feature)
                except:
                    features.append(0.0)
            else:
                features.append(0.0)
            
            return np.array(features, dtype=np.float32)
            
        except Exception as e:
            self.logger.debug(f"æå–å­å›¾ç‰¹å¾å¤±è´¥: {e}")
            return np.zeros(self.config['feature_dim'], dtype=np.float32)
    
    def build_feature_matrix(self) -> bool:
        """æ„å»ºç‰¹å¾çŸ©é˜µ"""
        try:
            if not self.enhanced_subgraphs:
                self.logger.error("æ²¡æœ‰åŠ è½½å­å›¾æ•°æ®")
                return False
            
            self.logger.info("å¼€å§‹æ„å»ºç‰¹å¾çŸ©é˜µ...")
            
            # å®šä¹‰ç‰¹å¾åç§°
            self.feature_names = [
                'total_nodes', 'comment_count', 'user_count', 'word_count', 
                'video_count', 'total_edges', 'node_density', 'weak_label',
                'type_encoding', 'hour_feature'
            ]
            
            # æå–ç‰¹å¾
            features_list = []
            for subgraph in tqdm(self.enhanced_subgraphs, desc="æå–ç‰¹å¾"):
                features = self.extract_subgraph_features(subgraph)
                features_list.append(features)
            
            self.subgraph_features = np.vstack(features_list)
            
            self.logger.info(f"ç‰¹å¾çŸ©é˜µæ„å»ºå®Œæˆ: {self.subgraph_features.shape}")
            self.logger.info(f"ç‰¹å¾ç»´åº¦: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ„å»ºç‰¹å¾çŸ©é˜µå¤±è´¥: {e}")
            return False
    
    def cluster_subgraphs(self) -> bool:
        """å¯¹éœ¸å‡Œå­å›¾è¿›è¡Œèšç±»"""
        try:
            if self.subgraph_features is None:
                self.logger.error("ç‰¹å¾çŸ©é˜µæœªæ„å»º")
                return False
            
            # åªå¯¹éœ¸å‡Œå­å›¾è¿›è¡Œèšç±»
            bullying_indices = [i for i, subgraph in enumerate(self.enhanced_subgraphs) 
                              if subgraph.get('weak_label') == 1]
            
            if len(bullying_indices) == 0:
                self.logger.error("æ²¡æœ‰éœ¸å‡Œå­å›¾è¿›è¡Œèšç±»")
                return False
            
            bullying_features = self.subgraph_features[bullying_indices]
            
            self.logger.info(f"å¼€å§‹å¯¹{len(bullying_indices)}ä¸ªéœ¸å‡Œå­å›¾è¿›è¡Œèšç±»...")
            
            # æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆé™¤äº†weak_labelåˆ—ï¼‰
            scaler = StandardScaler()
            feature_indices = [i for i, name in enumerate(self.feature_names) if name != 'weak_label']
            scaled_features = bullying_features.copy()
            scaled_features[:, feature_indices] = scaler.fit_transform(bullying_features[:, feature_indices])
            
            # DBSCANèšç±»
            clustering = DBSCAN(
                eps=self.config['eps'],
                min_samples=self.config['min_samples'],
                metric='euclidean'
            )
            
            cluster_labels = clustering.fit_predict(scaled_features[:, feature_indices])
            
            # ç»Ÿè®¡èšç±»ç»“æœ
            unique_labels = set(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            self.logger.info(f"èšç±»å®Œæˆ:")
            self.logger.info(f"  èšç±»æ•°é‡: {n_clusters}")
            self.logger.info(f"  å™ªå£°ç‚¹æ•°: {n_noise}")
            
            # å­˜å‚¨èšç±»ç»“æœ
            self.clusters = {
                'labels': cluster_labels,
                'bullying_indices': bullying_indices,
                'n_clusters': n_clusters,
                'n_noise': n_noise
            }
            
            return n_clusters > 0
            
        except Exception as e:
            self.logger.error(f"èšç±»å¤±è´¥: {e}")
            return False
    
    def calculate_prototype_purity(self, cluster_indices: List[int]) -> float:
        """è®¡ç®—åŸå‹çº¯åº¦"""
        try:
            if not cluster_indices:
                return 0.0
            
            bullying_count = 0
            for idx in cluster_indices:
                subgraph = self.enhanced_subgraphs[idx]
                if subgraph.get('weak_label') == 1:
                    bullying_count += 1
            
            purity = bullying_count / len(cluster_indices)
            return purity
            
        except Exception as e:
            self.logger.debug(f"è®¡ç®—çº¯åº¦å¤±è´¥: {e}")
            return 0.0
    
    def calculate_prototype_discrimination(self, cluster_indices: List[int]) -> float:
        """è®¡ç®—åŸå‹åŒºåˆ†åº¦"""
        try:
            if not cluster_indices or not self.normal_subgraphs:
                return 0.0
            
            # è®¡ç®—èšç±»ä¸­å¿ƒ
            cluster_features = self.subgraph_features[cluster_indices]
            cluster_center = np.mean(cluster_features, axis=0)
            
            # éšæœºé‡‡æ ·æ­£å¸¸å­å›¾
            normal_indices = [i for i, subgraph in enumerate(self.enhanced_subgraphs) 
                            if subgraph.get('weak_label') == 0]
            
            if len(normal_indices) == 0:
                return 0.0
            
            # é‡‡æ ·æœ€å¤š1000ä¸ªæ­£å¸¸å­å›¾
            sample_size = min(1000, len(normal_indices))
            sampled_indices = np.random.choice(normal_indices, sample_size, replace=False)
            normal_features = self.subgraph_features[sampled_indices]
            
            # è®¡ç®—è·ç¦»
            distances = euclidean_distances([cluster_center], normal_features)[0]
            discrimination_score = np.mean(distances)
            
            return discrimination_score
            
        except Exception as e:
            self.logger.debug(f"è®¡ç®—åŒºåˆ†åº¦å¤±è´¥: {e}")
            return 0.0
    
    def calculate_prototype_coverage(self, cluster_indices: List[int]) -> float:
        """è®¡ç®—åŸå‹è¦†ç›–åº¦"""
        try:
            if not cluster_indices:
                return 0.0
            
            # ç»Ÿè®¡è¦†ç›–çš„éœ¸å‡Œä¼šè¯
            covered_sessions = set()
            for idx in cluster_indices:
                subgraph = self.enhanced_subgraphs[idx]
                session_id = subgraph.get('session_id', '')
                if session_id in self.session_labels and self.session_labels[session_id] == 1:
                    covered_sessions.add(session_id)
            
            # è®¡ç®—è¦†ç›–ç‡
            total_bullying_sessions = self.stats['bullying_sessions']
            coverage = len(covered_sessions) / max(total_bullying_sessions, 1)
            
            return coverage
            
        except Exception as e:
            self.logger.debug(f"è®¡ç®—è¦†ç›–åº¦å¤±è´¥: {e}")
            return 0.0
    
    def calculate_prototype_stability(self, cluster_indices: List[int]) -> float:
        """è®¡ç®—åŸå‹ç¨³å®šæ€§"""
        try:
            if len(cluster_indices) < 2:
                return 0.0
            
            cluster_features = self.subgraph_features[cluster_indices]
            
            # è®¡ç®—ç‰¹å¾æ–¹å·®çš„å¹³å‡å€¼ï¼ˆé™¤äº†weak_labelï¼‰
            feature_indices = [i for i, name in enumerate(self.feature_names) if name != 'weak_label']
            variances = np.var(cluster_features[:, feature_indices], axis=0)
            stability_score = 1.0 / (1.0 + np.mean(variances))  # æ–¹å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜
            
            return stability_score
            
        except Exception as e:
            self.logger.debug(f"è®¡ç®—ç¨³å®šæ€§å¤±è´¥: {e}")
            return 0.0
    
    def evaluate_prototype_quality(self, cluster_id: int, cluster_indices: List[int]) -> Dict:
        """è¯„ä¼°åŸå‹è´¨é‡"""
        try:
            # è®¡ç®—4ä¸ªè´¨é‡æŒ‡æ ‡
            purity = self.calculate_prototype_purity(cluster_indices)
            discrimination = self.calculate_prototype_discrimination(cluster_indices)
            coverage = self.calculate_prototype_coverage(cluster_indices)
            stability = self.calculate_prototype_stability(cluster_indices)
            
            # æ„å»ºè´¨é‡è¯„ä¼°ç»“æœ
            quality_metrics = {
                'purity': float(purity),
                'discrimination': float(discrimination),
                'coverage': float(coverage),
                'stability': float(stability),
                'cluster_size': len(cluster_indices),
                'passes_purity_threshold': purity >= self.config['purity_threshold']
            }
            
            return quality_metrics
            
        except Exception as e:
            self.logger.debug(f"è¯„ä¼°åŸå‹è´¨é‡å¤±è´¥: {e}")
            return {
                'purity': 0.0,
                'discrimination': 0.0,
                'coverage': 0.0,
                'stability': 0.0,
                'cluster_size': 0,
                'passes_purity_threshold': False
            }
    
    def extract_prototypes(self) -> bool:
        """æå–åŸå‹"""
        try:
            if self.clusters is None:
                self.logger.error("æœªè¿›è¡Œèšç±»")
                return False
            
            cluster_labels = self.clusters['labels']
            bullying_indices = self.clusters['bullying_indices']
            
            self.logger.info("å¼€å§‹æå–åŸå‹...")
            
            # æŒ‰èšç±»IDåˆ†ç»„
            cluster_groups = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                if label != -1:  # æ’é™¤å™ªå£°ç‚¹
                    original_idx = bullying_indices[i]
                    cluster_groups[label].append(original_idx)
            
            # è¯„ä¼°æ¯ä¸ªèšç±»çš„è´¨é‡
            prototype_candidates = []
            for cluster_id, indices in cluster_groups.items():
                if len(indices) >= self.config['min_cluster_size']:
                    quality_metrics = self.evaluate_prototype_quality(cluster_id, indices)
                    
                    prototype_candidates.append({
                        'cluster_id': cluster_id,
                        'indices': indices,
                        'quality_metrics': quality_metrics
                    })
            
            self.logger.info(f"å€™é€‰åŸå‹æ•°é‡: {len(prototype_candidates)}")
            
            # ç­›é€‰é«˜è´¨é‡åŸå‹
            high_quality_prototypes = [
                candidate for candidate in prototype_candidates
                if candidate['quality_metrics']['passes_purity_threshold']
            ]
            
            self.logger.info(f"é€šè¿‡çº¯åº¦ç­›é€‰çš„åŸå‹: {len(high_quality_prototypes)}")
            
            # æŒ‰åŒºåˆ†åº¦æ’åº
            high_quality_prototypes.sort(
                key=lambda x: x['quality_metrics']['discrimination'], 
                reverse=True
            )
            
            # é€‰æ‹©æœ€ç»ˆåŸå‹
            max_prototypes = min(self.config['max_prototypes'], len(high_quality_prototypes))
            selected_prototypes = high_quality_prototypes[:max_prototypes]
            
            # ç”ŸæˆåŸå‹å¯¹è±¡
            self.prototypes = []
            for i, prototype_data in enumerate(selected_prototypes):
                prototype = self._create_prototype_object(i + 1, prototype_data)
                self.prototypes.append(prototype)
            
            self.stats['extracted_prototypes'] = len(self.prototypes)
            
            self.logger.info(f"åŸå‹æå–å®Œæˆ: {len(self.prototypes)}ä¸ªåŸå‹")
            
            return len(self.prototypes) > 0
            
        except Exception as e:
            self.logger.error(f"æå–åŸå‹å¤±è´¥: {e}")
            return False
    
    def _create_prototype_object(self, prototype_id: int, prototype_data: Dict) -> Dict:
        """åˆ›å»ºåŸå‹å¯¹è±¡"""
        try:
            indices = prototype_data['indices']
            quality_metrics = prototype_data['quality_metrics']
            
            # è®¡ç®—ä»£è¡¨æ€§ç‰¹å¾
            cluster_features = self.subgraph_features[indices]
            representative_features = np.mean(cluster_features, axis=0)
            
            # è·å–ä»£è¡¨æ€§å­å›¾
            cluster_center_idx = self._find_cluster_center(indices)
            representative_subgraph = self.enhanced_subgraphs[cluster_center_idx]
            
            # ç»Ÿè®¡ä¼šè¯åˆ†å¸ƒ
            session_distribution = defaultdict(int)
            for idx in indices:
                session_id = self.enhanced_subgraphs[idx].get('session_id', '')
                if session_id:
                    session_distribution[session_id] += 1
            
            prototype = {
                'id': prototype_id,
                'name': f"Bullying_Prototype_{prototype_id}",
                'cluster_id': prototype_data['cluster_id'],
                'subgraph_indices': indices,
                'subgraph_count': len(indices),
                'quality_metrics': quality_metrics,
                'representative_features': representative_features.tolist(),
                'representative_subgraph': representative_subgraph,
                'session_distribution': dict(session_distribution),
                'extraction_timestamp': datetime.now().isoformat()
            }
            
            return prototype
            
        except Exception as e:
            self.logger.debug(f"åˆ›å»ºåŸå‹å¯¹è±¡å¤±è´¥: {e}")
            return {}
    
    def _find_cluster_center(self, indices: List[int]) -> int:
        """æ‰¾åˆ°èšç±»ä¸­å¿ƒ"""
        try:
            if not indices:
                return 0
            
            cluster_features = self.subgraph_features[indices]
            cluster_center = np.mean(cluster_features, axis=0)
            
            # æ‰¾åˆ°æœ€æ¥è¿‘ä¸­å¿ƒçš„ç‚¹
            distances = euclidean_distances([cluster_center], cluster_features)[0]
            center_idx = indices[np.argmin(distances)]
            
            return center_idx
            
        except Exception as e:
            self.logger.debug(f"å¯»æ‰¾èšç±»ä¸­å¿ƒå¤±è´¥: {e}")
            return indices[0] if indices else 0
    
    def save_results(self, output_dir: str = "data/prototypes") -> Dict:
        """ä¿å­˜ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åŸå‹
            prototypes_file = output_path / f"extracted_prototypes_v6_refactored_{timestamp}.pkl"
            with open(prototypes_file, 'wb') as f:
                pickle.dump(self.prototypes, f)
            
            # ä¿å­˜æ‘˜è¦
            summary = self._generate_summary()
            summary_file = output_path / f"prototype_summary_v6_refactored_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            self.logger.info("ç»“æœä¿å­˜å®Œæˆ:")
            self.logger.info(f"  åŸå‹æ–‡ä»¶: {prototypes_file}")
            self.logger.info(f"  æ‘˜è¦æ–‡ä»¶: {summary_file}")
            
            return {
                'prototypes_file': str(prototypes_file),
                'summary_file': str(summary_file),
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        summary = {
            'extraction_method': 'weak_supervised_v6_refactored',
            'timestamp': datetime.now().isoformat(),
            'configuration': self.config.copy(),
            'statistics': self.stats.copy(),
            'quality_evaluation_method': {
                'purity': 'åŸå‹ä¸­éœ¸å‡Œå­å›¾çš„æ¯”ä¾‹',
                'discrimination': 'ä¸æ­£å¸¸å­å›¾çš„ç‰¹å¾å·®å¼‚',
                'coverage': 'è¦†ç›–çš„éœ¸å‡Œä¼šè¯æ¯”ä¾‹',
                'stability': 'åŸå‹å†…éƒ¨ç‰¹å¾çš„ä¸€è‡´æ€§'
            },
            'prototypes': []
        }
        
        # æ·»åŠ åŸå‹ä¿¡æ¯
        for prototype in self.prototypes:
            prototype_info = {
                'id': prototype['id'],
                'name': prototype['name'],
                'subgraph_count': prototype['subgraph_count'],
                'quality_metrics': prototype['quality_metrics'],
                'covered_sessions': len(prototype['session_distribution']),
                'top_sessions': sorted(
                    prototype['session_distribution'].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]  # å‰5ä¸ªæœ€å¤šçš„ä¼šè¯
            }
            summary['prototypes'].append(prototype_info)
        
        return summary
    
    def run_full_extraction(self, 
                          enhanced_dir: str = "data/subgraphs/universal_enhanced",
                          output_dir: str = "data/prototypes") -> Dict:
        """è¿è¡Œå®Œæ•´çš„åŸå‹æå–æµç¨‹"""
        start_time = datetime.now()
        
        try:
            self.logger.info("å¼€å§‹V6é‡æ„ç‰ˆåŸå‹æå–æµç¨‹...")
            
            # æ­¥éª¤1ï¼šåŠ è½½ä¼šè¯æ ‡ç­¾
            if not self.load_session_labels():
                return {'success': False, 'error': 'åŠ è½½ä¼šè¯æ ‡ç­¾å¤±è´¥'}
            
            # æ­¥éª¤2ï¼šåŠ è½½å¼‚æ„å›¾
            if not self.load_heterogeneous_graph():
                return {'success': False, 'error': 'åŠ è½½å¼‚æ„å›¾å¤±è´¥'}
            
            # æ­¥éª¤3ï¼šåŠ è½½å¢å¼ºç‰ˆå­å›¾
            if not self.load_enhanced_subgraphs(enhanced_dir):
                return {'success': False, 'error': 'åŠ è½½å¢å¼ºç‰ˆå­å›¾å¤±è´¥'}
            
            # æ­¥éª¤4ï¼šæ„å»ºç‰¹å¾çŸ©é˜µ
            if not self.build_feature_matrix():
                return {'success': False, 'error': 'æ„å»ºç‰¹å¾çŸ©é˜µå¤±è´¥'}
            
            # æ­¥éª¤5ï¼šèšç±»
            if not self.cluster_subgraphs():
                return {'success': False, 'error': 'èšç±»å¤±è´¥'}
            
            # æ­¥éª¤6ï¼šæå–åŸå‹
            if not self.extract_prototypes():
                return {'success': False, 'error': 'æå–åŸå‹å¤±è´¥'}
            
            # æ­¥éª¤7ï¼šä¿å­˜ç»“æœ
            save_result = self.save_results(output_dir)
            if not save_result['success']:
                return save_result
            
            # è®¡ç®—æ€»è€—æ—¶
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.logger.info(f"V6é‡æ„ç‰ˆåŸå‹æå–å®Œæˆï¼Œè€—æ—¶ {duration:.1f} ç§’")
            
            return {
                'success': True,
                'duration': duration,
                'statistics': self.stats.copy(),
                'prototypes_count': len(self.prototypes),
                'files': save_result
            }
            
        except Exception as e:
            self.logger.error(f"åŸå‹æå–æµç¨‹å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    print("ProtoBullyé¡¹ç›® - åŸå‹æå–å™¨V6é‡æ„ç‰ˆ")
    print("åŸºäºå¼±ç›‘ç£å­¦ä¹ çš„åŸå‹è´¨é‡è¯„ä¼°")
    print("=" * 50)
    
    # åˆ›å»ºæå–å™¨
    config = {
        'purity_threshold': 0.7,
        'min_cluster_size': 15,
        'eps': 0.3,
        'min_samples': 8,
        'max_prototypes': 15,
        'feature_dim': 10,
        'random_state': 42
    }
    
    extractor = PrototypeExtractorV6Refactored(config)
    
    # è¿è¡Œæå–æµç¨‹
    result = extractor.run_full_extraction()
    
    if result['success']:
        print(f"\nâœ… åŸå‹æå–æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in result['statistics'].items():
            print(f"  {key}: {value:,}")
        print(f"â±ï¸  æ€»è€—æ—¶: {result['duration']:.1f} ç§’")
        print(f"ğŸ¯ æå–çš„åŸå‹æ•°é‡: {result['prototypes_count']}")
    else:
        print(f"\nâŒ åŸå‹æå–å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    main() 