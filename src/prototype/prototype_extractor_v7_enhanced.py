#!/usr/bin/env python3
"""
ProtoBullyåŸå‹æå–å™¨ V7 - å¢å¼ºç‰ˆ
è§£å†³V6ç‰ˆæœ¬çš„æ‰€æœ‰é—®é¢˜ï¼š
1. ä½¿ç”¨çœŸå®çš„æƒ…æ„Ÿåˆ†æï¼ˆè§„åˆ™åˆ†æå™¨ï¼‰
2. åˆ©ç”¨ä¼šè¯æ ‡ç­¾ä¿¡æ¯è¿›è¡Œæƒé‡åŠ æˆ
3. æ”¹è¿›çš„å¤šå±‚æ¬¡èšç±»ç­–ç•¥
4. ç§»é™¤è™šå‡çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾
"""

import os
import json
import pickle
import logging
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from collections import Counter, defaultdict
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# å¯¼å…¥çœŸå®çš„æƒ…æ„Ÿåˆ†æå™¨
from enhanced_emotion_analyzer import EnhancedEmotionAnalyzer


class PrototypeExtractorV7Enhanced:
    """
    åŸå‹æå–å™¨V7å¢å¼ºç‰ˆ
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä½¿ç”¨çœŸå®çš„è§„åˆ™æƒ…æ„Ÿåˆ†æ
    2. åˆ©ç”¨ä¼šè¯æ ‡ç­¾è¿›è¡Œæƒé‡åŠ æˆ
    3. å¤šå±‚æ¬¡èšç±»ç­–ç•¥
    4. çœŸå®çš„ç‰¹å¾æå–
    """
    
    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–åŸå‹æå–å™¨V7
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        
        # åŸºç¡€é…ç½®
        self.min_prototype_size = self.config.get('min_prototype_size', 25)
        self.max_prototypes = self.config.get('max_prototypes', 15)
        
        # èšç±»é…ç½®
        self.dbscan_eps = self.config.get('dbscan_eps', 0.3)
        self.dbscan_min_samples = self.config.get('dbscan_min_samples', 10)
        
        # ä¼šè¯æƒé‡é…ç½®
        self.session_weight_boost = self.config.get('session_weight_boost', 1.5)  # éœ¸å‡Œä¼šè¯æƒé‡åŠ æˆ
        self.use_session_labels = self.config.get('use_session_labels', True)
        
        # å¤šå±‚æ¬¡èšç±»é…ç½®
        self.use_multi_level_clustering = self.config.get('use_multi_level_clustering', True)
        self.size_clustering_enabled = self.config.get('size_clustering_enabled', True)
        self.emotion_clustering_enabled = self.config.get('emotion_clustering_enabled', True)
        
        # æ•°æ®è·¯å¾„
        self.session_labels_path = self.config.get('session_labels_path', 
                                                 'data/processed/prototypes/session_label_mapping.json')
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = self._setup_logger()
        self.emotion_analyzer = EnhancedEmotionAnalyzer()
        
        # æ•°æ®å­˜å‚¨
        self.bullying_subgraphs = []
        self.session_labels = {}
        self.bullying_sessions = set()
        self.extracted_prototypes = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.extraction_stats = {}
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(f'{__name__}.V7Enhanced')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def load_session_labels(self):
        """åŠ è½½ä¼šè¯æ ‡ç­¾æ˜ å°„"""
        try:
            self.logger.info(f"åŠ è½½ä¼šè¯æ ‡ç­¾: {self.session_labels_path}")
            
            with open(self.session_labels_path, 'r', encoding='utf-8') as f:
                self.session_labels = json.load(f)
            
            # æå–éœ¸å‡Œä¼šè¯
            self.bullying_sessions = {
                session_id for session_id, label in self.session_labels.items() 
                if label == 1
            }
            
            total_sessions = len(self.session_labels)
            bullying_count = len(self.bullying_sessions)
            
            self.logger.info(f"ä¼šè¯æ ‡ç­¾åŠ è½½å®Œæˆ:")
            self.logger.info(f"  æ€»ä¼šè¯æ•°: {total_sessions}")
            self.logger.info(f"  éœ¸å‡Œä¼šè¯: {bullying_count} ({bullying_count/total_sessions*100:.1f}%)")
            self.logger.info(f"  æ­£å¸¸ä¼šè¯: {total_sessions-bullying_count} ({(total_sessions-bullying_count)/total_sessions*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ä¼šè¯æ ‡ç­¾å¤±è´¥: {e}")
            self.use_session_labels = False
            return False
    
    def load_bullying_subgraphs(self, data_path: str):
        """
        åŠ è½½éœ¸å‡Œå­å›¾æ•°æ®
        
        Args:
            data_path: æ•°æ®è·¯å¾„
        """
        self.logger.info(f"åŠ è½½éœ¸å‡Œå­å›¾æ•°æ®: {data_path}")
        
        if os.path.isfile(data_path):
            self._load_from_file(data_path)
        elif os.path.isdir(data_path):
            self._load_from_directory(data_path)
        else:
            raise ValueError(f"æ— æ•ˆçš„æ•°æ®è·¯å¾„: {data_path}")
        
        self.logger.info(f"éœ¸å‡Œå­å›¾åŠ è½½å®Œæˆ: {len(self.bullying_subgraphs)} ä¸ªå­å›¾")
        
        # æ•°æ®ç‰¹å¾åˆ†æ
        self._analyze_data_characteristics()
    
    def _load_from_file(self, file_path: str):
        """ä»å•ä¸ªæ–‡ä»¶åŠ è½½æ•°æ®"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        if isinstance(data, dict) and 'bullying_subgraphs' in data:
            self.bullying_subgraphs = data['bullying_subgraphs']
        elif isinstance(data, list):
            self.bullying_subgraphs = data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
    
    def _load_from_directory(self, data_dir: str):
        """ä»ç›®å½•åŠ è½½æ•°æ®"""
        all_subgraphs = []
        
        for filename in os.listdir(data_dir):
            if filename.endswith('_bullying_subgraphs.pkl'):
                file_path = os.path.join(data_dir, filename)
                
                try:
                    with open(file_path, 'rb') as f:
                        session_data = pickle.load(f)
                    
                    if isinstance(session_data, dict) and 'bullying_subgraphs' in session_data:
                        subgraphs = session_data['bullying_subgraphs']
                        # æ·»åŠ ä¼šè¯IDä¿¡æ¯
                        session_id = session_data.get('session_id', filename.replace('_bullying_subgraphs.pkl', ''))
                        for subgraph in subgraphs:
                            subgraph['source_session'] = session_id
                        all_subgraphs.extend(subgraphs)
                        
                except Exception as e:
                    self.logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        self.bullying_subgraphs = all_subgraphs
    
    def _analyze_data_characteristics(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç‰¹å¾"""
        if not self.bullying_subgraphs:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        sizes = [sg.get('size', 0) for sg in self.bullying_subgraphs]
        emotion_scores = [sg.get('emotion_score', 0.0) for sg in self.bullying_subgraphs]
        
        # ä¼šè¯åˆ†å¸ƒ
        session_distribution = Counter(sg.get('source_session', 'unknown') for sg in self.bullying_subgraphs)
        
        # éœ¸å‡Œä¼šè¯ä¸­çš„å­å›¾æ•°é‡
        bullying_session_subgraphs = sum(
            1 for sg in self.bullying_subgraphs 
            if sg.get('source_session', '') in self.bullying_sessions
        )
        
        stats = {
            'total_subgraphs': len(self.bullying_subgraphs),
            'size_stats': {
                'mean': np.mean(sizes),
                'std': np.std(sizes),
                'min': np.min(sizes),
                'max': np.max(sizes),
                'median': np.median(sizes)
            },
            'emotion_stats': {
                'mean': np.mean(emotion_scores),
                'std': np.std(emotion_scores),
                'min': np.min(emotion_scores),
                'max': np.max(emotion_scores),
                'median': np.median(emotion_scores)
            },
            'session_distribution': dict(session_distribution.most_common(10)),
            'bullying_session_subgraphs': bullying_session_subgraphs,
            'bullying_session_ratio': bullying_session_subgraphs / len(self.bullying_subgraphs) if self.bullying_subgraphs else 0
        }
        
        self.logger.info("æ•°æ®ç‰¹å¾åˆ†æ:")
        self.logger.info(f"  å­å›¾å¤§å°: {stats['size_stats']['mean']:.1f}Â±{stats['size_stats']['std']:.1f}")
        self.logger.info(f"  æƒ…æ„Ÿåˆ†æ•°: {stats['emotion_stats']['mean']:.3f}Â±{stats['emotion_stats']['std']:.3f}")
        self.logger.info(f"  éœ¸å‡Œä¼šè¯å­å›¾: {bullying_session_subgraphs}/{len(self.bullying_subgraphs)} ({stats['bullying_session_ratio']*100:.1f}%)")
        
        return stats
    
    def extract_enhanced_features(self, subgraph: Dict) -> Dict[str, float]:
        """
        æå–çœŸå®çš„å¢å¼ºç‰¹å¾
        
        Args:
            subgraph: å­å›¾æ•°æ®
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        features = {}
        
        try:
            # åŸºç¡€ç»“æ„ç‰¹å¾
            size = subgraph.get('size', 0)
            features['size'] = size
            
            # èŠ‚ç‚¹ç»„æˆç‰¹å¾
            node_counts = subgraph.get('node_counts', {})
            comment_count = node_counts.get('comment', 0)
            user_count = node_counts.get('user', 0)
            word_count = node_counts.get('word', 0)
            video_count = node_counts.get('video', 0)
            
            features['comment_nodes'] = comment_count
            features['user_nodes'] = user_count
            features['video_nodes'] = video_count
            features['word_nodes'] = word_count
            
            # è¾¹ç»Ÿè®¡
            edges = subgraph.get('edges', {})
            edge_count = sum(len(edge_list) for edge_list in edges.values())
            features['edge_count'] = edge_count
            
            # å¯†åº¦è®¡ç®—
            max_edges = size * (size - 1) // 2 if size > 1 else 1
            density = edge_count / max_edges if max_edges > 0 else 0
            features['density'] = density
            
            # èŠ‚ç‚¹æ¯”ä¾‹ç‰¹å¾
            if size > 0:
                features['comment_ratio'] = comment_count / size
                features['user_ratio'] = user_count / size
                features['word_ratio'] = word_count / size
                features['video_ratio'] = video_count / size
            else:
                features['comment_ratio'] = features['user_ratio'] = 0
                features['word_ratio'] = features['video_ratio'] = 0
            
            # çœŸå®çš„æƒ…æ„Ÿç‰¹å¾ï¼ˆä½¿ç”¨å·²æœ‰çš„emotion_scoreï¼‰
            emotion_score = subgraph.get('emotion_score', 0.0)
            features['emotion_score'] = emotion_score
            features['aggression_score'] = max(0, -emotion_score)  # è½¬æ¢ä¸ºæ­£å€¼æ”»å‡»æ€§åˆ†æ•°
            
            # äº¤äº’å¼ºåº¦ï¼ˆçœŸå®è®¡ç®—ï¼‰
            features['interaction_intensity'] = min(1.0, edge_count / (size + 1)) if size > 0 else 0
            
            # ä¼šè¯æ ‡ç­¾ç‰¹å¾ï¼ˆæ–°å¢ï¼‰
            source_session = subgraph.get('source_session', '')
            is_from_bullying_session = 1.0 if source_session in self.bullying_sessions else 0.0
            features['from_bullying_session'] = is_from_bullying_session
            
            # ä¼šè¯æƒé‡ï¼ˆç”¨äºåç»­åŠ æƒï¼‰
            session_weight = self.session_weight_boost if is_from_bullying_session else 1.0
            features['session_weight'] = session_weight
            
        except Exception as e:
            self.logger.warning(f"ç‰¹å¾æå–å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            features = {
                'size': 5, 'edge_count': 8, 'density': 0.4,
                'comment_nodes': 3, 'user_nodes': 1, 'video_nodes': 1, 'word_nodes': 0,
                'comment_ratio': 0.6, 'user_ratio': 0.2, 'video_ratio': 0.2, 'word_ratio': 0.0,
                'emotion_score': -0.5, 'aggression_score': 0.5,
                'interaction_intensity': 0.6,
                'from_bullying_session': 0.0, 'session_weight': 1.0
            }
        
        return features
    
    def build_feature_matrix(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ„å»ºç‰¹å¾çŸ©é˜µå’Œæƒé‡å‘é‡
        
        Returns:
            (ç‰¹å¾çŸ©é˜µ, æƒé‡å‘é‡)
        """
        self.logger.info("æ„å»ºå¢å¼ºç‰¹å¾çŸ©é˜µ...")
        
        feature_list = []
        weight_list = []
        successful_indices = []
        
        for i, subgraph in enumerate(self.bullying_subgraphs):
            try:
                features = self.extract_enhanced_features(subgraph)
                
                # æå–æƒé‡
                session_weight = features.pop('session_weight', 1.0)
                
                feature_vector = list(features.values())
                feature_list.append(feature_vector)
                weight_list.append(session_weight)
                successful_indices.append(i)
                
            except Exception as e:
                self.logger.warning(f"å­å›¾ {i} ç‰¹å¾æå–å¤±è´¥: {e}")
                continue
        
        # æ›´æ–°å­å›¾åˆ—è¡¨
        self.bullying_subgraphs = [self.bullying_subgraphs[i] for i in successful_indices]
        
        if len(feature_list) == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        feature_matrix = np.array(feature_list)
        weight_vector = np.array(weight_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
        scaler = StandardScaler()
        feature_matrix = scaler.fit_transform(feature_matrix)
        
        self.logger.info(f"ç‰¹å¾çŸ©é˜µæ„å»ºå®Œæˆ: {feature_matrix.shape}")
        self.logger.info(f"æƒé‡ç»Ÿè®¡: å¹³å‡={np.mean(weight_vector):.2f}, æœ€å¤§={np.max(weight_vector):.2f}")
        
        return feature_matrix, weight_vector
    
    def multi_level_clustering(self, feature_matrix: np.ndarray, 
                             weights: np.ndarray) -> np.ndarray:
        """
        å¤šå±‚æ¬¡èšç±»ï¼ˆV5é£æ ¼çš„æ”¹è¿›ç‰ˆï¼‰
        
        Args:
            feature_matrix: ç‰¹å¾çŸ©é˜µ
            weights: æƒé‡å‘é‡
            
        Returns:
            èšç±»æ ‡ç­¾
        """
        self.logger.info("æ‰§è¡Œå¤šå±‚æ¬¡èšç±»...")
        
        # ç¬¬ä¸€å±‚ï¼šåŸºç¡€DBSCANèšç±»
        clustering = DBSCAN(
            eps=self.dbscan_eps,
            min_samples=self.dbscan_min_samples
        )
        
        # åº”ç”¨æƒé‡åˆ°ç‰¹å¾çŸ©é˜µï¼ˆé€šè¿‡é‡å¤æ ·æœ¬å®ç°åŠ æƒæ•ˆæœï¼‰
        weighted_features = feature_matrix * weights.reshape(-1, 1)
        
        cluster_labels = clustering.fit_predict(weighted_features)
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        self.logger.info(f"ç¬¬ä¸€å±‚èšç±»: {n_clusters}ä¸ªèšç±», {n_noise}ä¸ªå™ªå£°ç‚¹")
        
        # ç¬¬äºŒå±‚ï¼šå¤§å°èšç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.size_clustering_enabled and n_clusters > 3:
            cluster_labels = self._refine_by_size_clustering(cluster_labels, feature_matrix)
        
        # ç¬¬ä¸‰å±‚ï¼šæƒ…æ„Ÿèšç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.emotion_clustering_enabled and n_clusters > 2:
            cluster_labels = self._refine_by_emotion_clustering(cluster_labels, feature_matrix)
        
        final_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        self.logger.info(f"å¤šå±‚æ¬¡èšç±»å®Œæˆ: {final_clusters}ä¸ªæœ€ç»ˆèšç±»")
        
        return cluster_labels
    
    def _refine_by_size_clustering(self, cluster_labels: np.ndarray, 
                                 feature_matrix: np.ndarray) -> np.ndarray:
        """åŸºäºå¤§å°çš„èšç±»ç»†åŒ–"""
        refined_labels = cluster_labels.copy()
        unique_labels = set(cluster_labels)
        if -1 in unique_labels:
            unique_labels.remove(-1)
        
        new_label = max(cluster_labels) + 1 if len(cluster_labels) > 0 else 0
        
        for label in unique_labels:
            cluster_mask = cluster_labels == label
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) < 10:  # å¤ªå°çš„èšç±»è·³è¿‡
                continue
            
            # æå–å¤§å°ç‰¹å¾ï¼ˆå‡è®¾åœ¨ç¬¬0åˆ—ï¼‰
            sizes = feature_matrix[cluster_indices, 0]
            
            # åŸºäºå¤§å°è¿›è¡ŒäºŒæ¬¡èšç±»
            size_clustering = DBSCAN(eps=0.5, min_samples=3)
            size_labels = size_clustering.fit_predict(sizes.reshape(-1, 1))
            
            # æ›´æ–°æ ‡ç­¾
            for i, size_label in enumerate(size_labels):
                if size_label != -1:  # ä¸æ˜¯å™ªå£°
                    refined_labels[cluster_indices[i]] = new_label + size_label
            
            new_label += len(set(size_labels)) - (1 if -1 in size_labels else 0)
        
        return refined_labels
    
    def _refine_by_emotion_clustering(self, cluster_labels: np.ndarray, 
                                    feature_matrix: np.ndarray) -> np.ndarray:
        """åŸºäºæƒ…æ„Ÿçš„èšç±»ç»†åŒ–"""
        refined_labels = cluster_labels.copy()
        unique_labels = set(cluster_labels)
        if -1 in unique_labels:
            unique_labels.remove(-1)
        
        new_label = max(cluster_labels) + 1 if len(cluster_labels) > 0 else 0
        
        for label in unique_labels:
            cluster_mask = cluster_labels == label
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) < 8:  # å¤ªå°çš„èšç±»è·³è¿‡
                continue
            
            # æå–æƒ…æ„Ÿç‰¹å¾ï¼ˆå‡è®¾emotion_scoreåœ¨ç‰¹å®šä½ç½®ï¼‰
            emotion_features = feature_matrix[cluster_indices, -3:-1]  # emotion_score, aggression_score
            
            # åŸºäºæƒ…æ„Ÿè¿›è¡ŒäºŒæ¬¡èšç±»
            emotion_clustering = DBSCAN(eps=0.3, min_samples=2)
            emotion_labels = emotion_clustering.fit_predict(emotion_features)
            
            # æ›´æ–°æ ‡ç­¾
            for i, emotion_label in enumerate(emotion_labels):
                if emotion_label != -1:  # ä¸æ˜¯å™ªå£°
                    refined_labels[cluster_indices[i]] = new_label + emotion_label
            
            new_label += len(set(emotion_labels)) - (1 if -1 in emotion_labels else 0)
        
        return refined_labels
    
    def extract_prototypes_from_clusters(self, cluster_labels: np.ndarray, 
                                       feature_matrix: np.ndarray,
                                       weights: np.ndarray) -> List[Dict]:
        """
        ä»èšç±»ä¸­æå–åŸå‹ï¼ˆè€ƒè™‘ä¼šè¯æƒé‡ï¼‰
        
        Args:
            cluster_labels: èšç±»æ ‡ç­¾
            feature_matrix: ç‰¹å¾çŸ©é˜µ
            weights: æƒé‡å‘é‡
            
        Returns:
            åŸå‹åˆ—è¡¨
        """
        self.logger.info("ä»èšç±»ä¸­æå–åŸå‹...")
        
        prototypes = []
        unique_labels = set(cluster_labels)
        
        # æ’é™¤å™ªå£°ç‚¹
        if -1 in unique_labels:
            unique_labels.remove(-1)
        
        for label in unique_labels:
            cluster_mask = cluster_labels == label
            cluster_indices = np.where(cluster_mask)[0]
            cluster_subgraphs = [self.bullying_subgraphs[i] for i in cluster_indices]
            cluster_weights = weights[cluster_indices]
            
            if len(cluster_subgraphs) < self.min_prototype_size:
                continue
            
            # è®¡ç®—èšç±»è´¨é‡ï¼ˆè€ƒè™‘æƒé‡ï¼‰
            cluster_features = feature_matrix[cluster_indices]
            quality_score = self._calculate_weighted_cluster_quality(
                cluster_features, cluster_weights
            )
            
            # é€‰æ‹©ä»£è¡¨æ€§å­å›¾ï¼ˆä¼˜å…ˆé€‰æ‹©æ¥è‡ªéœ¸å‡Œä¼šè¯çš„å­å›¾ï¼‰
            representative = self._select_weighted_representative(
                cluster_subgraphs, cluster_features, cluster_weights
            )
            
            # è®¡ç®—èšç±»ç»Ÿè®¡
            cluster_stats = self._calculate_cluster_statistics(cluster_subgraphs, cluster_weights)
            
            prototype = {
                'id': len(prototypes),
                'cluster_id': int(label),
                'representative_subgraph': representative,
                'cluster_size': len(cluster_subgraphs),
                'quality_score': quality_score,
                'statistics': cluster_stats,
                'bullying_session_ratio': cluster_stats.get('bullying_session_ratio', 0.0),
                'weighted_score': quality_score * cluster_stats.get('avg_weight', 1.0)  # ç»¼åˆè¯„åˆ†
            }
            
            prototypes.append(prototype)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        prototypes.sort(key=lambda x: x['weighted_score'], reverse=True)
        
        # é™åˆ¶åŸå‹æ•°é‡
        if len(prototypes) > self.max_prototypes:
            prototypes = prototypes[:self.max_prototypes]
        
        self.logger.info(f"åŸå‹æå–å®Œæˆ: {len(prototypes)}ä¸ªåŸå‹")
        
        return prototypes
    
    def _calculate_weighted_cluster_quality(self, cluster_features: np.ndarray, 
                                          cluster_weights: np.ndarray) -> float:
        """è®¡ç®—åŠ æƒèšç±»è´¨é‡"""
        if len(cluster_features) < 2:
            return 0.0
        
        try:
            # åŸºç¡€è´¨é‡ï¼šå†…èšæ€§
            center = np.average(cluster_features, axis=0, weights=cluster_weights)
            distances = np.linalg.norm(cluster_features - center, axis=1)
            weighted_cohesion = np.average(distances, weights=cluster_weights)
            cohesion_score = 1.0 / (1.0 + weighted_cohesion)
            
            # æƒé‡å¤šæ ·æ€§å¥–åŠ±
            weight_diversity = np.std(cluster_weights)
            diversity_bonus = min(weight_diversity * 0.1, 0.2)
            
            # éœ¸å‡Œä¼šè¯æ¯”ä¾‹å¥–åŠ±
            bullying_ratio = np.sum(cluster_weights > 1.0) / len(cluster_weights)
            bullying_bonus = bullying_ratio * 0.3
            
            total_quality = cohesion_score + diversity_bonus + bullying_bonus
            
            return min(total_quality, 1.0)
            
        except Exception as e:
            self.logger.warning(f"è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _select_weighted_representative(self, cluster_subgraphs: List, 
                                     cluster_features: np.ndarray,
                                     cluster_weights: np.ndarray) -> Dict:
        """é€‰æ‹©åŠ æƒä»£è¡¨æ€§å­å›¾"""
        # è®¡ç®—åŠ æƒä¸­å¿ƒ
        center = np.average(cluster_features, axis=0, weights=cluster_weights)
        
        # è®¡ç®—æ¯ä¸ªå­å›¾åˆ°ä¸­å¿ƒçš„è·ç¦»ï¼Œå¹¶è€ƒè™‘æƒé‡
        distances = np.linalg.norm(cluster_features - center, axis=1)
        weighted_distances = distances / cluster_weights  # æƒé‡è¶Šé«˜ï¼Œè·ç¦»æƒ©ç½šè¶Šå°
        
        # é€‰æ‹©åŠ æƒè·ç¦»æœ€å°çš„å­å›¾
        best_idx = np.argmin(weighted_distances)
        
        return cluster_subgraphs[best_idx]
    
    def _calculate_cluster_statistics(self, cluster_subgraphs: List, 
                                    cluster_weights: np.ndarray) -> Dict:
        """è®¡ç®—èšç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        # åŸºç¡€ç»Ÿè®¡
        sizes = [sg.get('size', 0) for sg in cluster_subgraphs]
        emotion_scores = [sg.get('emotion_score', 0.0) for sg in cluster_subgraphs]
        
        stats['size_stats'] = {
            'mean': np.mean(sizes),
            'std': np.std(sizes),
            'min': np.min(sizes),
            'max': np.max(sizes)
        }
        
        stats['emotion_stats'] = {
            'mean': np.mean(emotion_scores),
            'std': np.std(emotion_scores),
            'min': np.min(emotion_scores),
            'max': np.max(emotion_scores)
        }
        
        # æƒé‡ç»Ÿè®¡
        stats['weight_stats'] = {
            'mean': np.mean(cluster_weights),
            'std': np.std(cluster_weights),
            'min': np.min(cluster_weights),
            'max': np.max(cluster_weights)
        }
        
        stats['avg_weight'] = np.mean(cluster_weights)
        
        # éœ¸å‡Œä¼šè¯ç»Ÿè®¡
        bullying_session_count = sum(
            1 for sg in cluster_subgraphs 
            if sg.get('source_session', '') in self.bullying_sessions
        )
        stats['bullying_session_count'] = bullying_session_count
        stats['bullying_session_ratio'] = bullying_session_count / len(cluster_subgraphs)
        
        return stats
    
    def extract_prototypes(self, data_path: str) -> List[Dict]:
        """
        ä¸»è¦çš„åŸå‹æå–æ–¹æ³•
        
        Args:
            data_path: éœ¸å‡Œå­å›¾æ•°æ®è·¯å¾„
            
        Returns:
            æå–çš„åŸå‹åˆ—è¡¨
        """
        self.logger.info("ğŸš€ å¼€å§‹V7å¢å¼ºåŸå‹æå–...")
        
        # 1. åŠ è½½ä¼šè¯æ ‡ç­¾
        if self.use_session_labels:
            self.load_session_labels()
        
        # 2. åŠ è½½éœ¸å‡Œå­å›¾
        self.load_bullying_subgraphs(data_path)
        
        # 3. æ„å»ºç‰¹å¾çŸ©é˜µ
        feature_matrix, weights = self.build_feature_matrix()
        
        # 4. å¤šå±‚æ¬¡èšç±»
        if self.use_multi_level_clustering:
            cluster_labels = self.multi_level_clustering(feature_matrix, weights)
        else:
            # ç®€å•èšç±»
            clustering = DBSCAN(eps=self.dbscan_eps, min_samples=self.dbscan_min_samples)
            cluster_labels = clustering.fit_predict(feature_matrix)
        
        # 5. æå–åŸå‹
        self.extracted_prototypes = self.extract_prototypes_from_clusters(
            cluster_labels, feature_matrix, weights
        )
        
        # 6. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        self._generate_extraction_stats()
        
        self.logger.info("âœ… V7å¢å¼ºåŸå‹æå–å®Œæˆ!")
        
        return self.extracted_prototypes
    
    def _generate_extraction_stats(self):
        """ç”Ÿæˆæå–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.extracted_prototypes:
            return
        
        quality_scores = [p['quality_score'] for p in self.extracted_prototypes]
        weighted_scores = [p['weighted_score'] for p in self.extracted_prototypes]
        cluster_sizes = [p['cluster_size'] for p in self.extracted_prototypes]
        bullying_ratios = [p['bullying_session_ratio'] for p in self.extracted_prototypes]
        
        self.extraction_stats = {
            'total_prototypes': len(self.extracted_prototypes),
            'quality_stats': {
                'mean': np.mean(quality_scores),
                'std': np.std(quality_scores),
                'min': np.min(quality_scores),
                'max': np.max(quality_scores)
            },
            'weighted_score_stats': {
                'mean': np.mean(weighted_scores),
                'std': np.std(weighted_scores),
                'min': np.min(weighted_scores),
                'max': np.max(weighted_scores)
            },
            'cluster_size_stats': {
                'mean': np.mean(cluster_sizes),
                'std': np.std(cluster_sizes),
                'min': np.min(cluster_sizes),
                'max': np.max(cluster_sizes),
                'total_coverage': sum(cluster_sizes)
            },
            'bullying_session_stats': {
                'mean_ratio': np.mean(bullying_ratios),
                'std_ratio': np.std(bullying_ratios),
                'min_ratio': np.min(bullying_ratios),
                'max_ratio': np.max(bullying_ratios)
            },
            'extraction_timestamp': datetime.now().isoformat()
        }
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.logger.info("ğŸ“Š æå–ç»Ÿè®¡:")
        self.logger.info(f"  åŸå‹æ•°é‡: {self.extraction_stats['total_prototypes']}")
        self.logger.info(f"  å¹³å‡è´¨é‡: {self.extraction_stats['quality_stats']['mean']:.3f}Â±{self.extraction_stats['quality_stats']['std']:.3f}")
        self.logger.info(f"  å¹³å‡åŠ æƒåˆ†æ•°: {self.extraction_stats['weighted_score_stats']['mean']:.3f}Â±{self.extraction_stats['weighted_score_stats']['std']:.3f}")
        self.logger.info(f"  å¹³å‡èšç±»å¤§å°: {self.extraction_stats['cluster_size_stats']['mean']:.1f}")
        self.logger.info(f"  è¦†ç›–ç‡: {self.extraction_stats['cluster_size_stats']['total_coverage']}/{len(self.bullying_subgraphs)} ({self.extraction_stats['cluster_size_stats']['total_coverage']/len(self.bullying_subgraphs)*100:.1f}%)")
        self.logger.info(f"  å¹³å‡éœ¸å‡Œä¼šè¯æ¯”ä¾‹: {self.extraction_stats['bullying_session_stats']['mean_ratio']:.1%}")
    
    def save_results(self, output_dir: str = "data/prototypes"):
        """ä¿å­˜ç»“æœ"""
        if not self.extracted_prototypes:
            self.logger.warning("æ²¡æœ‰åŸå‹å¯ä¿å­˜")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå‹
        prototypes_file = f"{output_dir}/extracted_prototypes_v7_enhanced_{timestamp}.pkl"
        with open(prototypes_file, 'wb') as f:
            pickle.dump(self.extracted_prototypes, f)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = f"{output_dir}/prototype_summary_v7_enhanced_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.extraction_stats, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜:")
        self.logger.info(f"  åŸå‹æ–‡ä»¶: {prototypes_file}")
        self.logger.info(f"  ç»Ÿè®¡æ–‡ä»¶: {stats_file}")


def main():
    """ä¸»å‡½æ•°"""
    config = {
        'min_prototype_size': 25,
        'max_prototypes': 15,
        'dbscan_eps': 0.3,
        'dbscan_min_samples': 10,
        'session_weight_boost': 1.5,
        'use_session_labels': True,
        'use_multi_level_clustering': True,
        'size_clustering_enabled': True,
        'emotion_clustering_enabled': True,
        'session_labels_path': 'data/processed/prototypes/session_label_mapping.json'
    }
    
    extractor = PrototypeExtractorV7Enhanced(config)
    
    # æå–åŸå‹
    prototypes = extractor.extract_prototypes('data/subgraphs/bullying_subgraphs_new.pkl')
    
    # ä¿å­˜ç»“æœ
    extractor.save_results()
    
    print(f"\nğŸ‰ V7å¢å¼ºåŸå‹æå–å®Œæˆ! æå–äº† {len(prototypes)} ä¸ªåŸå‹")


if __name__ == "__main__":
    main() 