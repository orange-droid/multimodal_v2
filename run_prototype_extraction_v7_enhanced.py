#!/usr/bin/env python3
"""
è¿è¡ŒProtoBullyåŸå‹æå–å™¨V7å¢å¼ºç‰ˆ
è§£å†³æ‰€æœ‰V6ç‰ˆæœ¬çš„é—®é¢˜ï¼š
1. ä½¿ç”¨çœŸå®çš„æƒ…æ„Ÿåˆ†æ
2. åˆ©ç”¨ä¼šè¯æ ‡ç­¾è¿›è¡Œæƒé‡åŠ æˆ
3. ç§»é™¤è™šå‡ç‰¹å¾
4. æ”¹è¿›èšç±»ç­–ç•¥
"""

import os
import sys
import json
import pickle
import logging
import numpy as np
from typing import Dict, List, Any, Tuple
from datetime import datetime
from collections import Counter
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# æ·»åŠ srcè·¯å¾„
sys.path.append('src')

class PrototypeExtractorV7Enhanced:
    """åŸå‹æå–å™¨V7å¢å¼ºç‰ˆ"""
    
    def __init__(self, config: Dict = None):
        """åˆå§‹åŒ–åŸå‹æå–å™¨V7"""
        self.config = config or {}
        
        # åŸºç¡€é…ç½®
        self.min_prototype_size = self.config.get('min_prototype_size', 25)
        self.max_prototypes = self.config.get('max_prototypes', 15)
        
        # èšç±»é…ç½®
        self.dbscan_eps = self.config.get('dbscan_eps', 0.3)
        self.dbscan_min_samples = self.config.get('dbscan_min_samples', 10)
        
        # ä¼šè¯æƒé‡é…ç½®
        self.session_weight_boost = self.config.get('session_weight_boost', 1.5)
        self.use_session_labels = self.config.get('use_session_labels', True)
        
        # æ•°æ®è·¯å¾„
        self.session_labels_path = self.config.get('session_labels_path', 
                                                 'data/processed/prototypes/session_label_mapping.json')
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = self._setup_logger()
        
        # æ•°æ®å­˜å‚¨
        self.bullying_subgraphs = []
        self.session_labels = {}
        self.bullying_sessions = set()
        self.extracted_prototypes = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.extraction_stats = {}
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(f'{__name__}.V7Enhanced')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def load_session_labels(self):
        """åŠ è½½ä¼šè¯æ ‡ç­¾æ˜ å°„"""
        try:
            self.logger.info(f"åŠ è½½ä¼šè¯æ ‡ç­¾: {self.session_labels_path}")
            
            with open(self.session_labels_path, 'r', encoding='utf-8') as f:
                self.session_labels = json.load(f)
            
            # æå–éœ¸å‡Œä¼šè¯
            self.bullying_sessions = {
                session_id for session_id, label in self.session_labels.items() 
                if label == 1
            }
            
            total_sessions = len(self.session_labels)
            bullying_count = len(self.bullying_sessions)
            
            self.logger.info(f"ä¼šè¯æ ‡ç­¾åŠ è½½å®Œæˆ:")
            self.logger.info(f"  æ€»ä¼šè¯æ•°: {total_sessions}")
            self.logger.info(f"  éœ¸å‡Œä¼šè¯: {bullying_count} ({bullying_count/total_sessions*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ä¼šè¯æ ‡ç­¾å¤±è´¥: {e}")
            self.use_session_labels = False
            return False
    
    def load_bullying_subgraphs(self, data_path: str):
        """åŠ è½½éœ¸å‡Œå­å›¾æ•°æ®"""
        self.logger.info(f"åŠ è½½éœ¸å‡Œå­å›¾æ•°æ®: {data_path}")
        
        if os.path.isfile(data_path):
            self._load_from_file(data_path)
        elif os.path.isdir(data_path):
            self._load_from_directory(data_path)
        else:
            raise ValueError(f"æ— æ•ˆçš„æ•°æ®è·¯å¾„: {data_path}")
        
        self.logger.info(f"éœ¸å‡Œå­å›¾åŠ è½½å®Œæˆ: {len(self.bullying_subgraphs)} ä¸ªå­å›¾")
        
        # æ•°æ®ç‰¹å¾åˆ†æ
        self._analyze_data_characteristics()
    
    def _load_from_file(self, file_path: str):
        """ä»å•ä¸ªæ–‡ä»¶åŠ è½½æ•°æ®"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        if isinstance(data, dict) and 'bullying_subgraphs' in data:
            self.bullying_subgraphs = data['bullying_subgraphs']
        elif isinstance(data, list):
            self.bullying_subgraphs = data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
    
    def _load_from_directory(self, data_dir: str):
        """ä»ç›®å½•åŠ è½½æ•°æ®"""
        all_subgraphs = []
        
        for filename in os.listdir(data_dir):
            if filename.endswith('_bullying_subgraphs.pkl'):
                file_path = os.path.join(data_dir, filename)
                
                try:
                    with open(file_path, 'rb') as f:
                        session_data = pickle.load(f)
                    
                    if isinstance(session_data, dict) and 'bullying_subgraphs' in session_data:
                        subgraphs = session_data['bullying_subgraphs']
                        # æ·»åŠ ä¼šè¯IDä¿¡æ¯
                        session_id = session_data.get('session_id', filename.replace('_bullying_subgraphs.pkl', ''))
                        for subgraph in subgraphs:
                            subgraph['source_session'] = session_id
                        all_subgraphs.extend(subgraphs)
                        
                except Exception as e:
                    self.logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        self.bullying_subgraphs = all_subgraphs
    
    def _analyze_data_characteristics(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç‰¹å¾"""
        if not self.bullying_subgraphs:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        sizes = [sg.get('size', 0) for sg in self.bullying_subgraphs]
        emotion_scores = [sg.get('emotion_score', 0.0) for sg in self.bullying_subgraphs]
        
        # ä¼šè¯åˆ†å¸ƒ
        session_distribution = Counter(sg.get('source_session', 'unknown') for sg in self.bullying_subgraphs)
        
        # éœ¸å‡Œä¼šè¯ä¸­çš„å­å›¾æ•°é‡
        bullying_session_subgraphs = sum(
            1 for sg in self.bullying_subgraphs 
            if sg.get('session_id', '') in self.bullying_sessions
        )
        
        stats = {
            'total_subgraphs': len(self.bullying_subgraphs),
            'bullying_session_subgraphs': bullying_session_subgraphs,
            'bullying_session_ratio': bullying_session_subgraphs / len(self.bullying_subgraphs) if self.bullying_subgraphs else 0
        }
        
        self.logger.info("æ•°æ®ç‰¹å¾åˆ†æ:")
        self.logger.info(f"  æ€»å­å›¾æ•°: {len(self.bullying_subgraphs)}")
        self.logger.info(f"  éœ¸å‡Œä¼šè¯å­å›¾: {bullying_session_subgraphs} ({stats['bullying_session_ratio']*100:.1f}%)")
        
        return stats
    
    def extract_enhanced_features(self, subgraph: Dict) -> Dict[str, float]:
        """æå–çœŸå®çš„å¢å¼ºç‰¹å¾ï¼ˆç§»é™¤è™šå‡ç‰¹å¾ï¼‰"""
        features = {}
        
        try:
            # åŸºç¡€ç»“æ„ç‰¹å¾
            size = subgraph.get('size', 0)
            features['size'] = size
            
            # èŠ‚ç‚¹ç»„æˆç‰¹å¾
            node_counts = subgraph.get('node_counts', {})
            comment_count = node_counts.get('comment', 0)
            user_count = node_counts.get('user', 0)
            word_count = node_counts.get('word', 0)
            video_count = node_counts.get('video', 0)
            
            features['comment_nodes'] = comment_count
            features['user_nodes'] = user_count
            features['video_nodes'] = video_count
            features['word_nodes'] = word_count
            
            # è¾¹ç»Ÿè®¡
            edges = subgraph.get('edges', {})
            edge_count = sum(len(edge_list) for edge_list in edges.values())
            features['edge_count'] = edge_count
            
            # å¯†åº¦è®¡ç®—
            max_edges = size * (size - 1) // 2 if size > 1 else 1
            density = edge_count / max_edges if max_edges > 0 else 0
            features['density'] = density
            
            # èŠ‚ç‚¹æ¯”ä¾‹ç‰¹å¾
            if size > 0:
                features['comment_ratio'] = comment_count / size
                features['user_ratio'] = user_count / size
                features['word_ratio'] = word_count / size
                features['video_ratio'] = video_count / size
            else:
                features['comment_ratio'] = features['user_ratio'] = 0
                features['word_ratio'] = features['video_ratio'] = 0
            
            # çœŸå®çš„æƒ…æ„Ÿç‰¹å¾ï¼ˆä½¿ç”¨å·²æœ‰çš„emotion_scoreï¼Œæ¥è‡ªè§„åˆ™åˆ†æå™¨ï¼‰
            emotion_score = subgraph.get('emotion_score', 0.0)
            features['emotion_score'] = emotion_score
            features['aggression_score'] = max(0, -emotion_score)  # è½¬æ¢ä¸ºæ­£å€¼æ”»å‡»æ€§åˆ†æ•°
            
            # äº¤äº’å¼ºåº¦ï¼ˆçœŸå®è®¡ç®—ï¼‰
            features['interaction_intensity'] = min(1.0, edge_count / (size + 1)) if size > 0 else 0
            
            # ä¼šè¯æ ‡ç­¾ç‰¹å¾ï¼ˆæ–°å¢ - æ ¸å¿ƒæ”¹è¿›ï¼‰
            session_id = subgraph.get('session_id', '')
            is_from_bullying_session = 1.0 if session_id in self.bullying_sessions else 0.0
            features['from_bullying_session'] = is_from_bullying_session
            
            # ä¼šè¯æƒé‡ï¼ˆç”¨äºåç»­åŠ æƒ - æ ¸å¿ƒæ”¹è¿›ï¼‰
            session_weight = self.session_weight_boost if is_from_bullying_session else 1.0
            features['session_weight'] = session_weight
            
        except Exception as e:
            self.logger.warning(f"ç‰¹å¾æå–å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            features = {
                'size': 5, 'edge_count': 8, 'density': 0.4,
                'comment_nodes': 3, 'user_nodes': 1, 'video_nodes': 1, 'word_nodes': 0,
                'comment_ratio': 0.6, 'user_ratio': 0.2, 'video_ratio': 0.2, 'word_ratio': 0.0,
                'emotion_score': -0.5, 'aggression_score': 0.5,
                'interaction_intensity': 0.6,
                'from_bullying_session': 0.0, 'session_weight': 1.0
            }
        
        return features
    
    def build_feature_matrix(self) -> Tuple[np.ndarray, np.ndarray]:
        """æ„å»ºç‰¹å¾çŸ©é˜µå’Œæƒé‡å‘é‡"""
        self.logger.info("æ„å»ºå¢å¼ºç‰¹å¾çŸ©é˜µ...")
        
        feature_list = []
        weight_list = []
        successful_indices = []
        
        for i, subgraph in enumerate(self.bullying_subgraphs):
            try:
                features = self.extract_enhanced_features(subgraph)
                
                # æå–æƒé‡ï¼ˆè¿™æ˜¯æ ¸å¿ƒæ”¹è¿›ï¼‰
                session_weight = features.pop('session_weight', 1.0)
                
                feature_vector = list(features.values())
                feature_list.append(feature_vector)
                weight_list.append(session_weight)
                successful_indices.append(i)
                
            except Exception as e:
                self.logger.warning(f"å­å›¾ {i} ç‰¹å¾æå–å¤±è´¥: {e}")
                continue
        
        # æ›´æ–°å­å›¾åˆ—è¡¨
        self.bullying_subgraphs = [self.bullying_subgraphs[i] for i in successful_indices]
        
        if len(feature_list) == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        feature_matrix = np.array(feature_list)
        weight_vector = np.array(weight_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
        scaler = StandardScaler()
        feature_matrix = scaler.fit_transform(feature_matrix)
        
        self.logger.info(f"ç‰¹å¾çŸ©é˜µæ„å»ºå®Œæˆ: {feature_matrix.shape}")
        self.logger.info(f"æƒé‡ç»Ÿè®¡: å¹³å‡={np.mean(weight_vector):.2f}, æœ€å¤§={np.max(weight_vector):.2f}")
        self.logger.info(f"éœ¸å‡Œä¼šè¯æƒé‡åŠ æˆå­å›¾: {np.sum(weight_vector > 1.0)}/{len(weight_vector)} ({np.sum(weight_vector > 1.0)/len(weight_vector)*100:.1f}%)")
        
        return feature_matrix, weight_vector
    
    def weighted_clustering(self, feature_matrix: np.ndarray, 
                          weights: np.ndarray) -> np.ndarray:
        """åŠ æƒèšç±»ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰"""
        self.logger.info("æ‰§è¡ŒåŠ æƒèšç±»...")
        
        # åº”ç”¨æƒé‡åˆ°ç‰¹å¾çŸ©é˜µï¼ˆéœ¸å‡Œä¼šè¯çš„å­å›¾ç‰¹å¾è¢«æ”¾å¤§ï¼‰
        weighted_features = feature_matrix * weights.reshape(-1, 1)
        
        clustering = DBSCAN(
            eps=self.dbscan_eps,
            min_samples=self.dbscan_min_samples
        )
        
        cluster_labels = clustering.fit_predict(weighted_features)
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        self.logger.info(f"åŠ æƒèšç±»å®Œæˆ: {n_clusters}ä¸ªèšç±», {n_noise}ä¸ªå™ªå£°ç‚¹")
        
        return cluster_labels
    
    def extract_prototypes_from_clusters(self, cluster_labels: np.ndarray, 
                                       feature_matrix: np.ndarray,
                                       weights: np.ndarray) -> List[Dict]:
        """ä»èšç±»ä¸­æå–åŸå‹ï¼ˆè€ƒè™‘ä¼šè¯æƒé‡ï¼‰"""
        self.logger.info("ä»èšç±»ä¸­æå–åŸå‹...")
        
        prototypes = []
        unique_labels = set(cluster_labels)
        
        # æ’é™¤å™ªå£°ç‚¹
        if -1 in unique_labels:
            unique_labels.remove(-1)
        
        for label in unique_labels:
            cluster_mask = cluster_labels == label
            cluster_indices = np.where(cluster_mask)[0]
            cluster_subgraphs = [self.bullying_subgraphs[i] for i in cluster_indices]
            cluster_weights = weights[cluster_indices]
            
            if len(cluster_subgraphs) < self.min_prototype_size:
                continue
            
            # è®¡ç®—èšç±»è´¨é‡ï¼ˆè€ƒè™‘æƒé‡ï¼‰
            cluster_features = feature_matrix[cluster_indices]
            quality_score = self._calculate_weighted_cluster_quality(
                cluster_features, cluster_weights
            )
            
            # é€‰æ‹©ä»£è¡¨æ€§å­å›¾ï¼ˆä¼˜å…ˆé€‰æ‹©æ¥è‡ªéœ¸å‡Œä¼šè¯çš„å­å›¾ï¼‰
            representative = self._select_weighted_representative(
                cluster_subgraphs, cluster_features, cluster_weights
            )
            
            # è®¡ç®—èšç±»ç»Ÿè®¡
            cluster_stats = self._calculate_cluster_statistics(cluster_subgraphs, cluster_weights)
            
            prototype = {
                'id': len(prototypes),
                'cluster_id': int(label),
                'representative_subgraph': representative,
                'cluster_size': len(cluster_subgraphs),
                'quality_score': quality_score,
                'statistics': cluster_stats,
                'bullying_session_ratio': cluster_stats.get('bullying_session_ratio', 0.0),
                'weighted_score': quality_score * cluster_stats.get('avg_weight', 1.0)  # ç»¼åˆè¯„åˆ†
            }
            
            prototypes.append(prototype)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åºï¼ˆéœ¸å‡Œä¼šè¯åŸå‹ä¼˜å…ˆï¼‰
        prototypes.sort(key=lambda x: x['weighted_score'], reverse=True)
        
        # é™åˆ¶åŸå‹æ•°é‡
        if len(prototypes) > self.max_prototypes:
            prototypes = prototypes[:self.max_prototypes]
        
        self.logger.info(f"åŸå‹æå–å®Œæˆ: {len(prototypes)}ä¸ªåŸå‹")
        
        return prototypes
    
    def _calculate_weighted_cluster_quality(self, cluster_features: np.ndarray, 
                                          cluster_weights: np.ndarray) -> float:
        """è®¡ç®—åŠ æƒèšç±»è´¨é‡"""
        if len(cluster_features) < 2:
            return 0.0
        
        try:
            # åŸºç¡€è´¨é‡ï¼šå†…èšæ€§ï¼ˆä¸»è¦åˆ†æ•°ï¼‰
            center = np.average(cluster_features, axis=0, weights=cluster_weights)
            distances = np.linalg.norm(cluster_features - center, axis=1)
            weighted_cohesion = np.average(distances, weights=cluster_weights)
            cohesion_score = 1.0 / (1.0 + weighted_cohesion)
            
            # éœ¸å‡Œä¼šè¯æ¯”ä¾‹å¥–åŠ±ï¼ˆä¿®å¤ï¼šåŸºäºå®é™…æƒé‡åˆ†å¸ƒï¼‰
            # æƒé‡>1.0è¡¨ç¤ºæ¥è‡ªéœ¸å‡Œä¼šè¯ï¼ˆ1.5å€æƒé‡ï¼‰
            bullying_count = np.sum(cluster_weights > 1.0)
            bullying_ratio = bullying_count / len(cluster_weights)
            bullying_bonus = bullying_ratio * 0.15  # é™ä½å¥–åŠ±é¿å…è¿‡åº¦
            
            # æƒé‡å¤šæ ·æ€§è¯„ä¼°ï¼ˆä¿®å¤ï¼šæ›´åˆç†çš„å¤šæ ·æ€§è®¡ç®—ï¼‰
            if len(cluster_weights) > 1:
                weight_std = np.std(cluster_weights)
                weight_mean = np.mean(cluster_weights)
                weight_cv = weight_std / weight_mean if weight_mean > 0 else 0
                diversity_bonus = min(weight_cv * 0.1, 0.1)  # å˜å¼‚ç³»æ•°ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
            else:
                diversity_bonus = 0.0
            
            # æ€»è´¨é‡åˆ†æ•°ï¼ˆä¸æˆªæ–­ï¼Œå…è®¸è¶…è¿‡1.0ï¼‰
            total_quality = cohesion_score + bullying_bonus + diversity_bonus
            
            # è®°å½•è¯¦ç»†è®¡ç®—è¿‡ç¨‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            self.logger.debug(f"è´¨é‡è®¡ç®—è¯¦æƒ…: cohesion={cohesion_score:.3f}, "
                            f"bullying_bonus={bullying_bonus:.3f} (ratio={bullying_ratio:.3f}), "
                            f"diversity_bonus={diversity_bonus:.3f}, total={total_quality:.3f}")
            
            return total_quality
            
        except Exception as e:
            self.logger.warning(f"è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _select_weighted_representative(self, cluster_subgraphs: List, 
                                     cluster_features: np.ndarray,
                                     cluster_weights: np.ndarray) -> Dict:
        """é€‰æ‹©åŠ æƒä»£è¡¨æ€§å­å›¾ï¼ˆä¼˜å…ˆéœ¸å‡Œä¼šè¯ï¼‰"""
        # è®¡ç®—åŠ æƒä¸­å¿ƒ
        center = np.average(cluster_features, axis=0, weights=cluster_weights)
        
        # è®¡ç®—æ¯ä¸ªå­å›¾åˆ°ä¸­å¿ƒçš„è·ç¦»ï¼Œå¹¶è€ƒè™‘æƒé‡
        distances = np.linalg.norm(cluster_features - center, axis=1)
        weighted_distances = distances / cluster_weights  # æƒé‡è¶Šé«˜ï¼Œè·ç¦»æƒ©ç½šè¶Šå°
        
        # é€‰æ‹©åŠ æƒè·ç¦»æœ€å°çš„å­å›¾
        best_idx = np.argmin(weighted_distances)
        
        return cluster_subgraphs[best_idx]
    
    def _calculate_cluster_statistics(self, cluster_subgraphs: List, 
                                    cluster_weights: np.ndarray) -> Dict:
        """è®¡ç®—èšç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        # æƒé‡ç»Ÿè®¡
        stats['avg_weight'] = np.mean(cluster_weights)
        stats['max_weight'] = np.max(cluster_weights)
        stats['min_weight'] = np.min(cluster_weights)
        
        # éœ¸å‡Œä¼šè¯ç»Ÿè®¡ï¼ˆä¿®å¤ï¼šæ›´å‡†ç¡®çš„ç»Ÿè®¡æ–¹å¼ï¼‰
        # æ–¹æ³•1ï¼šåŸºäºæƒé‡åˆ¤æ–­ï¼ˆæƒé‡>1.0è¡¨ç¤ºæ¥è‡ªéœ¸å‡Œä¼šè¯ï¼‰
        bullying_count_by_weight = np.sum(cluster_weights > 1.0)
        
        # æ–¹æ³•2ï¼šåŸºäºsession_idåˆ¤æ–­ï¼ˆå¦‚æœæœ‰ä¼šè¯æ ‡ç­¾çš„è¯ï¼‰
        bullying_count_by_session = 0
        if hasattr(self, 'bullying_sessions') and self.bullying_sessions:
            bullying_count_by_session = sum(
                1 for sg in cluster_subgraphs 
                if sg.get('session_id', '') in self.bullying_sessions
            )
        
        # ä½¿ç”¨æƒé‡æ–¹æ³•ä½œä¸ºä¸»è¦ç»Ÿè®¡ï¼ˆæ›´å¯é ï¼‰
        stats['bullying_session_count'] = bullying_count_by_weight
        stats['bullying_session_ratio'] = bullying_count_by_weight / len(cluster_subgraphs)
        
        # è®°å½•ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”ï¼ˆç”¨äºéªŒè¯ï¼‰
        if bullying_count_by_session > 0:
            stats['bullying_count_by_session'] = bullying_count_by_session
            stats['bullying_ratio_by_session'] = bullying_count_by_session / len(cluster_subgraphs)
        
        return stats
    
    def extract_prototypes(self, data_path: str) -> List[Dict]:
        """ä¸»è¦çš„åŸå‹æå–æ–¹æ³•"""
        self.logger.info("ğŸš€ å¼€å§‹V7å¢å¼ºåŸå‹æå–...")
        
        # 1. åŠ è½½ä¼šè¯æ ‡ç­¾
        if self.use_session_labels:
            self.load_session_labels()
        
        # 2. åŠ è½½éœ¸å‡Œå­å›¾
        self.load_bullying_subgraphs(data_path)
        
        # 3. æ„å»ºç‰¹å¾çŸ©é˜µ
        feature_matrix, weights = self.build_feature_matrix()
        
        # 4. åŠ æƒèšç±»
        cluster_labels = self.weighted_clustering(feature_matrix, weights)
        
        # 5. æå–åŸå‹
        self.extracted_prototypes = self.extract_prototypes_from_clusters(
            cluster_labels, feature_matrix, weights
        )
        
        # 6. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        self._generate_extraction_stats()
        
        self.logger.info("âœ… V7å¢å¼ºåŸå‹æå–å®Œæˆ!")
        
        return self.extracted_prototypes
    
    def _generate_extraction_stats(self):
        """ç”Ÿæˆæå–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.extracted_prototypes:
            return
        
        quality_scores = [p['quality_score'] for p in self.extracted_prototypes]
        weighted_scores = [p['weighted_score'] for p in self.extracted_prototypes]
        cluster_sizes = [p['cluster_size'] for p in self.extracted_prototypes]
        bullying_ratios = [p['bullying_session_ratio'] for p in self.extracted_prototypes]
        
        self.extraction_stats = {
            'total_prototypes': len(self.extracted_prototypes),
            'avg_quality': np.mean(quality_scores),
            'avg_weighted_score': np.mean(weighted_scores),
            'avg_cluster_size': np.mean(cluster_sizes),
            'total_coverage': sum(cluster_sizes),
            'avg_bullying_ratio': np.mean(bullying_ratios),
            'extraction_timestamp': datetime.now().isoformat()
        }
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.logger.info("ğŸ“Š æå–ç»Ÿè®¡:")
        self.logger.info(f"  åŸå‹æ•°é‡: {self.extraction_stats['total_prototypes']}")
        self.logger.info(f"  å¹³å‡è´¨é‡: {self.extraction_stats['avg_quality']:.3f}")
        self.logger.info(f"  å¹³å‡åŠ æƒåˆ†æ•°: {self.extraction_stats['avg_weighted_score']:.3f}")
        self.logger.info(f"  è¦†ç›–ç‡: {self.extraction_stats['total_coverage']}/{len(self.bullying_subgraphs)} ({self.extraction_stats['total_coverage']/len(self.bullying_subgraphs)*100:.1f}%)")
        self.logger.info(f"  å¹³å‡éœ¸å‡Œä¼šè¯æ¯”ä¾‹: {self.extraction_stats['avg_bullying_ratio']:.1%}")
    
    def save_results(self, output_dir: str = "data/prototypes"):
        """ä¿å­˜ç»“æœ"""
        if not self.extracted_prototypes:
            self.logger.warning("æ²¡æœ‰åŸå‹å¯ä¿å­˜")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå‹
        prototypes_file = f"{output_dir}/extracted_prototypes_v7_enhanced_{timestamp}.pkl"
        with open(prototypes_file, 'wb') as f:
            pickle.dump(self.extracted_prototypes, f)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = f"{output_dir}/prototype_summary_v7_enhanced_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.extraction_stats, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜:")
        self.logger.info(f"  åŸå‹æ–‡ä»¶: {prototypes_file}")
        self.logger.info(f"  ç»Ÿè®¡æ–‡ä»¶: {stats_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ProtoBullyåŸå‹æå–å™¨V7å¢å¼ºç‰ˆ")
    print("=" * 50)
    
    # é…ç½®å‚æ•°ï¼ˆä¿®å¤ï¼šè°ƒæ•´æƒé‡åŠ æˆå’Œèšç±»å‚æ•°ï¼‰
    config = {
        'min_prototype_size': 25,
        'max_prototypes': 15,
        'dbscan_eps': 0.35,  # ç¨å¾®å¢å¤§ï¼Œå…è®¸æ›´å¤šæ ·çš„èšç±»
        'dbscan_min_samples': 8,  # é™ä½æœ€å°æ ·æœ¬æ•°ï¼Œå¢åŠ èšç±»å¤šæ ·æ€§
        'session_weight_boost': 1.3,  # é™ä½æƒé‡åŠ æˆï¼Œé¿å…è¿‡åº¦é›†ä¸­ï¼ˆä»1.5é™åˆ°1.3ï¼‰
        'use_session_labels': True,
        'session_labels_path': 'data/processed/prototypes/session_label_mapping.json'
    }
    
    print("ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    
    # åˆ›å»ºæå–å™¨
    extractor = PrototypeExtractorV7Enhanced(config)
    
    # æå–åŸå‹
    try:
        prototypes = extractor.extract_prototypes('data/subgraphs/bullying_subgraphs_new.pkl')
        
        # ä¿å­˜ç»“æœ
        extractor.save_results()
        
        print(f"\nğŸ‰ V7å¢å¼ºåŸå‹æå–æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š æå–äº† {len(prototypes)} ä¸ªé«˜è´¨é‡åŸå‹")
        
        # æ˜¾ç¤ºå‰5ä¸ªåŸå‹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä¿®å¤ï¼šæ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯ï¼‰
        if prototypes:
            print(f"\nğŸ“‹ å‰5ä¸ªåŸå‹è¯¦æƒ…:")
            for i, prototype in enumerate(prototypes[:5]):
                stats = prototype.get('statistics', {})
                print(f"  åŸå‹ {i+1}:")
                print(f"    è´¨é‡åˆ†æ•°: {prototype['quality_score']:.4f}")
                print(f"    åŠ æƒåˆ†æ•°: {prototype['weighted_score']:.4f}")
                print(f"    èšç±»å¤§å°: {prototype['cluster_size']}")
                print(f"    éœ¸å‡Œä¼šè¯æ¯”ä¾‹: {prototype['bullying_session_ratio']:.1%}")
                print(f"    å¹³å‡æƒé‡: {stats.get('avg_weight', 0):.3f}")
                print(f"    æƒé‡èŒƒå›´: {stats.get('min_weight', 0):.3f} - {stats.get('max_weight', 0):.3f}")
                
                # æ˜¾ç¤ºä»£è¡¨æ€§å­å›¾çš„åŸºæœ¬ä¿¡æ¯
                rep_sg = prototype.get('representative_subgraph', {})
                if rep_sg:
                    nodes = rep_sg.get('nodes', {})
                    print(f"    ä»£è¡¨å­å›¾: {sum(len(v) for v in nodes.values())}ä¸ªèŠ‚ç‚¹")
                    node_types = [f"{k}({len(v)})" for k, v in nodes.items() if len(v) > 0]
                    print(f"    èŠ‚ç‚¹æ„æˆ: {', '.join(node_types)}")
                print()
        
        print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æå–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 