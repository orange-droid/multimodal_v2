#!/usr/bin/env python3
"""
ç»§ç»­è¿è¡Œå¢å¼ºç‰ˆé€šç”¨å­å›¾æå–å™¨

åªå¤„ç†ç¼ºå¤±çš„ä¼šè¯ï¼ˆ945-958ï¼‰
"""

import os
import sys
import pickle
import logging
import time
import glob
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append('src')

from prototype.universal_subgraph_extractor import UniversalSubgraphExtractor

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'continue_enhanced_subgraph_extraction_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )

def get_missing_sessions(output_dir: str) -> list:
    """è·å–ç¼ºå¤±çš„ä¼šè¯åˆ—è¡¨"""
    # è·å–å·²å­˜åœ¨çš„ä¼šè¯æ–‡ä»¶
    existing_files = glob.glob(f'{output_dir}/media_session_*_subgraphs.pkl')
    existing_sessions = set()
    
    for file in existing_files:
        filename = os.path.basename(file)
        session_num = int(filename.replace('media_session_', '').replace('_subgraphs.pkl', ''))
        existing_sessions.add(session_num)
    
    # æ‰¾å‡ºç¼ºå¤±çš„ä¼šè¯
    all_sessions = set(range(959))
    missing_sessions = sorted(list(all_sessions - existing_sessions))
    
    return missing_sessions

class ContinueExtractor(UniversalSubgraphExtractor):
    """ç»§ç»­æå–çš„å­å›¾æå–å™¨"""
    
    def __init__(self, config=None, target_sessions=None):
        super().__init__(config)
        self.target_sessions = target_sessions or []
    
    def _get_all_sessions(self, graph):
        """åªè¿”å›ç›®æ ‡ä¼šè¯"""
        return [f"media_session_{i}" for i in self.target_sessions]

def main():
    """ä¸»å‡½æ•°"""
    print("=== ç»§ç»­å¢å¼ºç‰ˆé€šç”¨å­å›¾æå–å™¨ ===")
    print("åªå¤„ç†ç¼ºå¤±çš„ä¼šè¯")
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_dir = "data/subgraphs/universal_enhanced"
    if not os.path.exists(output_dir):
        logger.error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return
    
    # è·å–ç¼ºå¤±çš„ä¼šè¯
    missing_sessions = get_missing_sessions(output_dir)
    
    if not missing_sessions:
        logger.info("æ‰€æœ‰ä¼šè¯éƒ½å·²æå–å®Œæˆï¼Œæ— éœ€ç»§ç»­æå–")
        return
    
    logger.info(f"å‘ç° {len(missing_sessions)} ä¸ªç¼ºå¤±çš„ä¼šè¯: {missing_sessions}")
    
    # å¢å¼ºé…ç½®
    config = {
        # åŸºæœ¬é…ç½®
        'min_subgraph_size': 6,
        'max_subgraph_size': 15,
        'size_step': 1,
        
        # èŠ‚ç‚¹æ•°é‡é™åˆ¶
        'max_comments_per_video': 50,
        'max_users_per_subgraph': 8,
        'max_words_per_subgraph': 25,
        'max_seeds_per_session': 30,
        
        # å¤šç”¨æˆ·äº¤äº’é…ç½®
        'enable_multi_user_subgraphs': True,
        'min_interacting_users': 2,
        'max_multi_user_combinations': 100,
        
        # å®Œå…¨æšä¸¾é…ç½®
        'enable_smart_selection': True,
        'max_enumeration_combinations': 500,
        
        # å…¶ä»–ç­–ç•¥é…ç½®
        'enable_single_type_subgraphs': True,
        'single_type_min_size': 6,
        'single_type_max_size': 15,
        'random_sampling_ratio': 0.3,
        'diversity_boost': True,
        'enable_large_subgraphs': True,
        
        # ç­–ç•¥æƒé‡
        'strategy_weights': {
            'multi_user_interaction': 0.25,  # å¤šç”¨æˆ·äº¤äº’
            'smart_selection': 0.25,         # æ™ºèƒ½é€‰å–
            'traditional': 0.2,              # ä¼ ç»Ÿå¤šç±»å‹
            'single_type': 0.15,             # å•ç±»å‹
            'random_combo': 0.15             # éšæœºç»„åˆ
        }
    }
    
    logger.info("é…ç½®å‚æ•°:")
    for key, value in config.items():
        logger.info(f"  {key}: {value}")
    
    # åˆ›å»ºç»§ç»­æå–å™¨
    logger.info("åˆ›å»ºç»§ç»­å¢å¼ºç‰ˆå­å›¾æå–å™¨...")
    extractor = ContinueExtractor(config, missing_sessions)
    
    # åŠ è½½å›¾æ•°æ®
    graph_path = "data/graphs/heterogeneous_graph_final.pkl"
    logger.info(f"åŠ è½½å¼‚æ„å›¾æ•°æ®: {graph_path}")
    
    if not os.path.exists(graph_path):
        logger.error(f"å›¾æ–‡ä»¶ä¸å­˜åœ¨: {graph_path}")
        return
    
    try:
        with open(graph_path, 'rb') as f:
            graph = pickle.load(f)
        
        logger.info(f"å›¾åŠ è½½æˆåŠŸ:")
        logger.info(f"  èŠ‚ç‚¹ç±»å‹: {graph.node_types}")
        logger.info(f"  è¾¹ç±»å‹: {graph.edge_types}")
        logger.info(f"  èŠ‚ç‚¹æ•°é‡: {[f'{nt}({graph[nt].x.shape[0]})' for nt in graph.node_types]}")
        logger.info(f"  è¾¹æ•°é‡: {[f'{et}({graph[et].edge_index.size(1)})' for et in graph.edge_types]}")
        
    except Exception as e:
        logger.error(f"åŠ è½½å›¾æ•°æ®å¤±è´¥: {e}")
        return
    
    # å¼€å§‹æå–
    logger.info("å¼€å§‹ç»§ç»­å¢å¼ºç‰ˆå­å›¾æå–...")
    start_time = time.time()
    
    try:
        stats = extractor.extract_all_session_subgraphs(graph, output_dir)
        
        end_time = time.time()
        extraction_time = end_time - start_time
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info("=" * 60)
        logger.info("ç»§ç»­å¢å¼ºç‰ˆå­å›¾æå–å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"æ€»è€—æ—¶: {extraction_time:.2f} ç§’")
        logger.info(f"å¤„ç†ä¼šè¯æ•°: {stats['total_sessions']}")
        logger.info(f"æˆåŠŸä¼šè¯æ•°: {stats['successful_sessions']}")
        logger.info(f"æ€»å­å›¾æ•°: {stats['total_subgraphs']}")
        logger.info(f"å¹³å‡æ¯ä¼šè¯å­å›¾æ•°: {stats['avg_subgraphs_per_session']:.1f}")
        
        logger.info("\nå­å›¾ç±»å‹åˆ†å¸ƒ:")
        for subgraph_type, count in stats['subgraph_type_distribution'].items():
            percentage = (count / stats['total_subgraphs']) * 100 if stats['total_subgraphs'] > 0 else 0
            logger.info(f"  {subgraph_type}: {count} ({percentage:.1f}%)")
        
        logger.info("\nå­å›¾å¤§å°åˆ†å¸ƒ:")
        size_items = sorted(stats['size_distribution'].items())
        for size, count in size_items[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            percentage = (count / stats['total_subgraphs']) * 100 if stats['total_subgraphs'] > 0 else 0
            logger.info(f"  {size}èŠ‚ç‚¹: {count} ({percentage:.1f}%)")
        
        if len(size_items) > 10:
            logger.info(f"  ... è¿˜æœ‰ {len(size_items) - 10} ç§å¤§å°")
        
        logger.info("\nå¢å¼ºç‰¹æ€§ç»Ÿè®¡:")
        enhancement_features = stats['enhancement_features']
        logger.info(f"  å¤šç”¨æˆ·äº¤äº’å¯ç”¨: {enhancement_features['multi_user_enabled']}")
        logger.info(f"  æ™ºèƒ½é€‰å–å¯ç”¨: {enhancement_features['smart_selection_enabled']}")
        logger.info(f"  ç”¨æˆ·äº¤äº’å…³ç³»æ•°: {enhancement_features['user_interactions_loaded']}")
        logger.info(f"  èŠ‚ç‚¹æ•°é‡èŒƒå›´: {enhancement_features['size_range']}")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if extraction_time > 0:
            sessions_per_second = stats['successful_sessions'] / extraction_time
            subgraphs_per_second = stats['total_subgraphs'] / extraction_time
            logger.info(f"\næ€§èƒ½æŒ‡æ ‡:")
            logger.info(f"  å¤„ç†é€Ÿåº¦: {sessions_per_second:.2f} ä¼šè¯/ç§’")
            logger.info(f"  ç”Ÿæˆé€Ÿåº¦: {subgraphs_per_second:.2f} å­å›¾/ç§’")
        
        logger.info(f"\nç»“æœä¿å­˜åœ¨: {output_dir}")
        logger.info("=" * 60)
        
        # éªŒè¯æ˜¯å¦å®Œæˆ
        final_missing = get_missing_sessions(output_dir)
        if not final_missing:
            logger.info("ğŸ‰ æ‰€æœ‰ä¼šè¯å­å›¾æå–å®Œæˆï¼")
        else:
            logger.warning(f"ä»æœ‰ {len(final_missing)} ä¸ªä¼šè¯æœªå®Œæˆ: {final_missing}")
        
        return stats
        
    except Exception as e:
        logger.error(f"å­å›¾æå–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
        return None

if __name__ == "__main__":
    main() 